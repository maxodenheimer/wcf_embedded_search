{
  "url": "https://stratechery.com",
  "sections": [
    {
      "title": "Meta and Amazon, Meta Shops and ATT, Amazon Logistics and the Anti-Amazon Alliance",
      "publishedDate": "",
      "updatedDate": "",
      "contentHtml": "",
      "contentText": "",
      "subsections": []
    },
    {
      "title": "Disney Earnings, Disney 3.0, Streaming and Sports",
      "publishedDate": "",
      "updatedDate": "",
      "contentHtml": "",
      "contentText": "",
      "subsections": []
    },
    {
      "title": "An Interview with Intel CEO Pat Gelsinger About Intel’s Progress Towards Process Leadership",
      "publishedDate": "",
      "updatedDate": "",
      "contentHtml": "",
      "contentText": "",
      "subsections": []
    },
    {
      "title": "Realtors Lose in Court, Zillow and Real Estate Aggregation, From Franchises to Businesses",
      "publishedDate": "",
      "updatedDate": "",
      "contentHtml": "",
      "contentText": "",
      "subsections": []
    },
    {
      "title": "The OpenAI Keynote",
      "publishedDate": "2023-11-07T06:51:39-08:00",
      "updatedDate": "2023-11-07T20:33:18-08:00",
      "contentHtml": "\n\t\t<p>In 2013, when I started Stratechery, there was no bigger event than the launch of the new iPhone; its only rival was Google I/O, which is when the newest version of Android was unveiled (hardware always breaks the tie, including with Apple’s iOS introductions at WWDC). It wasn’t just that smartphones were relatively new and still adding critical features, but that the strategic decisions and ultimate fates of the platforms were still an open question. More than that, the entire future of the tech industry was clearly tied up in said platforms and their corresponding operating systems and devices; how could keynotes <em>not</em> be a big deal?</p>\n<p>Fast forward a decade and the tech keynote has diminished in importance and, in the case of Apple, disappeared completely, replaced by a pre-recorded marketing video. I want to be mad about it, but it makes sense: an iPhone introduction has been diminished not by Apple’s presentation, but rather Apple’s presentations reflect the reality that the most important questions around an iPhone are about marketing tactics. How do you segment the iPhone line? How do you price? What sort of brand affinity are you seeking to build? There, I just summarized <a href=\"https://stratechery.com/2023/apples-iphone-event-innovation-and-iteration-pricing-inflation-and-services/\">the iPhone 15 introduction</a>, and the reality that the smartphone era — <a href=\"https://stratechery.com/2020/the-end-of-the-beginning/\">The End of the Beginning</a> — is over as far as strategic considerations are concerned. iOS and Android are a given, but what is next and yet unknown?</p>\n<p>The answer is, clearly, AI, but even there, the energy seems muted: Apple hasn’t talked about generative AI other than to assure investors on earnings calls that they are working on it; Google I/O was of course about AI, but mostly in the context of Google’s own products — few of which have actually shipped — and <a href=\"https://stratechery.com/2023/google-i-o-and-the-coming-ai-battles/\">my Article at the time</a> was quickly side-tracked into philosophical discussions about both the nature of AI innovation (sustaining versus disruptive), the question of tech revolution versus alignment, and a preview of the coming battles of regulation that arrived with <a href=\"https://stratechery.com/2023/attenuating-innovation-ai/\">last week’s Executive Order on AI</a>.</p>\n<p><a href=\"https://stratechery.com/2023/ai-hardware-and-virtual-reality/\">Meta’s Connect keynote was much more interesting</a>: not only were AI characters being added to Meta’s social networks, but next year you will be able to take AI with you via Smart Glasses (I told you hardware was interesting!). Nothing, though, seemed to match the energy around yesterday’s OpenAI developer conference, their first ever: there is nothing more interesting in tech than a consumer product with product-market fit. And that, for me, is enough to bring back an old Stratechery standby: the keynote day-after.</p>\n<h3>Keynote Metaphysics and GPT-4 Turbo</h3>\n<p>This was, first and foremost, a really <em>good</em> keynote, in the keynote-as-artifact sense. CEO Sam Altman, in a humorous exchange with Microsoft CEO Satya Nadella, promised, “I won’t take too much of your time”; never mind that Nadella was presumably in San Francisco just for this event: in this case he stood in for the audience who witnessed a presentation that was tight, with content that was interesting, leaving them with a desire to learn more.</p>\n<p>Altman himself had a good stage presence, with the sort of nervous energy that is only present in a live keynote; the fact he never seemed to know which side of the stage a fellow presenter was coming from was humanizing. Meanwhile, the live demos not only went off without a hitch, but leveraged the fact that they were live: in one instance a presenter instructed a GPT she created to text Altman; he held up his phone to show he got the message. In another a GPT randomly selected five members of the audience to receive $500 in OpenAI API credits, only to then extend it to everyone.</p>\n<p>New products and features, meanwhile, were available “today”, not weeks or months in the future, as is increasingly the case for events like I/O or WWDC; everything combined to give a palpable sense of progress and excitement, which, when it comes to AI, is mostly true.</p>\n<p>GPT-4 Turbo is an excellent example of what I mean by “mostly”. The API consists of six new features:</p>\n<ul>\n<li>Increased context length</li>\n<li>More control, specifically in terms of model inputs and outputs</li>\n<li>Better knowledge, which both means updating the cut-off date for knowledge about the world to April 2023 and providing the ability for developers to easily add their own knowledge base</li>\n<li>New modalities, as DALL-E 3, Vision, and TTS (text-to-speech) will all be included in the API, with a new version of Whisper speech recognition coming.</li>\n<li>Customization, including fine-tuning, and custom models (which, Altman warned, won’t be cheap)</li>\n<li>Higher rate limits</li>\n</ul>\n<p>This is, to be clear, still the same foundational model (GPT-4); these features just make the API more usable, both in terms of features and also performance. It also speaks to how OpenAI is becoming more of a product company, with iterative enhancements of its core functionality. Yes, the mission still remains AGI (artificial general intelligence), and the core scientific team is almost certainly working on GPT-5, but Altman and team aren’t just tossing models over the wall for the rest of the industry to figure out.</p>\n<h3>Price and Microsoft</h3>\n<p>The next “feature” was tied into the GPT-4 Turbo introduction: the API is getting cheaper (3x cheaper for input tokens, and 2x cheaper for output tokens). Unsurprisingly this announcement elicited cheers from the developers in attendance; what I cheered as an analyst was Altman’s clear articulation of the company’s priorities: lower price first, speed later. You can certainly debate whether that is the right set of priorities (I think it is, because the biggest need now is for increased experimentation, not optimization), but what I appreciated was the clarity.</p>\n<p>It’s also appropriate that the segment after that was the brief “interview” with Nadella: OpenAI’s pricing is ultimately a function of Microsoft’s ability to build the infrastructure to support that pricing. Nadella actually explained how Microsoft is accomplishing that on the <a href=\"https://seekingalpha.com/article/4643129-microsoft-corporation-msft-q1-2024-earnings-call-transcript\">company’s most recent earnings call</a>:</p>\n<blockquote><p>\n  It is true that the approach we have taken is a full stack approach all the way from whether it’s ChatGPT or Bing Chat or all our Copilots, all share the same model. So in some sense, one of the things that we do have is very, very high leverage of the one model that we used, which we trained, and then the one model that we are doing inferencing at scale. And that advantage sort of trickles down all the way to both utilization internally, utilization of third parties, and also over time, you can see the sort of stack optimization all the way to the silicon, because the abstraction layer to which the developers are riding is much higher up than low-level kernels, if you will.</p>\n<p>  So, therefore, I think there is a fundamental approach we took, which was a technical approach of saying we’ll have Copilots and Copilot stack all available. That doesn’t mean we don’t have people doing training for open source models or proprietary models. We also have a bunch of open source models. We have a bunch of fine-tuning happening, a bunch of RLHF happening. So there’s all kinds of ways people use it. But the thing is, we have scale leverage of one large model that was trained and one large model that’s being used for inference across all our first-party SaaS apps, as well as our API in our Azure AI service…</p>\n<p>  The lesson learned from the cloud side is — we’re not running a conglomerate of different businesses, it’s all one tech stack up and down Microsoft’s portfolio, and that, I think, is going to be very important because that discipline, given what the spend like — it will look like for this AI transition any business that’s not disciplined about their capital spend accruing across all their businesses could run into trouble.\n</p></blockquote>\n<p>The fact that Microsoft is benefiting from OpenAI is obvious; what this makes clear is that OpenAI uniquely benefits from Microsoft as well, in a way they would not from another cloud provider: <a href=\"https://stratechery.com/2023/google-earnings-microsoft-earnings-ai-leverage/\">because Microsoft is also a product company investing in the infrastructure to run OpenAI’s models for said products</a>, it can afford to optimize and invest ahead of usage in a way that OpenAI alone, even with the support of another cloud provider, could not. In this case that is paying off in developers needing to pay less, or, ideally, have more latitude to discover use cases that result in them paying far more because usage is exploding.</p>\n<h3>GPTs and Computers</h3>\n<p>I mentioned GPTs before; you were probably confused, because this is a name that is either brilliant or a total disaster. Of course you could have said the same about ChatGPT: massive consumer uptake has a way of making arguably poor choices great ones in retrospect, and I can see why OpenAI is seeking to basically brand “GPT” — generative pre-trained transformer — as an OpenAI chatbot.</p>\n<p>Regardless, this was how Altman explains GPTs:</p>\n<div align=\"center\"><div id=\"v-0EkKoygR-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/0EkKoygR?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div>\n<blockquote><p>\n  GPTs are tailored version of ChatGPT for a specific purpose. You can build a GPT — a customized version of ChatGPT — for almost anything, with instructions, expanded knowledge, and actions, and then you can publish it for others to use. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. They’ll make it easier for you accomplish all sorts of tasks or just have more fun, and you’ll be able to use them right within ChatGPT. You can, in effect, program a GPT, with language, just by talking to it. It’s easy to customize the behavior so that it fits what you want. This makes building them very accessible, and it gives agency to everyone.</p>\n<p>  We’re going to show you what GPTs are, how to use them, how to build them, and then we’re going to talk about how they’ll be distributed and discovered. And then after that, for developers, we’re going to show you how to build these agent-like experiences into your own apps.\n</p></blockquote>\n<p>Altman’s examples included a lesson-planning GPT from Code.org and a natural language vision design GPT from Canva. As Altman noted, the second example might have seemed familiar: Canva had a plugin for ChatGPT, and Altman explained that “we’ve evolved our plugins to be custom actions for GPTs.”</p>\n<p>I found the plugin concept fascinating and a useful way to understand both the capabilities and limits of large language models; I wrote in <a href=\"https://stratechery.com/2023/chatgpt-learns-computing/\">ChatGPT Gets a Computer</a>:</p>\n<blockquote><p>\n  The implication of this approach is that computers are deterministic: if circuit X is open, then the proposition represented by X is true; 1 plus 1 is always 2; clicking “back” on your browser will exit this page. There are, of course, a huge number of abstractions and massive amounts of logic between an individual transistor and any action we might take with a computer — and an effectively infinite number of places for bugs — but the appropriate mental model for a computer is that they do exactly what they are told (indeed, a bug is not the computer making a mistake, but rather a manifestation of the programmer telling the computer to do the wrong thing)…\n</p></blockquote>\n<p>Large language models, though, with their probabilistic approach, are in many domains shockingly intuitive, and yet can hallucinate and are downright terrible at math; that is why the most compelling plug-in OpenAI launched was from Wolfram|Alpha. Stephen Wolfram <a href=\"https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/\">explained</a>:</p>\n<blockquote><p>\n  For decades there’s been a dichotomy in thinking about AI between “statistical approaches” of the kind ChatGPT uses, and “symbolic approaches” that are in effect the starting point for Wolfram|Alpha. But now—thanks to the success of ChatGPT—as well as all the work we’ve done in making Wolfram|Alpha understand natural language—there’s finally the opportunity to combine these to make something much stronger than either could ever achieve on their own.\n</p></blockquote>\n<p>That is the exact combination that happened, which led to the title of that Article:</p>\n<blockquote><p>\n  The fact this works so well is itself a testament to what Assistant AI’s are, and are not: they are not computing as we have previously understood it; they are shockingly human in their way of “thinking” and communicating. And frankly, I would have had a hard time solving those three questions as well — that’s what computers are for! And now ChatGPT has a computer of its own.\n</p></blockquote>\n<p>I still think the concept was incredibly elegant, but there was just one problem: the user interface was terrible. You had to get a plugin from the “marketplace”, then pre-select it before you began a conversation, and only then would you get workable results after a too-long process where ChatGPT negotiated with the plugin provider in question on the answer.</p>\n<p>This new model somewhat alleviates the problem: now, instead of having to select the correct plug-in (and thus restart your chat), you simply go directly to the GPT in question. In other words, if I want to create a poster, I don’t enable the Canva plugin in ChatGPT, I go to Canva GPT in the sidebar. Notice that this doesn’t actually <em>solve</em> the problem of needing to have selected the right tool; what it does do is make the choice more apparent to the user at a more appropriate stage in the process, and that’s no small thing. I also suspect that GPTs will be much faster than plug-ins, given they are integrated from the get-go. Finally, standalone GPTs are a much better fit with the store model that OpenAI is trying to develop.</p>\n<p>Still, there is a better way: Altman demoed it.</p>\n<h3>ChatGPT and the Universal Interface</h3>\n<p>Before Altman introduced the aforementioned GPTs he talked about improvements to ChatGPT:</p>\n<div align=\"center\"><div id=\"v-ot3CmlaF-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/ot3CmlaF?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div>\n<blockquote><p>\n  Even though this is a developer conference, we can’t help resist making some improvements to ChatGPT. A small one, ChatGPT now uses GPT-4 Turbo, with all of the latest improvements, including the latest cut-off, which we’ll continue to update — that’s all live today. It can now browse the web when it needs to, write and run code, analyze data, generate images, and much more, and we heard your feedback that that model picker was extremely annoying: that is gone, starting today. You will not have to click around a drop-down menu. All of this will just work together. ChatGPT will just know what to use and when you need it. But that’s not the main thing.\n</p></blockquote>\n<p>You may wonder why I put this section after GPTs, given they were, according to Altman, the main thing: it’s because I think this feature enhancement is actually much more important. As I just noted, GPTs are a somewhat better UI on an elegant plugin concept, in which a probabilisitic large language model gets access to a deterministic computer. The best UI, though, is no UI at all, or rather, just one UI, by which I mean “Universal Interface”.</p>\n<p>In this case “browsing” or “image generation” are basically plug-ins: they are specialized capabilities that, before today, you had to explicitly invoke; going forward they will just work. ChatGPT will seamlessly switch between text generation, image generation, and web browsing, without the user needing to change context. What is necessary for the plug-in/GPT idea to ultimately take root is for the same capabilities to be extended broadly: if my conversation involved math, ChatGPT should know to use Wolfram|Alpha on its own, without me adding the plug-in or going to a specialized GPT.</p>\n<p>I can understand why this capability doesn’t yet exist: the obvious technical challenges of properly exposing capabilities and training the model to know when to invoke those capabilities are a textbook example of Professor Clayton Christensen’s theory of integration and modularity, wherein integration works better when a product isn’t good enough; it is only when a product exceeds expectation that there is room for standardization and modularity. To that end, ChatGPT is only now getting the capability to generate an image without the mode being selected for it: I expect the ability to seek out less obvious tools will be fairly difficult.</p>\n<p>In fact, it’s possible that the entire plug-in/GPT approach ends up being a dead-end; towards the end of the keynote Romain Huet, the head of developer experience at OpenAI, explicitly demonstrated ChatGPT programming a computer. The scenario was splitting the tab for an Airbnb in Paris:</p>\n<div align=\"center\"><div id=\"v-En2RPQZh-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/En2RPQZh?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div>\n<blockquote><p>\n  Code Interpreter is now available today in the API as well. That gives the AI the ability to write and generate code on the file, or even to generate files. So let’s see that in action. If I say here, “Hey, we’ll be 4 friends staying at this Airbnb, what’s my share of it plus my flights?”</p>\n<p>  Now here what’s happening is that Code Interpreter noticed that it should write some code to answer this query so now it’s computing the number of days in Paris, the number of friends, it’s also doing some exchange rate calculation behind the scene to get this answer for us. Not the most complex math, but you get the picture: imagine you’re building a very complex finance app that’s counting countless numbers, plotting charts, really any tasks you might tackle with code, then Code Interpreter will work great.\n</p></blockquote>\n<p>Uhm, what tasks do you <em>not</em> tackle with code? To be fair, Huet is referring to fairly simple math-oriented tasks, not the wholesale recreation of every app on the Internet, but it is interesting to consider for which problems ChatGPT will gain the wisdom to choose the right tool, and for which it will simply brute force a new solution; the history of computing would actually give the latter a higher probability: there are a lot of problems that were solved less with clever algorithms and more with the <a href=\"https://stratechery.com/2023/china-chips-and-moores-law/\">application of Moore’s Law</a>.</p>\n<h3>Consumers and Hardware</h3>\n<p>Speaking of the first year of Stratechery, that is when I first wrote about integration and modularization, in <a href=\"https://stratechery.com/2013/clayton-christensen-got-wrong/\">What Clayton Christensen Got Wrong</a>; as the title suggests I didn’t think the theory was universal:</p>\n<blockquote><p>\n  Christensen himself laid out his theory’s primary flaw in the first quote excerpted above (from 2006):</p>\n<blockquote><p>\n    You also see it in aircrafts and software, and medical devices, and over and over.\n  </p></blockquote>\n<p>  That is the problem: Consumers don’t buy aircraft, software, or medical devices. Businesses do.</p>\n<p>  Christensen’s theory is based on examples drawn from buying decisions made by businesses, not consumers. The reason this matters is that the theory of low-end disruption presumes:</p>\n<ul>\n<li>Buyers are rational</li>\n<li>Every attribute that matters can be documented and measured</li>\n<li>Modular providers can become “good enough” on all the attributes that matter to the buyers</li>\n</ul>\n<p>  All three of the assumptions fail in the consumer market, and this, ultimately, is why Christensen’s theory fails as well. Let me take each one in turn:\n</p></blockquote>\n<p>To summarize the argument, consumers care about things in ways that are inconsistent with whatever price you might attach to their utility, they prioritize ease-of-use, and they care about the quality of the user experience and are thus especially bothered by the seams inherent in a modular solution. This means that integrated solutions win because nothing is ever “good enough”; as I noted in the context of Amazon, <a href=\"https://stratechery.com/2018/divine-discontent-disruptions-antidote/\">Divine Discontent is Disruption’s Antidote</a>:</p>\n<blockquote><p>\n  Bezos’s letter, though, reveals another advantage of focusing on customers: it makes it impossible to overshoot. When I wrote that piece five years ago, I was thinking of the opportunity provided by a focus on the user experience as if it were an asymptote: one could get ever closer to the ultimate user experience, but never achieve it:</p>\n<p>  <a href=\"https://stratechery.com/2018/divine-discontent-disruptions-antidote/\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?resize=640%2C389&amp;ssl=1\" alt=\"A drawing of The Asymptote Version of the User Experience\" width=\"640\" height=\"389\" class=\"alignnone size-full wp-image-3453\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?w=1300&amp;ssl=1 1300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?resize=300%2C183&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?resize=768%2C467&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?resize=1024%2C623&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?resize=1035%2C630&amp;ssl=1 1035w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>  In fact, though, consumer expectations are not static: they are, as Bezos’ memorably states, “divinely discontent”. What is amazing today is table stakes tomorrow, and, perhaps surprisingly, that makes for a tremendous business opportunity: if your company is predicated on delivering the best possible experience for consumers, then your company will never achieve its goal.</p>\n<p>  <a href=\"https://stratechery.com/2018/divine-discontent-disruptions-antidote/\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?resize=640%2C389&amp;ssl=1\" alt=\"A drawing of The Ever-Changing Version of the User Experience\" width=\"640\" height=\"389\" class=\"alignnone size-full wp-image-3452\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?w=1300&amp;ssl=1 1300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?resize=300%2C183&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?resize=768%2C467&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?resize=1024%2C623&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?resize=1035%2C630&amp;ssl=1 1035w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>  In the case of Amazon, that this unattainable and ever-changing objective is embedded in the company’s culture is, in conjunction with the company’s demonstrated ability to spin up new businesses on the profits of established ones, a sort of perpetual motion machine.\n</p></blockquote>\n<p>I see no reason why both Articles wouldn’t apply to ChatGPT: while I might make the argument that hallucination is, in a certain light, a feature not a bug, the fact of the matter is that a lot of people use ChatGPT for information despite the fact it has a well-documented flaw when it comes to the truth; that flaw is acceptable, because to the customer ease-of-use is worth the loss of accuracy. Or look at plug-ins: the concept as originally implemented has already been abandoned, because the complexity in the user interface was more detrimental than whatever utility might have been possible. It seems likely this pattern will continue: of course customers will <em>say</em> that they want accuracy and 3rd-party tools; their actions will continue to demonstrate that convenience and ease-of-use matter most.</p>\n<p>This has two implications. First, while this may have been OpenAI’s first developer conference, I remain unconvinced that OpenAI is going to ever be a true developer-focused company. I think that was Altman’s plan, but reality in the form of ChatGPT intervened: ChatGPT is the most important consumer-facing product since the iPhone, making OpenAI <a href=\"https://stratechery.com/2023/the-accidental-consumer-tech-company-chatgpt-meta-and-product-market-fit-aggregation-and-apis/\">The Accidental Consumer Tech Company</a>. That, by extension, means that integration will continue to matter more than modularization, which is great for Microsoft’s compute stack and maybe less exciting for developers.</p>\n<p>Second, there remains one massive patch of friction in using ChatGPT; from <a href=\"https://stratechery.com/2023/ai-hardware-and-virtual-reality/\">AI, Hardware, and Virtual Reality</a>:</p>\n<blockquote><p>\n  AI is truly something new and revolutionary and capable of being something more than just a homework aid, but I don’t think the existing interfaces are the right ones. Talking to ChatGPT is better than typing, but I still have to launch the app and set the mode; vision is an amazing capability, but it requires even more intent and friction to invoke. I could see a scenario where Meta’s AI is inferior technically to OpenAI, but more useful simply because it comes in a better form factor.\n</p></blockquote>\n<p>After highlighting some news stories about OpenAI potentially partnering with Jony Ive to build hardware, I concluded:</p>\n<blockquote><p>\n  There are obviously many steps before a potential hardware product, including actually agreeing to build one. And there is, of course, the fact that Apple and Google already make devices everyone carries, with the latter in particular investing heavily in its own AI capabilities; betting on the hardware in market winning the hardware opportunity in AI is the safest bet. That may not be a reason for either OpenAI or Meta to abandon their efforts, though: waging a hardware battle against Google and Apple would be difficult, but it might be even worse to be “just an app” if the full realization of AI’s capabilities depend on fully removing human friction from the process.\n</p></blockquote>\n<p>This is the implication of a Universal Interface, which ChatGPT is striving to be: it also requires universal access, and that will always be a challenge for any company that is “just an app.” Yes, as I noted, the odds seem long, thanks to Apple and Google’s dominance, but I think there is an outside chance that the paradigm-shifting keynote is only just beginning its comeback.</p>\n\n\t\t\t",
      "contentText": "\n\t\tIn 2013, when I started Stratechery, there was no bigger event than the launch of the new iPhone; its only rival was Google I/O, which is when the newest version of Android was unveiled (hardware always breaks the tie, including with Apple’s iOS introductions at WWDC). It wasn’t just that smartphones were relatively new and still adding critical features, but that the strategic decisions and ultimate fates of the platforms were still an open question. More than that, the entire future of the tech industry was clearly tied up in said platforms and their corresponding operating systems and devices; how could keynotes not be a big deal?\nFast forward a decade and the tech keynote has diminished in importance and, in the case of Apple, disappeared completely, replaced by a pre-recorded marketing video. I want to be mad about it, but it makes sense: an iPhone introduction has been diminished not by Apple’s presentation, but rather Apple’s presentations reflect the reality that the most important questions around an iPhone are about marketing tactics. How do you segment the iPhone line? How do you price? What sort of brand affinity are you seeking to build? There, I just summarized the iPhone 15 introduction, and the reality that the smartphone era — The End of the Beginning — is over as far as strategic considerations are concerned. iOS and Android are a given, but what is next and yet unknown?\nThe answer is, clearly, AI, but even there, the energy seems muted: Apple hasn’t talked about generative AI other than to assure investors on earnings calls that they are working on it; Google I/O was of course about AI, but mostly in the context of Google’s own products — few of which have actually shipped — and my Article at the time was quickly side-tracked into philosophical discussions about both the nature of AI innovation (sustaining versus disruptive), the question of tech revolution versus alignment, and a preview of the coming battles of regulation that arrived with last week’s Executive Order on AI.\nMeta’s Connect keynote was much more interesting: not only were AI characters being added to Meta’s social networks, but next year you will be able to take AI with you via Smart Glasses (I told you hardware was interesting!). Nothing, though, seemed to match the energy around yesterday’s OpenAI developer conference, their first ever: there is nothing more interesting in tech than a consumer product with product-market fit. And that, for me, is enough to bring back an old Stratechery standby: the keynote day-after.\nKeynote Metaphysics and GPT-4 Turbo\nThis was, first and foremost, a really good keynote, in the keynote-as-artifact sense. CEO Sam Altman, in a humorous exchange with Microsoft CEO Satya Nadella, promised, “I won’t take too much of your time”; never mind that Nadella was presumably in San Francisco just for this event: in this case he stood in for the audience who witnessed a presentation that was tight, with content that was interesting, leaving them with a desire to learn more.\nAltman himself had a good stage presence, with the sort of nervous energy that is only present in a live keynote; the fact he never seemed to know which side of the stage a fellow presenter was coming from was humanizing. Meanwhile, the live demos not only went off without a hitch, but leveraged the fact that they were live: in one instance a presenter instructed a GPT she created to text Altman; he held up his phone to show he got the message. In another a GPT randomly selected five members of the audience to receive $500 in OpenAI API credits, only to then extend it to everyone.\nNew products and features, meanwhile, were available “today”, not weeks or months in the future, as is increasingly the case for events like I/O or WWDC; everything combined to give a palpable sense of progress and excitement, which, when it comes to AI, is mostly true.\nGPT-4 Turbo is an excellent example of what I mean by “mostly”. The API consists of six new features:\n\nIncreased context length\nMore control, specifically in terms of model inputs and outputs\nBetter knowledge, which both means updating the cut-off date for knowledge about the world to April 2023 and providing the ability for developers to easily add their own knowledge base\nNew modalities, as DALL-E 3, Vision, and TTS (text-to-speech) will all be included in the API, with a new version of Whisper speech recognition coming.\nCustomization, including fine-tuning, and custom models (which, Altman warned, won’t be cheap)\nHigher rate limits\n\nThis is, to be clear, still the same foundational model (GPT-4); these features just make the API more usable, both in terms of features and also performance. It also speaks to how OpenAI is becoming more of a product company, with iterative enhancements of its core functionality. Yes, the mission still remains AGI (artificial general intelligence), and the core scientific team is almost certainly working on GPT-5, but Altman and team aren’t just tossing models over the wall for the rest of the industry to figure out.\nPrice and Microsoft\nThe next “feature” was tied into the GPT-4 Turbo introduction: the API is getting cheaper (3x cheaper for input tokens, and 2x cheaper for output tokens). Unsurprisingly this announcement elicited cheers from the developers in attendance; what I cheered as an analyst was Altman’s clear articulation of the company’s priorities: lower price first, speed later. You can certainly debate whether that is the right set of priorities (I think it is, because the biggest need now is for increased experimentation, not optimization), but what I appreciated was the clarity.\nIt’s also appropriate that the segment after that was the brief “interview” with Nadella: OpenAI’s pricing is ultimately a function of Microsoft’s ability to build the infrastructure to support that pricing. Nadella actually explained how Microsoft is accomplishing that on the company’s most recent earnings call:\n\n  It is true that the approach we have taken is a full stack approach all the way from whether it’s ChatGPT or Bing Chat or all our Copilots, all share the same model. So in some sense, one of the things that we do have is very, very high leverage of the one model that we used, which we trained, and then the one model that we are doing inferencing at scale. And that advantage sort of trickles down all the way to both utilization internally, utilization of third parties, and also over time, you can see the sort of stack optimization all the way to the silicon, because the abstraction layer to which the developers are riding is much higher up than low-level kernels, if you will.\n  So, therefore, I think there is a fundamental approach we took, which was a technical approach of saying we’ll have Copilots and Copilot stack all available. That doesn’t mean we don’t have people doing training for open source models or proprietary models. We also have a bunch of open source models. We have a bunch of fine-tuning happening, a bunch of RLHF happening. So there’s all kinds of ways people use it. But the thing is, we have scale leverage of one large model that was trained and one large model that’s being used for inference across all our first-party SaaS apps, as well as our API in our Azure AI service…\n  The lesson learned from the cloud side is — we’re not running a conglomerate of different businesses, it’s all one tech stack up and down Microsoft’s portfolio, and that, I think, is going to be very important because that discipline, given what the spend like — it will look like for this AI transition any business that’s not disciplined about their capital spend accruing across all their businesses could run into trouble.\n\nThe fact that Microsoft is benefiting from OpenAI is obvious; what this makes clear is that OpenAI uniquely benefits from Microsoft as well, in a way they would not from another cloud provider: because Microsoft is also a product company investing in the infrastructure to run OpenAI’s models for said products, it can afford to optimize and invest ahead of usage in a way that OpenAI alone, even with the support of another cloud provider, could not. In this case that is paying off in developers needing to pay less, or, ideally, have more latitude to discover use cases that result in them paying far more because usage is exploding.\nGPTs and Computers\nI mentioned GPTs before; you were probably confused, because this is a name that is either brilliant or a total disaster. Of course you could have said the same about ChatGPT: massive consumer uptake has a way of making arguably poor choices great ones in retrospect, and I can see why OpenAI is seeking to basically brand “GPT” — generative pre-trained transformer — as an OpenAI chatbot.\nRegardless, this was how Altman explains GPTs:\n\n\n  GPTs are tailored version of ChatGPT for a specific purpose. You can build a GPT — a customized version of ChatGPT — for almost anything, with instructions, expanded knowledge, and actions, and then you can publish it for others to use. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. They’ll make it easier for you accomplish all sorts of tasks or just have more fun, and you’ll be able to use them right within ChatGPT. You can, in effect, program a GPT, with language, just by talking to it. It’s easy to customize the behavior so that it fits what you want. This makes building them very accessible, and it gives agency to everyone.\n  We’re going to show you what GPTs are, how to use them, how to build them, and then we’re going to talk about how they’ll be distributed and discovered. And then after that, for developers, we’re going to show you how to build these agent-like experiences into your own apps.\n\nAltman’s examples included a lesson-planning GPT from Code.org and a natural language vision design GPT from Canva. As Altman noted, the second example might have seemed familiar: Canva had a plugin for ChatGPT, and Altman explained that “we’ve evolved our plugins to be custom actions for GPTs.”\nI found the plugin concept fascinating and a useful way to understand both the capabilities and limits of large language models; I wrote in ChatGPT Gets a Computer:\n\n  The implication of this approach is that computers are deterministic: if circuit X is open, then the proposition represented by X is true; 1 plus 1 is always 2; clicking “back” on your browser will exit this page. There are, of course, a huge number of abstractions and massive amounts of logic between an individual transistor and any action we might take with a computer — and an effectively infinite number of places for bugs — but the appropriate mental model for a computer is that they do exactly what they are told (indeed, a bug is not the computer making a mistake, but rather a manifestation of the programmer telling the computer to do the wrong thing)…\n\nLarge language models, though, with their probabilistic approach, are in many domains shockingly intuitive, and yet can hallucinate and are downright terrible at math; that is why the most compelling plug-in OpenAI launched was from Wolfram|Alpha. Stephen Wolfram explained:\n\n  For decades there’s been a dichotomy in thinking about AI between “statistical approaches” of the kind ChatGPT uses, and “symbolic approaches” that are in effect the starting point for Wolfram|Alpha. But now—thanks to the success of ChatGPT—as well as all the work we’ve done in making Wolfram|Alpha understand natural language—there’s finally the opportunity to combine these to make something much stronger than either could ever achieve on their own.\n\nThat is the exact combination that happened, which led to the title of that Article:\n\n  The fact this works so well is itself a testament to what Assistant AI’s are, and are not: they are not computing as we have previously understood it; they are shockingly human in their way of “thinking” and communicating. And frankly, I would have had a hard time solving those three questions as well — that’s what computers are for! And now ChatGPT has a computer of its own.\n\nI still think the concept was incredibly elegant, but there was just one problem: the user interface was terrible. You had to get a plugin from the “marketplace”, then pre-select it before you began a conversation, and only then would you get workable results after a too-long process where ChatGPT negotiated with the plugin provider in question on the answer.\nThis new model somewhat alleviates the problem: now, instead of having to select the correct plug-in (and thus restart your chat), you simply go directly to the GPT in question. In other words, if I want to create a poster, I don’t enable the Canva plugin in ChatGPT, I go to Canva GPT in the sidebar. Notice that this doesn’t actually solve the problem of needing to have selected the right tool; what it does do is make the choice more apparent to the user at a more appropriate stage in the process, and that’s no small thing. I also suspect that GPTs will be much faster than plug-ins, given they are integrated from the get-go. Finally, standalone GPTs are a much better fit with the store model that OpenAI is trying to develop.\nStill, there is a better way: Altman demoed it.\nChatGPT and the Universal Interface\nBefore Altman introduced the aforementioned GPTs he talked about improvements to ChatGPT:\n\n\n  Even though this is a developer conference, we can’t help resist making some improvements to ChatGPT. A small one, ChatGPT now uses GPT-4 Turbo, with all of the latest improvements, including the latest cut-off, which we’ll continue to update — that’s all live today. It can now browse the web when it needs to, write and run code, analyze data, generate images, and much more, and we heard your feedback that that model picker was extremely annoying: that is gone, starting today. You will not have to click around a drop-down menu. All of this will just work together. ChatGPT will just know what to use and when you need it. But that’s not the main thing.\n\nYou may wonder why I put this section after GPTs, given they were, according to Altman, the main thing: it’s because I think this feature enhancement is actually much more important. As I just noted, GPTs are a somewhat better UI on an elegant plugin concept, in which a probabilisitic large language model gets access to a deterministic computer. The best UI, though, is no UI at all, or rather, just one UI, by which I mean “Universal Interface”.\nIn this case “browsing” or “image generation” are basically plug-ins: they are specialized capabilities that, before today, you had to explicitly invoke; going forward they will just work. ChatGPT will seamlessly switch between text generation, image generation, and web browsing, without the user needing to change context. What is necessary for the plug-in/GPT idea to ultimately take root is for the same capabilities to be extended broadly: if my conversation involved math, ChatGPT should know to use Wolfram|Alpha on its own, without me adding the plug-in or going to a specialized GPT.\nI can understand why this capability doesn’t yet exist: the obvious technical challenges of properly exposing capabilities and training the model to know when to invoke those capabilities are a textbook example of Professor Clayton Christensen’s theory of integration and modularity, wherein integration works better when a product isn’t good enough; it is only when a product exceeds expectation that there is room for standardization and modularity. To that end, ChatGPT is only now getting the capability to generate an image without the mode being selected for it: I expect the ability to seek out less obvious tools will be fairly difficult.\nIn fact, it’s possible that the entire plug-in/GPT approach ends up being a dead-end; towards the end of the keynote Romain Huet, the head of developer experience at OpenAI, explicitly demonstrated ChatGPT programming a computer. The scenario was splitting the tab for an Airbnb in Paris:\n\n\n  Code Interpreter is now available today in the API as well. That gives the AI the ability to write and generate code on the file, or even to generate files. So let’s see that in action. If I say here, “Hey, we’ll be 4 friends staying at this Airbnb, what’s my share of it plus my flights?”\n  Now here what’s happening is that Code Interpreter noticed that it should write some code to answer this query so now it’s computing the number of days in Paris, the number of friends, it’s also doing some exchange rate calculation behind the scene to get this answer for us. Not the most complex math, but you get the picture: imagine you’re building a very complex finance app that’s counting countless numbers, plotting charts, really any tasks you might tackle with code, then Code Interpreter will work great.\n\nUhm, what tasks do you not tackle with code? To be fair, Huet is referring to fairly simple math-oriented tasks, not the wholesale recreation of every app on the Internet, but it is interesting to consider for which problems ChatGPT will gain the wisdom to choose the right tool, and for which it will simply brute force a new solution; the history of computing would actually give the latter a higher probability: there are a lot of problems that were solved less with clever algorithms and more with the application of Moore’s Law.\nConsumers and Hardware\nSpeaking of the first year of Stratechery, that is when I first wrote about integration and modularization, in What Clayton Christensen Got Wrong; as the title suggests I didn’t think the theory was universal:\n\n  Christensen himself laid out his theory’s primary flaw in the first quote excerpted above (from 2006):\n\n    You also see it in aircrafts and software, and medical devices, and over and over.\n  \n  That is the problem: Consumers don’t buy aircraft, software, or medical devices. Businesses do.\n  Christensen’s theory is based on examples drawn from buying decisions made by businesses, not consumers. The reason this matters is that the theory of low-end disruption presumes:\n\nBuyers are rational\nEvery attribute that matters can be documented and measured\nModular providers can become “good enough” on all the attributes that matter to the buyers\n\n  All three of the assumptions fail in the consumer market, and this, ultimately, is why Christensen’s theory fails as well. Let me take each one in turn:\n\nTo summarize the argument, consumers care about things in ways that are inconsistent with whatever price you might attach to their utility, they prioritize ease-of-use, and they care about the quality of the user experience and are thus especially bothered by the seams inherent in a modular solution. This means that integrated solutions win because nothing is ever “good enough”; as I noted in the context of Amazon, Divine Discontent is Disruption’s Antidote:\n\n  Bezos’s letter, though, reveals another advantage of focusing on customers: it makes it impossible to overshoot. When I wrote that piece five years ago, I was thinking of the opportunity provided by a focus on the user experience as if it were an asymptote: one could get ever closer to the ultimate user experience, but never achieve it:\n  \n  In fact, though, consumer expectations are not static: they are, as Bezos’ memorably states, “divinely discontent”. What is amazing today is table stakes tomorrow, and, perhaps surprisingly, that makes for a tremendous business opportunity: if your company is predicated on delivering the best possible experience for consumers, then your company will never achieve its goal.\n  \n  In the case of Amazon, that this unattainable and ever-changing objective is embedded in the company’s culture is, in conjunction with the company’s demonstrated ability to spin up new businesses on the profits of established ones, a sort of perpetual motion machine.\n\nI see no reason why both Articles wouldn’t apply to ChatGPT: while I might make the argument that hallucination is, in a certain light, a feature not a bug, the fact of the matter is that a lot of people use ChatGPT for information despite the fact it has a well-documented flaw when it comes to the truth; that flaw is acceptable, because to the customer ease-of-use is worth the loss of accuracy. Or look at plug-ins: the concept as originally implemented has already been abandoned, because the complexity in the user interface was more detrimental than whatever utility might have been possible. It seems likely this pattern will continue: of course customers will say that they want accuracy and 3rd-party tools; their actions will continue to demonstrate that convenience and ease-of-use matter most.\nThis has two implications. First, while this may have been OpenAI’s first developer conference, I remain unconvinced that OpenAI is going to ever be a true developer-focused company. I think that was Altman’s plan, but reality in the form of ChatGPT intervened: ChatGPT is the most important consumer-facing product since the iPhone, making OpenAI The Accidental Consumer Tech Company. That, by extension, means that integration will continue to matter more than modularization, which is great for Microsoft’s compute stack and maybe less exciting for developers.\nSecond, there remains one massive patch of friction in using ChatGPT; from AI, Hardware, and Virtual Reality:\n\n  AI is truly something new and revolutionary and capable of being something more than just a homework aid, but I don’t think the existing interfaces are the right ones. Talking to ChatGPT is better than typing, but I still have to launch the app and set the mode; vision is an amazing capability, but it requires even more intent and friction to invoke. I could see a scenario where Meta’s AI is inferior technically to OpenAI, but more useful simply because it comes in a better form factor.\n\nAfter highlighting some news stories about OpenAI potentially partnering with Jony Ive to build hardware, I concluded:\n\n  There are obviously many steps before a potential hardware product, including actually agreeing to build one. And there is, of course, the fact that Apple and Google already make devices everyone carries, with the latter in particular investing heavily in its own AI capabilities; betting on the hardware in market winning the hardware opportunity in AI is the safest bet. That may not be a reason for either OpenAI or Meta to abandon their efforts, though: waging a hardware battle against Google and Apple would be difficult, but it might be even worse to be “just an app” if the full realization of AI’s capabilities depend on fully removing human friction from the process.\n\nThis is the implication of a Universal Interface, which ChatGPT is striving to be: it also requires universal access, and that will always be a challenge for any company that is “just an app.” Yes, as I noted, the odds seem long, thanks to Apple and Google’s dominance, but I think there is an outside chance that the paradigm-shifting keynote is only just beginning its comeback.\n\n\t\t\t",
      "subsections": [
        {
          "subtitle": "Keynote Metaphysics and GPT-4 Turbo",
          "contentHtml": "<p>This was, first and foremost, a really <em>good</em> keynote, in the keynote-as-artifact sense. CEO Sam Altman, in a humorous exchange with Microsoft CEO Satya Nadella, promised, “I won’t take too much of your time”; never mind that Nadella was presumably in San Francisco just for this event: in this case he stood in for the audience who witnessed a presentation that was tight, with content that was interesting, leaving them with a desire to learn more.</p><p>Altman himself had a good stage presence, with the sort of nervous energy that is only present in a live keynote; the fact he never seemed to know which side of the stage a fellow presenter was coming from was humanizing. Meanwhile, the live demos not only went off without a hitch, but leveraged the fact that they were live: in one instance a presenter instructed a GPT she created to text Altman; he held up his phone to show he got the message. In another a GPT randomly selected five members of the audience to receive $500 in OpenAI API credits, only to then extend it to everyone.</p><p>New products and features, meanwhile, were available “today”, not weeks or months in the future, as is increasingly the case for events like I/O or WWDC; everything combined to give a palpable sense of progress and excitement, which, when it comes to AI, is mostly true.</p><p>GPT-4 Turbo is an excellent example of what I mean by “mostly”. The API consists of six new features:</p><ul>\n<li>Increased context length</li>\n<li>More control, specifically in terms of model inputs and outputs</li>\n<li>Better knowledge, which both means updating the cut-off date for knowledge about the world to April 2023 and providing the ability for developers to easily add their own knowledge base</li>\n<li>New modalities, as DALL-E 3, Vision, and TTS (text-to-speech) will all be included in the API, with a new version of Whisper speech recognition coming.</li>\n<li>Customization, including fine-tuning, and custom models (which, Altman warned, won’t be cheap)</li>\n<li>Higher rate limits</li>\n</ul><p>This is, to be clear, still the same foundational model (GPT-4); these features just make the API more usable, both in terms of features and also performance. It also speaks to how OpenAI is becoming more of a product company, with iterative enhancements of its core functionality. Yes, the mission still remains AGI (artificial general intelligence), and the core scientific team is almost certainly working on GPT-5, but Altman and team aren’t just tossing models over the wall for the rest of the industry to figure out.</p>",
          "contentText": "This was, first and foremost, a really good keynote, in the keynote-as-artifact sense. CEO Sam Altman, in a humorous exchange with Microsoft CEO Satya Nadella, promised, “I won’t take too much of your time”; never mind that Nadella was presumably in San Francisco just for this event: in this case he stood in for the audience who witnessed a presentation that was tight, with content that was interesting, leaving them with a desire to learn more.Altman himself had a good stage presence, with the sort of nervous energy that is only present in a live keynote; the fact he never seemed to know which side of the stage a fellow presenter was coming from was humanizing. Meanwhile, the live demos not only went off without a hitch, but leveraged the fact that they were live: in one instance a presenter instructed a GPT she created to text Altman; he held up his phone to show he got the message. In another a GPT randomly selected five members of the audience to receive $500 in OpenAI API credits, only to then extend it to everyone.New products and features, meanwhile, were available “today”, not weeks or months in the future, as is increasingly the case for events like I/O or WWDC; everything combined to give a palpable sense of progress and excitement, which, when it comes to AI, is mostly true.GPT-4 Turbo is an excellent example of what I mean by “mostly”. The API consists of six new features:\nIncreased context length\nMore control, specifically in terms of model inputs and outputs\nBetter knowledge, which both means updating the cut-off date for knowledge about the world to April 2023 and providing the ability for developers to easily add their own knowledge base\nNew modalities, as DALL-E 3, Vision, and TTS (text-to-speech) will all be included in the API, with a new version of Whisper speech recognition coming.\nCustomization, including fine-tuning, and custom models (which, Altman warned, won’t be cheap)\nHigher rate limits\nThis is, to be clear, still the same foundational model (GPT-4); these features just make the API more usable, both in terms of features and also performance. It also speaks to how OpenAI is becoming more of a product company, with iterative enhancements of its core functionality. Yes, the mission still remains AGI (artificial general intelligence), and the core scientific team is almost certainly working on GPT-5, but Altman and team aren’t just tossing models over the wall for the rest of the industry to figure out."
        },
        {
          "subtitle": "Price and Microsoft",
          "contentHtml": "<p>The next “feature” was tied into the GPT-4 Turbo introduction: the API is getting cheaper (3x cheaper for input tokens, and 2x cheaper for output tokens). Unsurprisingly this announcement elicited cheers from the developers in attendance; what I cheered as an analyst was Altman’s clear articulation of the company’s priorities: lower price first, speed later. You can certainly debate whether that is the right set of priorities (I think it is, because the biggest need now is for increased experimentation, not optimization), but what I appreciated was the clarity.</p><p>It’s also appropriate that the segment after that was the brief “interview” with Nadella: OpenAI’s pricing is ultimately a function of Microsoft’s ability to build the infrastructure to support that pricing. Nadella actually explained how Microsoft is accomplishing that on the <a href=\"https://seekingalpha.com/article/4643129-microsoft-corporation-msft-q1-2024-earnings-call-transcript\">company’s most recent earnings call</a>:</p><blockquote><p>\n  It is true that the approach we have taken is a full stack approach all the way from whether it’s ChatGPT or Bing Chat or all our Copilots, all share the same model. So in some sense, one of the things that we do have is very, very high leverage of the one model that we used, which we trained, and then the one model that we are doing inferencing at scale. And that advantage sort of trickles down all the way to both utilization internally, utilization of third parties, and also over time, you can see the sort of stack optimization all the way to the silicon, because the abstraction layer to which the developers are riding is much higher up than low-level kernels, if you will.</p>\n<p>  So, therefore, I think there is a fundamental approach we took, which was a technical approach of saying we’ll have Copilots and Copilot stack all available. That doesn’t mean we don’t have people doing training for open source models or proprietary models. We also have a bunch of open source models. We have a bunch of fine-tuning happening, a bunch of RLHF happening. So there’s all kinds of ways people use it. But the thing is, we have scale leverage of one large model that was trained and one large model that’s being used for inference across all our first-party SaaS apps, as well as our API in our Azure AI service…</p>\n<p>  The lesson learned from the cloud side is — we’re not running a conglomerate of different businesses, it’s all one tech stack up and down Microsoft’s portfolio, and that, I think, is going to be very important because that discipline, given what the spend like — it will look like for this AI transition any business that’s not disciplined about their capital spend accruing across all their businesses could run into trouble.\n</p></blockquote><p>The fact that Microsoft is benefiting from OpenAI is obvious; what this makes clear is that OpenAI uniquely benefits from Microsoft as well, in a way they would not from another cloud provider: <a href=\"https://stratechery.com/2023/google-earnings-microsoft-earnings-ai-leverage/\">because Microsoft is also a product company investing in the infrastructure to run OpenAI’s models for said products</a>, it can afford to optimize and invest ahead of usage in a way that OpenAI alone, even with the support of another cloud provider, could not. In this case that is paying off in developers needing to pay less, or, ideally, have more latitude to discover use cases that result in them paying far more because usage is exploding.</p>",
          "contentText": "The next “feature” was tied into the GPT-4 Turbo introduction: the API is getting cheaper (3x cheaper for input tokens, and 2x cheaper for output tokens). Unsurprisingly this announcement elicited cheers from the developers in attendance; what I cheered as an analyst was Altman’s clear articulation of the company’s priorities: lower price first, speed later. You can certainly debate whether that is the right set of priorities (I think it is, because the biggest need now is for increased experimentation, not optimization), but what I appreciated was the clarity.It’s also appropriate that the segment after that was the brief “interview” with Nadella: OpenAI’s pricing is ultimately a function of Microsoft’s ability to build the infrastructure to support that pricing. Nadella actually explained how Microsoft is accomplishing that on the company’s most recent earnings call:\n  It is true that the approach we have taken is a full stack approach all the way from whether it’s ChatGPT or Bing Chat or all our Copilots, all share the same model. So in some sense, one of the things that we do have is very, very high leverage of the one model that we used, which we trained, and then the one model that we are doing inferencing at scale. And that advantage sort of trickles down all the way to both utilization internally, utilization of third parties, and also over time, you can see the sort of stack optimization all the way to the silicon, because the abstraction layer to which the developers are riding is much higher up than low-level kernels, if you will.\n  So, therefore, I think there is a fundamental approach we took, which was a technical approach of saying we’ll have Copilots and Copilot stack all available. That doesn’t mean we don’t have people doing training for open source models or proprietary models. We also have a bunch of open source models. We have a bunch of fine-tuning happening, a bunch of RLHF happening. So there’s all kinds of ways people use it. But the thing is, we have scale leverage of one large model that was trained and one large model that’s being used for inference across all our first-party SaaS apps, as well as our API in our Azure AI service…\n  The lesson learned from the cloud side is — we’re not running a conglomerate of different businesses, it’s all one tech stack up and down Microsoft’s portfolio, and that, I think, is going to be very important because that discipline, given what the spend like — it will look like for this AI transition any business that’s not disciplined about their capital spend accruing across all their businesses could run into trouble.\nThe fact that Microsoft is benefiting from OpenAI is obvious; what this makes clear is that OpenAI uniquely benefits from Microsoft as well, in a way they would not from another cloud provider: because Microsoft is also a product company investing in the infrastructure to run OpenAI’s models for said products, it can afford to optimize and invest ahead of usage in a way that OpenAI alone, even with the support of another cloud provider, could not. In this case that is paying off in developers needing to pay less, or, ideally, have more latitude to discover use cases that result in them paying far more because usage is exploding."
        },
        {
          "subtitle": "GPTs and Computers",
          "contentHtml": "<p>I mentioned GPTs before; you were probably confused, because this is a name that is either brilliant or a total disaster. Of course you could have said the same about ChatGPT: massive consumer uptake has a way of making arguably poor choices great ones in retrospect, and I can see why OpenAI is seeking to basically brand “GPT” — generative pre-trained transformer — as an OpenAI chatbot.</p><p>Regardless, this was how Altman explains GPTs:</p><div align=\"center\"><div id=\"v-0EkKoygR-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/0EkKoygR?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div><blockquote><p>\n  GPTs are tailored version of ChatGPT for a specific purpose. You can build a GPT — a customized version of ChatGPT — for almost anything, with instructions, expanded knowledge, and actions, and then you can publish it for others to use. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. They’ll make it easier for you accomplish all sorts of tasks or just have more fun, and you’ll be able to use them right within ChatGPT. You can, in effect, program a GPT, with language, just by talking to it. It’s easy to customize the behavior so that it fits what you want. This makes building them very accessible, and it gives agency to everyone.</p>\n<p>  We’re going to show you what GPTs are, how to use them, how to build them, and then we’re going to talk about how they’ll be distributed and discovered. And then after that, for developers, we’re going to show you how to build these agent-like experiences into your own apps.\n</p></blockquote><p>Altman’s examples included a lesson-planning GPT from Code.org and a natural language vision design GPT from Canva. As Altman noted, the second example might have seemed familiar: Canva had a plugin for ChatGPT, and Altman explained that “we’ve evolved our plugins to be custom actions for GPTs.”</p><p>I found the plugin concept fascinating and a useful way to understand both the capabilities and limits of large language models; I wrote in <a href=\"https://stratechery.com/2023/chatgpt-learns-computing/\">ChatGPT Gets a Computer</a>:</p><blockquote><p>\n  The implication of this approach is that computers are deterministic: if circuit X is open, then the proposition represented by X is true; 1 plus 1 is always 2; clicking “back” on your browser will exit this page. There are, of course, a huge number of abstractions and massive amounts of logic between an individual transistor and any action we might take with a computer — and an effectively infinite number of places for bugs — but the appropriate mental model for a computer is that they do exactly what they are told (indeed, a bug is not the computer making a mistake, but rather a manifestation of the programmer telling the computer to do the wrong thing)…\n</p></blockquote><p>Large language models, though, with their probabilistic approach, are in many domains shockingly intuitive, and yet can hallucinate and are downright terrible at math; that is why the most compelling plug-in OpenAI launched was from Wolfram|Alpha. Stephen Wolfram <a href=\"https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/\">explained</a>:</p><blockquote><p>\n  For decades there’s been a dichotomy in thinking about AI between “statistical approaches” of the kind ChatGPT uses, and “symbolic approaches” that are in effect the starting point for Wolfram|Alpha. But now—thanks to the success of ChatGPT—as well as all the work we’ve done in making Wolfram|Alpha understand natural language—there’s finally the opportunity to combine these to make something much stronger than either could ever achieve on their own.\n</p></blockquote><p>That is the exact combination that happened, which led to the title of that Article:</p><blockquote><p>\n  The fact this works so well is itself a testament to what Assistant AI’s are, and are not: they are not computing as we have previously understood it; they are shockingly human in their way of “thinking” and communicating. And frankly, I would have had a hard time solving those three questions as well — that’s what computers are for! And now ChatGPT has a computer of its own.\n</p></blockquote><p>I still think the concept was incredibly elegant, but there was just one problem: the user interface was terrible. You had to get a plugin from the “marketplace”, then pre-select it before you began a conversation, and only then would you get workable results after a too-long process where ChatGPT negotiated with the plugin provider in question on the answer.</p><p>This new model somewhat alleviates the problem: now, instead of having to select the correct plug-in (and thus restart your chat), you simply go directly to the GPT in question. In other words, if I want to create a poster, I don’t enable the Canva plugin in ChatGPT, I go to Canva GPT in the sidebar. Notice that this doesn’t actually <em>solve</em> the problem of needing to have selected the right tool; what it does do is make the choice more apparent to the user at a more appropriate stage in the process, and that’s no small thing. I also suspect that GPTs will be much faster than plug-ins, given they are integrated from the get-go. Finally, standalone GPTs are a much better fit with the store model that OpenAI is trying to develop.</p><p>Still, there is a better way: Altman demoed it.</p>",
          "contentText": "I mentioned GPTs before; you were probably confused, because this is a name that is either brilliant or a total disaster. Of course you could have said the same about ChatGPT: massive consumer uptake has a way of making arguably poor choices great ones in retrospect, and I can see why OpenAI is seeking to basically brand “GPT” — generative pre-trained transformer — as an OpenAI chatbot.Regardless, this was how Altman explains GPTs:\n  GPTs are tailored version of ChatGPT for a specific purpose. You can build a GPT — a customized version of ChatGPT — for almost anything, with instructions, expanded knowledge, and actions, and then you can publish it for others to use. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. They’ll make it easier for you accomplish all sorts of tasks or just have more fun, and you’ll be able to use them right within ChatGPT. You can, in effect, program a GPT, with language, just by talking to it. It’s easy to customize the behavior so that it fits what you want. This makes building them very accessible, and it gives agency to everyone.\n  We’re going to show you what GPTs are, how to use them, how to build them, and then we’re going to talk about how they’ll be distributed and discovered. And then after that, for developers, we’re going to show you how to build these agent-like experiences into your own apps.\nAltman’s examples included a lesson-planning GPT from Code.org and a natural language vision design GPT from Canva. As Altman noted, the second example might have seemed familiar: Canva had a plugin for ChatGPT, and Altman explained that “we’ve evolved our plugins to be custom actions for GPTs.”I found the plugin concept fascinating and a useful way to understand both the capabilities and limits of large language models; I wrote in ChatGPT Gets a Computer:\n  The implication of this approach is that computers are deterministic: if circuit X is open, then the proposition represented by X is true; 1 plus 1 is always 2; clicking “back” on your browser will exit this page. There are, of course, a huge number of abstractions and massive amounts of logic between an individual transistor and any action we might take with a computer — and an effectively infinite number of places for bugs — but the appropriate mental model for a computer is that they do exactly what they are told (indeed, a bug is not the computer making a mistake, but rather a manifestation of the programmer telling the computer to do the wrong thing)…\nLarge language models, though, with their probabilistic approach, are in many domains shockingly intuitive, and yet can hallucinate and are downright terrible at math; that is why the most compelling plug-in OpenAI launched was from Wolfram|Alpha. Stephen Wolfram explained:\n  For decades there’s been a dichotomy in thinking about AI between “statistical approaches” of the kind ChatGPT uses, and “symbolic approaches” that are in effect the starting point for Wolfram|Alpha. But now—thanks to the success of ChatGPT—as well as all the work we’ve done in making Wolfram|Alpha understand natural language—there’s finally the opportunity to combine these to make something much stronger than either could ever achieve on their own.\nThat is the exact combination that happened, which led to the title of that Article:\n  The fact this works so well is itself a testament to what Assistant AI’s are, and are not: they are not computing as we have previously understood it; they are shockingly human in their way of “thinking” and communicating. And frankly, I would have had a hard time solving those three questions as well — that’s what computers are for! And now ChatGPT has a computer of its own.\nI still think the concept was incredibly elegant, but there was just one problem: the user interface was terrible. You had to get a plugin from the “marketplace”, then pre-select it before you began a conversation, and only then would you get workable results after a too-long process where ChatGPT negotiated with the plugin provider in question on the answer.This new model somewhat alleviates the problem: now, instead of having to select the correct plug-in (and thus restart your chat), you simply go directly to the GPT in question. In other words, if I want to create a poster, I don’t enable the Canva plugin in ChatGPT, I go to Canva GPT in the sidebar. Notice that this doesn’t actually solve the problem of needing to have selected the right tool; what it does do is make the choice more apparent to the user at a more appropriate stage in the process, and that’s no small thing. I also suspect that GPTs will be much faster than plug-ins, given they are integrated from the get-go. Finally, standalone GPTs are a much better fit with the store model that OpenAI is trying to develop.Still, there is a better way: Altman demoed it."
        },
        {
          "subtitle": "ChatGPT and the Universal Interface",
          "contentHtml": "<p>Before Altman introduced the aforementioned GPTs he talked about improvements to ChatGPT:</p><div align=\"center\"><div id=\"v-ot3CmlaF-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/ot3CmlaF?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div><blockquote><p>\n  Even though this is a developer conference, we can’t help resist making some improvements to ChatGPT. A small one, ChatGPT now uses GPT-4 Turbo, with all of the latest improvements, including the latest cut-off, which we’ll continue to update — that’s all live today. It can now browse the web when it needs to, write and run code, analyze data, generate images, and much more, and we heard your feedback that that model picker was extremely annoying: that is gone, starting today. You will not have to click around a drop-down menu. All of this will just work together. ChatGPT will just know what to use and when you need it. But that’s not the main thing.\n</p></blockquote><p>You may wonder why I put this section after GPTs, given they were, according to Altman, the main thing: it’s because I think this feature enhancement is actually much more important. As I just noted, GPTs are a somewhat better UI on an elegant plugin concept, in which a probabilisitic large language model gets access to a deterministic computer. The best UI, though, is no UI at all, or rather, just one UI, by which I mean “Universal Interface”.</p><p>In this case “browsing” or “image generation” are basically plug-ins: they are specialized capabilities that, before today, you had to explicitly invoke; going forward they will just work. ChatGPT will seamlessly switch between text generation, image generation, and web browsing, without the user needing to change context. What is necessary for the plug-in/GPT idea to ultimately take root is for the same capabilities to be extended broadly: if my conversation involved math, ChatGPT should know to use Wolfram|Alpha on its own, without me adding the plug-in or going to a specialized GPT.</p><p>I can understand why this capability doesn’t yet exist: the obvious technical challenges of properly exposing capabilities and training the model to know when to invoke those capabilities are a textbook example of Professor Clayton Christensen’s theory of integration and modularity, wherein integration works better when a product isn’t good enough; it is only when a product exceeds expectation that there is room for standardization and modularity. To that end, ChatGPT is only now getting the capability to generate an image without the mode being selected for it: I expect the ability to seek out less obvious tools will be fairly difficult.</p><p>In fact, it’s possible that the entire plug-in/GPT approach ends up being a dead-end; towards the end of the keynote Romain Huet, the head of developer experience at OpenAI, explicitly demonstrated ChatGPT programming a computer. The scenario was splitting the tab for an Airbnb in Paris:</p><div align=\"center\"><div id=\"v-En2RPQZh-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/En2RPQZh?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div><blockquote><p>\n  Code Interpreter is now available today in the API as well. That gives the AI the ability to write and generate code on the file, or even to generate files. So let’s see that in action. If I say here, “Hey, we’ll be 4 friends staying at this Airbnb, what’s my share of it plus my flights?”</p>\n<p>  Now here what’s happening is that Code Interpreter noticed that it should write some code to answer this query so now it’s computing the number of days in Paris, the number of friends, it’s also doing some exchange rate calculation behind the scene to get this answer for us. Not the most complex math, but you get the picture: imagine you’re building a very complex finance app that’s counting countless numbers, plotting charts, really any tasks you might tackle with code, then Code Interpreter will work great.\n</p></blockquote><p>Uhm, what tasks do you <em>not</em> tackle with code? To be fair, Huet is referring to fairly simple math-oriented tasks, not the wholesale recreation of every app on the Internet, but it is interesting to consider for which problems ChatGPT will gain the wisdom to choose the right tool, and for which it will simply brute force a new solution; the history of computing would actually give the latter a higher probability: there are a lot of problems that were solved less with clever algorithms and more with the <a href=\"https://stratechery.com/2023/china-chips-and-moores-law/\">application of Moore’s Law</a>.</p>",
          "contentText": "Before Altman introduced the aforementioned GPTs he talked about improvements to ChatGPT:\n  Even though this is a developer conference, we can’t help resist making some improvements to ChatGPT. A small one, ChatGPT now uses GPT-4 Turbo, with all of the latest improvements, including the latest cut-off, which we’ll continue to update — that’s all live today. It can now browse the web when it needs to, write and run code, analyze data, generate images, and much more, and we heard your feedback that that model picker was extremely annoying: that is gone, starting today. You will not have to click around a drop-down menu. All of this will just work together. ChatGPT will just know what to use and when you need it. But that’s not the main thing.\nYou may wonder why I put this section after GPTs, given they were, according to Altman, the main thing: it’s because I think this feature enhancement is actually much more important. As I just noted, GPTs are a somewhat better UI on an elegant plugin concept, in which a probabilisitic large language model gets access to a deterministic computer. The best UI, though, is no UI at all, or rather, just one UI, by which I mean “Universal Interface”.In this case “browsing” or “image generation” are basically plug-ins: they are specialized capabilities that, before today, you had to explicitly invoke; going forward they will just work. ChatGPT will seamlessly switch between text generation, image generation, and web browsing, without the user needing to change context. What is necessary for the plug-in/GPT idea to ultimately take root is for the same capabilities to be extended broadly: if my conversation involved math, ChatGPT should know to use Wolfram|Alpha on its own, without me adding the plug-in or going to a specialized GPT.I can understand why this capability doesn’t yet exist: the obvious technical challenges of properly exposing capabilities and training the model to know when to invoke those capabilities are a textbook example of Professor Clayton Christensen’s theory of integration and modularity, wherein integration works better when a product isn’t good enough; it is only when a product exceeds expectation that there is room for standardization and modularity. To that end, ChatGPT is only now getting the capability to generate an image without the mode being selected for it: I expect the ability to seek out less obvious tools will be fairly difficult.In fact, it’s possible that the entire plug-in/GPT approach ends up being a dead-end; towards the end of the keynote Romain Huet, the head of developer experience at OpenAI, explicitly demonstrated ChatGPT programming a computer. The scenario was splitting the tab for an Airbnb in Paris:\n  Code Interpreter is now available today in the API as well. That gives the AI the ability to write and generate code on the file, or even to generate files. So let’s see that in action. If I say here, “Hey, we’ll be 4 friends staying at this Airbnb, what’s my share of it plus my flights?”\n  Now here what’s happening is that Code Interpreter noticed that it should write some code to answer this query so now it’s computing the number of days in Paris, the number of friends, it’s also doing some exchange rate calculation behind the scene to get this answer for us. Not the most complex math, but you get the picture: imagine you’re building a very complex finance app that’s counting countless numbers, plotting charts, really any tasks you might tackle with code, then Code Interpreter will work great.\nUhm, what tasks do you not tackle with code? To be fair, Huet is referring to fairly simple math-oriented tasks, not the wholesale recreation of every app on the Internet, but it is interesting to consider for which problems ChatGPT will gain the wisdom to choose the right tool, and for which it will simply brute force a new solution; the history of computing would actually give the latter a higher probability: there are a lot of problems that were solved less with clever algorithms and more with the application of Moore’s Law."
        },
        {
          "subtitle": "Consumers and Hardware",
          "contentHtml": "<p>Speaking of the first year of Stratechery, that is when I first wrote about integration and modularization, in <a href=\"https://stratechery.com/2013/clayton-christensen-got-wrong/\">What Clayton Christensen Got Wrong</a>; as the title suggests I didn’t think the theory was universal:</p><blockquote><p>\n  Christensen himself laid out his theory’s primary flaw in the first quote excerpted above (from 2006):</p>\n<blockquote><p>\n    You also see it in aircrafts and software, and medical devices, and over and over.\n  </p></blockquote>\n<p>  That is the problem: Consumers don’t buy aircraft, software, or medical devices. Businesses do.</p>\n<p>  Christensen’s theory is based on examples drawn from buying decisions made by businesses, not consumers. The reason this matters is that the theory of low-end disruption presumes:</p>\n<ul>\n<li>Buyers are rational</li>\n<li>Every attribute that matters can be documented and measured</li>\n<li>Modular providers can become “good enough” on all the attributes that matter to the buyers</li>\n</ul>\n<p>  All three of the assumptions fail in the consumer market, and this, ultimately, is why Christensen’s theory fails as well. Let me take each one in turn:\n</p></blockquote><p>To summarize the argument, consumers care about things in ways that are inconsistent with whatever price you might attach to their utility, they prioritize ease-of-use, and they care about the quality of the user experience and are thus especially bothered by the seams inherent in a modular solution. This means that integrated solutions win because nothing is ever “good enough”; as I noted in the context of Amazon, <a href=\"https://stratechery.com/2018/divine-discontent-disruptions-antidote/\">Divine Discontent is Disruption’s Antidote</a>:</p><blockquote><p>\n  Bezos’s letter, though, reveals another advantage of focusing on customers: it makes it impossible to overshoot. When I wrote that piece five years ago, I was thinking of the opportunity provided by a focus on the user experience as if it were an asymptote: one could get ever closer to the ultimate user experience, but never achieve it:</p>\n<p>  <a href=\"https://stratechery.com/2018/divine-discontent-disruptions-antidote/\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?resize=640%2C389&amp;ssl=1\" alt=\"A drawing of The Asymptote Version of the User Experience\" width=\"640\" height=\"389\" class=\"alignnone size-full wp-image-3453\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?w=1300&amp;ssl=1 1300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?resize=300%2C183&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?resize=768%2C467&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?resize=1024%2C623&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.347.png?resize=1035%2C630&amp;ssl=1 1035w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>  In fact, though, consumer expectations are not static: they are, as Bezos’ memorably states, “divinely discontent”. What is amazing today is table stakes tomorrow, and, perhaps surprisingly, that makes for a tremendous business opportunity: if your company is predicated on delivering the best possible experience for consumers, then your company will never achieve its goal.</p>\n<p>  <a href=\"https://stratechery.com/2018/divine-discontent-disruptions-antidote/\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?resize=640%2C389&amp;ssl=1\" alt=\"A drawing of The Ever-Changing Version of the User Experience\" width=\"640\" height=\"389\" class=\"alignnone size-full wp-image-3452\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?w=1300&amp;ssl=1 1300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?resize=300%2C183&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?resize=768%2C467&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?resize=1024%2C623&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2018/05/Paper.stratechery-Year-One.348.png?resize=1035%2C630&amp;ssl=1 1035w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>  In the case of Amazon, that this unattainable and ever-changing objective is embedded in the company’s culture is, in conjunction with the company’s demonstrated ability to spin up new businesses on the profits of established ones, a sort of perpetual motion machine.\n</p></blockquote><p>I see no reason why both Articles wouldn’t apply to ChatGPT: while I might make the argument that hallucination is, in a certain light, a feature not a bug, the fact of the matter is that a lot of people use ChatGPT for information despite the fact it has a well-documented flaw when it comes to the truth; that flaw is acceptable, because to the customer ease-of-use is worth the loss of accuracy. Or look at plug-ins: the concept as originally implemented has already been abandoned, because the complexity in the user interface was more detrimental than whatever utility might have been possible. It seems likely this pattern will continue: of course customers will <em>say</em> that they want accuracy and 3rd-party tools; their actions will continue to demonstrate that convenience and ease-of-use matter most.</p><p>This has two implications. First, while this may have been OpenAI’s first developer conference, I remain unconvinced that OpenAI is going to ever be a true developer-focused company. I think that was Altman’s plan, but reality in the form of ChatGPT intervened: ChatGPT is the most important consumer-facing product since the iPhone, making OpenAI <a href=\"https://stratechery.com/2023/the-accidental-consumer-tech-company-chatgpt-meta-and-product-market-fit-aggregation-and-apis/\">The Accidental Consumer Tech Company</a>. That, by extension, means that integration will continue to matter more than modularization, which is great for Microsoft’s compute stack and maybe less exciting for developers.</p><p>Second, there remains one massive patch of friction in using ChatGPT; from <a href=\"https://stratechery.com/2023/ai-hardware-and-virtual-reality/\">AI, Hardware, and Virtual Reality</a>:</p><blockquote><p>\n  AI is truly something new and revolutionary and capable of being something more than just a homework aid, but I don’t think the existing interfaces are the right ones. Talking to ChatGPT is better than typing, but I still have to launch the app and set the mode; vision is an amazing capability, but it requires even more intent and friction to invoke. I could see a scenario where Meta’s AI is inferior technically to OpenAI, but more useful simply because it comes in a better form factor.\n</p></blockquote><p>After highlighting some news stories about OpenAI potentially partnering with Jony Ive to build hardware, I concluded:</p><blockquote><p>\n  There are obviously many steps before a potential hardware product, including actually agreeing to build one. And there is, of course, the fact that Apple and Google already make devices everyone carries, with the latter in particular investing heavily in its own AI capabilities; betting on the hardware in market winning the hardware opportunity in AI is the safest bet. That may not be a reason for either OpenAI or Meta to abandon their efforts, though: waging a hardware battle against Google and Apple would be difficult, but it might be even worse to be “just an app” if the full realization of AI’s capabilities depend on fully removing human friction from the process.\n</p></blockquote><p>This is the implication of a Universal Interface, which ChatGPT is striving to be: it also requires universal access, and that will always be a challenge for any company that is “just an app.” Yes, as I noted, the odds seem long, thanks to Apple and Google’s dominance, but I think there is an outside chance that the paradigm-shifting keynote is only just beginning its comeback.</p>",
          "contentText": "Speaking of the first year of Stratechery, that is when I first wrote about integration and modularization, in What Clayton Christensen Got Wrong; as the title suggests I didn’t think the theory was universal:\n  Christensen himself laid out his theory’s primary flaw in the first quote excerpted above (from 2006):\n\n    You also see it in aircrafts and software, and medical devices, and over and over.\n  \n  That is the problem: Consumers don’t buy aircraft, software, or medical devices. Businesses do.\n  Christensen’s theory is based on examples drawn from buying decisions made by businesses, not consumers. The reason this matters is that the theory of low-end disruption presumes:\n\nBuyers are rational\nEvery attribute that matters can be documented and measured\nModular providers can become “good enough” on all the attributes that matter to the buyers\n\n  All three of the assumptions fail in the consumer market, and this, ultimately, is why Christensen’s theory fails as well. Let me take each one in turn:\nTo summarize the argument, consumers care about things in ways that are inconsistent with whatever price you might attach to their utility, they prioritize ease-of-use, and they care about the quality of the user experience and are thus especially bothered by the seams inherent in a modular solution. This means that integrated solutions win because nothing is ever “good enough”; as I noted in the context of Amazon, Divine Discontent is Disruption’s Antidote:\n  Bezos’s letter, though, reveals another advantage of focusing on customers: it makes it impossible to overshoot. When I wrote that piece five years ago, I was thinking of the opportunity provided by a focus on the user experience as if it were an asymptote: one could get ever closer to the ultimate user experience, but never achieve it:\n  \n  In fact, though, consumer expectations are not static: they are, as Bezos’ memorably states, “divinely discontent”. What is amazing today is table stakes tomorrow, and, perhaps surprisingly, that makes for a tremendous business opportunity: if your company is predicated on delivering the best possible experience for consumers, then your company will never achieve its goal.\n  \n  In the case of Amazon, that this unattainable and ever-changing objective is embedded in the company’s culture is, in conjunction with the company’s demonstrated ability to spin up new businesses on the profits of established ones, a sort of perpetual motion machine.\nI see no reason why both Articles wouldn’t apply to ChatGPT: while I might make the argument that hallucination is, in a certain light, a feature not a bug, the fact of the matter is that a lot of people use ChatGPT for information despite the fact it has a well-documented flaw when it comes to the truth; that flaw is acceptable, because to the customer ease-of-use is worth the loss of accuracy. Or look at plug-ins: the concept as originally implemented has already been abandoned, because the complexity in the user interface was more detrimental than whatever utility might have been possible. It seems likely this pattern will continue: of course customers will say that they want accuracy and 3rd-party tools; their actions will continue to demonstrate that convenience and ease-of-use matter most.This has two implications. First, while this may have been OpenAI’s first developer conference, I remain unconvinced that OpenAI is going to ever be a true developer-focused company. I think that was Altman’s plan, but reality in the form of ChatGPT intervened: ChatGPT is the most important consumer-facing product since the iPhone, making OpenAI The Accidental Consumer Tech Company. That, by extension, means that integration will continue to matter more than modularization, which is great for Microsoft’s compute stack and maybe less exciting for developers.Second, there remains one massive patch of friction in using ChatGPT; from AI, Hardware, and Virtual Reality:\n  AI is truly something new and revolutionary and capable of being something more than just a homework aid, but I don’t think the existing interfaces are the right ones. Talking to ChatGPT is better than typing, but I still have to launch the app and set the mode; vision is an amazing capability, but it requires even more intent and friction to invoke. I could see a scenario where Meta’s AI is inferior technically to OpenAI, but more useful simply because it comes in a better form factor.\nAfter highlighting some news stories about OpenAI potentially partnering with Jony Ive to build hardware, I concluded:\n  There are obviously many steps before a potential hardware product, including actually agreeing to build one. And there is, of course, the fact that Apple and Google already make devices everyone carries, with the latter in particular investing heavily in its own AI capabilities; betting on the hardware in market winning the hardware opportunity in AI is the safest bet. That may not be a reason for either OpenAI or Meta to abandon their efforts, though: waging a hardware battle against Google and Apple would be difficult, but it might be even worse to be “just an app” if the full realization of AI’s capabilities depend on fully removing human friction from the process.\nThis is the implication of a Universal Interface, which ChatGPT is striving to be: it also requires universal access, and that will always be a challenge for any company that is “just an app.” Yes, as I noted, the odds seem long, thanks to Apple and Google’s dominance, but I think there is an outside chance that the paradigm-shifting keynote is only just beginning its comeback."
        }
      ]
    },
    {
      "title": "Attenuating Innovation (AI)",
      "publishedDate": "2023-11-01T06:58:49-07:00",
      "updatedDate": "2023-11-02T05:55:41-07:00",
      "contentHtml": "\n\t\t<p>In 2019, a very animated <a href=\"https://www.youtube.com/watch?v=_15DReQKbt8\">Bill Gates explained to Andrew Ross Sorkin</a> why Microsoft lost mobile:</p>\n<div align=\"center\"><div id=\"v-eyhrEO1d-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/eyhrEO1d?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div>\n<blockquote><p>\n  There’s no doubt that the antitrust lawsuit was bad for Microsoft. We would have been more focused on creating the phone operating system so that instead of using Android today, you would be using Windows Mobile. If it hadn’t been for the antitrust case, Microsoft would have…</p>\n<p>  <strong>You’re convinced?</strong></p>\n<p>  Oh we were so close. I was just too distracted. I screwed that up because of the distraction. We were just three months too late with a release that Motorola would have used on a phone, so yes, it’s a winner-take-all game, that is for sure. Now nobody here has ever heard of Windows Mobile, but oh well. That’s a few hundred billion here or there.\n</p></blockquote>\n<p>This opinion is, to use a technical term favored by analysts, bullshit. Windows Mobile wasn’t three months late relative to Android; Windows Mobile launched as the <em>Pocket PC 2000</em> operating system in, you guessed it, 2000, a full eight years before the first Android device hit the market.</p>\n<p>The issue with Windows Mobile was, first and foremost, Gates himself: in his view of the world the Windows-based PC was the center of a user’s computing life, and the phone a satellite; small wonder that Windows Mobile looked and operated like a shrunken-down version of Windows: there was a Start button, and Windows Mobile 2003, the first version to have the “Windows Mobile” name, even had the same Sonoma Valley wallpaper as Windows XP:</p>\n<p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-2.png?resize=240%2C320&amp;ssl=1\" alt=\"A screenshot of Windows Mobile in 2003\" width=\"240\" height=\"320\" class=\"aligncenter size-full wp-image-11822\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-2.png?w=240&amp;ssl=1 240w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-2.png?resize=225%2C300&amp;ssl=1 225w\" sizes=\"(max-width: 240px) 100vw, 240px\" data-recalc-dims=\"1\"></p>\n<p>If anything, the problem with Windows Mobile is that it was too early: Android, which <a href=\"https://www.osnews.com/story/25264/did-android-really-look-like-blackberry-before-the-iphone/\">originally looked like a Blackberry</a>, had the benefit of copying the iPhone; the iPhone, in stark contrast to Windows Mobile, looked nothing like the Mac, despite sharing the same internals. Instead, Steve Jobs and company started with a new interface paradigm — multi-touch — and developed a user interface that was actually suited to a handheld device. Jobs — appropriately! —&nbsp;<a href=\"https://youtu.be/VQKMoT-6XSg?si=pYx5weeWg_3cFytS&amp;t=286\">called it revolutionary</a>.</p>\n<p>Fast forward four months from the iPhone introduction, and Jobs and Gates were together on stage for the D5 Conference, and Gates still didn’t get it; when Walt Mossberg asked him about what devices we would be using in five years, Gates still had a Windows device at the center:</p>\n<div align=\"center\"><div id=\"v-QEpiGUq2-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/QEpiGUq2?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div>\n<blockquote><p>\n  I don’t think you’ll have one device. I think you’ll have a full-screen device that you can carry around and you’ll do dramatically more reading off of that. I believe in the tablet form factor. I think you’ll have voice, I think you’ll have ink, I think you’ll have some way of having a hardware keyboard and some settings for that. And then you’ll have the device that fits in your pocket which the whole notion of how much function should you combine in there, there’s navigation computers, there’s media, there’s phone, technology is letting us put more things in there but then again, you really want to tune it so people what they expect. So there’s quite a bit of experimentation in that pocket-sized device. But I think those are natural form factors. We’ll have the evolution of the portable machine, and the evolution of the phone, will both be extremely high volume, complementary, that is if you own one you’re more likely to own the other.\n</p></blockquote>\n<p>In fact, in five years worldwide smartphone sales would total 700 million units, more than doubling the 348.7 million PCs that shipped that same year; yes, a lot of those smartphone sales went to people who already had PCs, but it was already apparent that for huge swathes of people — including in developed countries — the phone was the only device that you needed.</p>\n<p>What is even more fascinating about this conversation, though, is the way in which it illustrated how Jobs and Apple were able to invent the future, while Microsoft utterly missed it.</p>\n<div align=\"center\"><div id=\"v-8MqlEqHs-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/8MqlEqHs?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div>\n<p>Mossberg asked:</p>\n<blockquote><p>\n  The core functions of the device form factor formerly known as the cellphone, whatever we want to call it — the pocket device — what would you say the core functions are five years out?\n</p></blockquote>\n<p>Gates’ answer was redolent of so many experts trying to predict the future: he had some ideas and some inside knowledge of new technology, but no real vision of what might come next:</p>\n<blockquote><p>\n  How quickly all these things that have been somewhat specialized — the navigation device, the digital wallet, the phone, the camera, the video camera — how quickly those all come together, that’s hard to chart out, but eventually you’ll be able to make something that has the capability to do every one of those things. And yet given the small size, you still won’t want to edit your homework or edit a movie on a screen of that size, and so you’ll have something else that lets you do the reading and editing and those things. Now if we could ever get a screen that would just roll out like a scroll, then you might be able to have the device that did everything.\n</p></blockquote>\n<p>After a back-and-forth about e-ink and projection screens, Mossberg asked Jobs the same question, and his answer was profound:</p>\n<div align=\"center\"><div id=\"v-5jBPKazG-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/5jBPKazG?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div>\n<blockquote><p>\n  I don’t know.</p>\n<p>  The reason I don’t know is because I wouldn’t have thought that there would have been maps on it five years ago. But something comes along, gets really popular, people love it, get used to it, you want it on there. People are inventing things constantly and I think the art of it is balancing what’s on there and what’s not — it’s the editing function.\n</p></blockquote>\n<p>That right there is the recipe for genuine innovation:</p>\n<ul>\n<li>Embrace uncertainty and the fact one doesn’t know the future.</li>\n<li>Understand that people are inventing things — and not just technologies, but also use cases — constantly.</li>\n<li>Remember that the art comes in editing after the invention, not before.</li>\n</ul>\n<p>To be like Gates and Microsoft is to do the opposite: to think that you know the future; to assume you know what technologies and applications are coming; to proscribe what people will do or not do ahead of time. It is a mindset that does not accelerate innovation, but rather attenuates it.</p>\n<h3>A Cynical Read on AI Alarm</h3>\n<p>Last week in a <a href=\"https://stratechery.com/2023/an-interview-with-gregory-allen-about-the-updated-china-chip-ban/\">Stratechery Interview with Gregory Allen about the chip ban</a> we discussed why Washington D.C. suddenly had so much urgency about AI. The first reason was of course ChatGPT; it was the second, though, that set off alarm bells in my head. Here’s Allen:</p>\n<blockquote><p>\n  The other thing that’s happened that I do think is important just for folks to understand is, that&nbsp;<a href=\"https://www.safe.ai/press-release\">Center for AI Safety letter that came out</a>, that was signed by Sam Altman, that was signed by a bunch of other folks that said, “The risks of AI, including the risks of human extinction, should be viewed in the same light as nuclear weapons and pandemics.” The list of signatories to that letter was quite illustrious and quite long, and it’s really difficult to overstate the impact that that letter had on Washington, D. C. When you have the CEO of all these companies…when you get that kind of roster saying, “When you think of my technology, think of nuclear weapons,” you definitely get Washington’s attention.\n</p></blockquote>\n<p>It turns out you get more than that: on Monday the Biden administration released an <a href=\"https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/\">Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence</a>. This Executive Order goes far beyond setting up a commission or study about AI, a field that is obviously still under rapid development; instead it goes straight to proscription.</p>\n<p>Before I get to the executive order, though, I want to go back to Gates: that video at the top, where he blamed the Department of Justice for Microsoft having missed mobile, was the first thing I thought of during my interview with Allen. The fact of the matter is that Gates is the single most <em>unreliable</em> narrator about why Microsoft missed mobile, precisely because he was so intimately involved in the effort.</p>\n<p>By the time that interview happened in 2019, it was obvious to everyone that Microsoft had utterly failed in mobile, and that it cost the company billions of dollars along the way. It is exceptionally difficult, particularly for someone as intelligent and successful as Gates, to admit the obvious truth: Microsoft missed mobile because Microsoft approached the space with the entirely wrong paradigm in mind. Or, to be more blunt, Gates got it wrong. It is much easier to blame someone else than to face that failure, particularly when the federal government is sitting right there!</p>\n<p>In short, it is always necessary to carefully examine the motivations of a self-interested actor, and that certainly applies to the letter Allen referenced.</p>\n<hr>\n<p>To rewind just a bit, last January I wrote <a href=\"https://stratechery.com/2023/ai-and-the-big-five/\">AI and the Big Five</a>, which posited that the initial wave of generative AI would largely benefit the dominant tech companies. Apple’s strategy was unclear, but it controlled the devices via which AI would be accessed, and had the potential to benefit even more if AI could be run locally. Amazon had AWS, which held much of the data over which companies might wish to apply AI, but also lacked its own foundational models. Google likely had the greatest capabilities, but also the greatest business model challenges. Meta controlled the apps through which consumers might be most likely to encounter AI generated content. Microsoft, meanwhile, thanks to its partnership with OpenAI, was the best placed to ride the initial wave generated by ChatGPT.</p>\n<p>Nine months later and the Article holds up well: Apple is releasing ever more powerful devices, but still lacks a clear strategy; Amazon spent <a href=\"https://stratechery.com/2023/amazon-earnings-the-logistics-virtuous-cycle-amazon-aggregator-ads/\">its last earnings call</a> trying to convince investors that AI applications would come to their data, and talking up its partnership with Anthropic, OpenAI’s biggest competitor; Google has demonstrated great technology but has been slow to ship; Meta is pushing ahead with generative AI in its apps; and Microsoft is actually registering <a href=\"https://stratechery.com/2023/google-earnings-microsoft-earnings-ai-leverage/\">meaningful financial impact</a> from its OpenAI partnership.</p>\n<p>With this as context, it’s interesting to consider who signed that letter Allen referred to, which stated:</p>\n<blockquote><p>\n  Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\n</p></blockquote>\n<p>There are 30 signatories from OpenAI, including the aforementioned CEO Sam Altman. There are 15 signatories from Anthropic, including CEO Dario Amodei. There are seven signatories from Microsoft, including CTO Kevin Scott. There are 81 signatories from Google, including Google DeepMind CEO Demis Hassabis. There are none from Apple or Amazon, and two low-level employees from Meta.</p>\n<p>What is striking about this tally is the extent to which the totals and prominence align to the relative companies’ current position in the market. OpenAI has the lead, at least in terms of consumer and developer mindshare, and the company is deriving real revenue from ChatGPT; Anthropic is second, and has signed deals with both Google and Amazon. Google has great products and an internal paralysis around shipping them for business model reasons; urging caution is very much in their interest. Microsoft is in the middle: it is making money from AI, but it doesn’t control its own models; Apple and Amazon are both waiting for the market to come to them.</p>\n<p>In this ultra-cynical analysis the biggest surprise is probably Meta: the company has its own models, but no one of prominence has signed. These models, though, have been <a href=\"https://stratechery.com/2023/free-meta-open-sources-another-ai-model-moats-and-open-source-apple-and-meta/\">gradually open-sourced</a>: Meta is betting on distributed innovation to generate value that will best be captured via the consumer touchpoints the the company controls.</p>\n<p>The point is this: if you accept the premise that regulation locks in incumbents, then it sure is notable that the early AI winners seem the most invested in generating alarm in Washington, D.C. about AI. This despite the fact that their concern is apparently not sufficiently high to, you know, stop their work. No, they are the responsible ones, the ones who care enough to call for regulation; all the better if concerns about imagined harms kneecap inevitable competitors.</p>\n<h3>An Executive Order on Attenuating Innovation</h3>\n<p>There is another quote I thought of this week. It was delivered by Senator Amy Klobuchar <a href=\"https://twitter.com/amyklobuchar/status/1484566074104426497\">in a tweet</a>:</p>\n<p><a href=\"https://twitter.com/amyklobuchar/status/1484566074104426497\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?resize=640%2C372&amp;ssl=1\" alt=\"A tweet from Senator Klobuchar celebrating tech regulation\" width=\"640\" height=\"372\" class=\"aligncenter size-full wp-image-11821\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?resize=300%2C174&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?resize=1024%2C594&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?resize=768%2C446&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?resize=1085%2C630&amp;ssl=1 1085w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>I wrote at the time <a href=\"https://stratechery.com/2022/free-netflix-follow-up-the-senate-tech-bill/\">in an Update</a>:</p>\n<blockquote><p>\n  In 1991 — assuming that the “dawn of the Internet” was the launch of the World Wide Web — the following were the biggest companies by market cap:</p>\n<ol>\n<li><strong>$88 billion</strong> — General Electric </li>\n<li><strong>$80 billion</strong> — Exxon Mobil</li>\n<li><strong>$62 billion</strong> — Walmart</li>\n<li><strong>$54 billion</strong> — Coca-Cola</li>\n<li><strong>$42 billion</strong> —&nbsp;Merck</li>\n</ol>\n<p>  The only tech company in the top 10 was IBM, with a $31 billion market cap. Imagine proposing a bill then targeting companies with greater than $550 billion market caps, knowing that it is nothing but tech companies!</p>\n<p>  What doesn’t occur to Senator Klobuchar is the possibility that the relationship between the massive increase in wealth, and even greater gain in consumer welfare, produced by tech companies since the “dawn of the Internet” may in fact be related to the fact that there hasn’t been any major regulation (the most important piece of regulation, <a href=\"https://stratechery.com/topic/regulation/section-230/\">Section 230</a>, protected the Internet from lawsuits; this legislation invites them). I’m not saying that the lack of regulation is causal, but I am exceptionally skeptical that we would have had <em>more</em> growth with <em>more</em> regulation.</p>\n<p>  More broadly, tech sure seems like the only area where innovation and building is happening anywhere in the West. This isn’t to deny that the big tech companies aren’t sometimes bad actors, and that platforms in particular do, at least in theory, need regulation. But given the sclerosis present everywhere <em>but</em> tech it sure seems like it would be prudent to be exceptionally skeptical about the prospect of new regulation; I definitely wouldn’t be celebrating it as if it were some sort of overdue accomplishment.\n</p></blockquote>\n<p>Unfortunately this week’s Executive Order takes the exact opposite approach to AI that we took to technology previously. As Steven Sinofsky explains in <a href=\"https://hardcoresoftware.learningbyshipping.com/p/211-regulating-ai-by-executive-order\">this excellent article</a>:</p>\n<blockquote><p>\n  This document is the work of aggregating policy inputs from an extended committee of interested constituencies while also navigating the law — literally what is it that can be done to throttle artificial intelligence legally without passing any new laws that might throttle artificial intelligence. There is no clear owner of this document. There is no leading science consensus or direction that we can discern. It is impossible to separate out the document from the process and approach used to “govern” AI innovation. Govern is quoted because it is the word used in the EO. This is so much less a document of what should be done with the potential of technology than it is a document pushing the limits of what can be done legally to slow innovation.\n</p></blockquote>\n<p>Much attention has been focused on the Executive Order’s ultra-specific limits on model sizes and attributes (you can exceed those limits if you are registered and approved, a game best played by large established companies like the list I just detailed); unfortunately that is only the beginning of the issues with this Executive Order, but again, I urge you to read <a href=\"https://hardcoresoftware.learningbyshipping.com/p/211-regulating-ai-by-executive-order\">Sinofsky’s post</a>.</p>\n<p>What is so disappointing to me is how utterly opposed this executive order is to how innovation actually happens:</p>\n<ul>\n<li>The Biden administration is not embracing uncertainty: it is operating from an assumption that AI is dangerous, despite the fact that many of the listed harms, like learning how to build a bomb or synthesize dangerous chemicals or conduct cyber attacks, are already trivially accomplished on today’s Internet. What is completely lacking is anything other than the briefest of hand waves at AI’s potential upside. The government is Bill Gates, imagining what might be possible, when it ought to be Steve Jobs, humble enough to know it cannot predict the future.</li>\n<li>The Biden administration is operating with a fundamental lack of trust in the capability of humans to invent new things, not just technologies, but also use cases, many of which will create new jobs. It can envision how the spreadsheet might imperil bookkeepers, but it can’t imagine how that same tool might unlock entire new industries.</li>\n<li>The Biden administration is arrogantly insisting that it ought have a role in dictating the outcomes of an innovation that few if any of its members understand, and almost certainly could not invent. There is, to be sure, a role for oversight and regulation, but that is a blunt instrument best applied after the invention, like an editor.</li>\n</ul>\n<p>In short, this Executive Order is a lot like Gates’ approach to mobile: rooted in the past, yet arrogant about an unknowable future; proscriptive instead of adaptive; and, worst of all, trivially influenced by motivated reasoning best understood as some of the most cynical attempts at regulatory capture the tech industry has ever seen.</p>\n<h3>The Sclerotic Shiggoth</h3>\n<p>I fully endorse Sinofsky’s conclusion:</p>\n<blockquote><p>\n  This approach to regulation is not about innovation despite all the verbiage proclaiming it to be. This Order is about stifling innovation and turning the next platform over to incumbents in the US and far more likely new companies in other countries that did not see it as a priority to halt innovation before it even happens.</p>\n<p>  I am by no means certain if AI is the next technology platform the likes of which will make the smartphone revolution that has literally benefitted every human on earth look small. I don’t know sitting here today if the AI products just in market less than a year are the next biggest thing ever. They may turn out to be a way stop on the trajectory of innovation. They may turn out to be ingredients that everyone incorporates into existing products. There are so many things that we do not yet know.</p>\n<p>  What we do know is that we are at the very earliest stages. We simply have no in-market products, and that means no in-market problems, upon which to base such concerns of fear and need to “govern” regulation. Alarmists or “existentialists” say they have enough evidence. If that’s the case then then so be it, but then the only way to truly make that case is to embark on the legislative process and use democracy to validate those concerns. I just know that we have plenty of past evidence that every technology has come with its alarmists and concerns and somehow optimism prevailed. Why should the pessimists prevail now?\n</p></blockquote>\n<p>They should not. We should accelerate innovation, not attenuate it. Innovation —&nbsp;technology, broadly speaking — is the only way to grow the pie, and to solve the problems we face that actually exist in any sort of knowable way, from climate change to China, from pandemics to poverty, and from diseases to demographics. To attack the solution is denialism at best, outright sabotage at worst. Indeed, the shoggoth to fear is our societal sclerosis seeking to drag the most exciting new technology in years into an innovation anti-pattern.</p>\n<div align=\"center\"><figure id=\"attachment_11831\" aria-describedby=\"caption-attachment-11831\" style=\"width: 1024px\" class=\"wp-caption alignnone\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?resize=640%2C640&amp;ssl=1\" alt=\"Photo of a radiant, downscaled city teetering on the brink of an expansive abyss, with a dark, murky quagmire below containing decayed structures reminiscent of historic landmarks. The city is a beacon of the future, with flying cars, green buildings, and residents in futuristic attire. The influence of AI is subtly interwoven, with robots helping citizens and digital screens integrated into the environment. Below, the haunting silhouette of a shoggoth, with its eerie tendrils, endeavors to pull the city into the depths, illustrating the clash between forward-moving evolution and outdated forces.\" width=\"640\" height=\"640\" class=\"size-full wp-image-11831\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?w=1024&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?resize=630%2C630&amp;ssl=1 630w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"><figcaption id=\"caption-attachment-11831\" class=\"wp-caption-text\">Photo generated by Dall-E 3, with the following prompt: “Photo of a radiant, downscaled city teetering on the brink of an expansive abyss, with a dark, murky quagmire below containing decayed structures reminiscent of historic landmarks. The city is a beacon of the future, with flying cars, green buildings, and residents in futuristic attire. The influence of AI is subtly interwoven, with robots helping citizens and digital screens integrated into the environment. Below, the haunting silhouette of a shoggoth, with its eerie tendrils, endeavors to pull the city into the depths, illustrating the clash between forward-moving evolution and outdated forces.”</figcaption></figure></div>\n\n\t\t\t",
      "contentText": "\n\t\tIn 2019, a very animated Bill Gates explained to Andrew Ross Sorkin why Microsoft lost mobile:\n\n\n  There’s no doubt that the antitrust lawsuit was bad for Microsoft. We would have been more focused on creating the phone operating system so that instead of using Android today, you would be using Windows Mobile. If it hadn’t been for the antitrust case, Microsoft would have…\n  You’re convinced?\n  Oh we were so close. I was just too distracted. I screwed that up because of the distraction. We were just three months too late with a release that Motorola would have used on a phone, so yes, it’s a winner-take-all game, that is for sure. Now nobody here has ever heard of Windows Mobile, but oh well. That’s a few hundred billion here or there.\n\nThis opinion is, to use a technical term favored by analysts, bullshit. Windows Mobile wasn’t three months late relative to Android; Windows Mobile launched as the Pocket PC 2000 operating system in, you guessed it, 2000, a full eight years before the first Android device hit the market.\nThe issue with Windows Mobile was, first and foremost, Gates himself: in his view of the world the Windows-based PC was the center of a user’s computing life, and the phone a satellite; small wonder that Windows Mobile looked and operated like a shrunken-down version of Windows: there was a Start button, and Windows Mobile 2003, the first version to have the “Windows Mobile” name, even had the same Sonoma Valley wallpaper as Windows XP:\n\nIf anything, the problem with Windows Mobile is that it was too early: Android, which originally looked like a Blackberry, had the benefit of copying the iPhone; the iPhone, in stark contrast to Windows Mobile, looked nothing like the Mac, despite sharing the same internals. Instead, Steve Jobs and company started with a new interface paradigm — multi-touch — and developed a user interface that was actually suited to a handheld device. Jobs — appropriately! — called it revolutionary.\nFast forward four months from the iPhone introduction, and Jobs and Gates were together on stage for the D5 Conference, and Gates still didn’t get it; when Walt Mossberg asked him about what devices we would be using in five years, Gates still had a Windows device at the center:\n\n\n  I don’t think you’ll have one device. I think you’ll have a full-screen device that you can carry around and you’ll do dramatically more reading off of that. I believe in the tablet form factor. I think you’ll have voice, I think you’ll have ink, I think you’ll have some way of having a hardware keyboard and some settings for that. And then you’ll have the device that fits in your pocket which the whole notion of how much function should you combine in there, there’s navigation computers, there’s media, there’s phone, technology is letting us put more things in there but then again, you really want to tune it so people what they expect. So there’s quite a bit of experimentation in that pocket-sized device. But I think those are natural form factors. We’ll have the evolution of the portable machine, and the evolution of the phone, will both be extremely high volume, complementary, that is if you own one you’re more likely to own the other.\n\nIn fact, in five years worldwide smartphone sales would total 700 million units, more than doubling the 348.7 million PCs that shipped that same year; yes, a lot of those smartphone sales went to people who already had PCs, but it was already apparent that for huge swathes of people — including in developed countries — the phone was the only device that you needed.\nWhat is even more fascinating about this conversation, though, is the way in which it illustrated how Jobs and Apple were able to invent the future, while Microsoft utterly missed it.\n\nMossberg asked:\n\n  The core functions of the device form factor formerly known as the cellphone, whatever we want to call it — the pocket device — what would you say the core functions are five years out?\n\nGates’ answer was redolent of so many experts trying to predict the future: he had some ideas and some inside knowledge of new technology, but no real vision of what might come next:\n\n  How quickly all these things that have been somewhat specialized — the navigation device, the digital wallet, the phone, the camera, the video camera — how quickly those all come together, that’s hard to chart out, but eventually you’ll be able to make something that has the capability to do every one of those things. And yet given the small size, you still won’t want to edit your homework or edit a movie on a screen of that size, and so you’ll have something else that lets you do the reading and editing and those things. Now if we could ever get a screen that would just roll out like a scroll, then you might be able to have the device that did everything.\n\nAfter a back-and-forth about e-ink and projection screens, Mossberg asked Jobs the same question, and his answer was profound:\n\n\n  I don’t know.\n  The reason I don’t know is because I wouldn’t have thought that there would have been maps on it five years ago. But something comes along, gets really popular, people love it, get used to it, you want it on there. People are inventing things constantly and I think the art of it is balancing what’s on there and what’s not — it’s the editing function.\n\nThat right there is the recipe for genuine innovation:\n\nEmbrace uncertainty and the fact one doesn’t know the future.\nUnderstand that people are inventing things — and not just technologies, but also use cases — constantly.\nRemember that the art comes in editing after the invention, not before.\n\nTo be like Gates and Microsoft is to do the opposite: to think that you know the future; to assume you know what technologies and applications are coming; to proscribe what people will do or not do ahead of time. It is a mindset that does not accelerate innovation, but rather attenuates it.\nA Cynical Read on AI Alarm\nLast week in a Stratechery Interview with Gregory Allen about the chip ban we discussed why Washington D.C. suddenly had so much urgency about AI. The first reason was of course ChatGPT; it was the second, though, that set off alarm bells in my head. Here’s Allen:\n\n  The other thing that’s happened that I do think is important just for folks to understand is, that Center for AI Safety letter that came out, that was signed by Sam Altman, that was signed by a bunch of other folks that said, “The risks of AI, including the risks of human extinction, should be viewed in the same light as nuclear weapons and pandemics.” The list of signatories to that letter was quite illustrious and quite long, and it’s really difficult to overstate the impact that that letter had on Washington, D. C. When you have the CEO of all these companies…when you get that kind of roster saying, “When you think of my technology, think of nuclear weapons,” you definitely get Washington’s attention.\n\nIt turns out you get more than that: on Monday the Biden administration released an Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. This Executive Order goes far beyond setting up a commission or study about AI, a field that is obviously still under rapid development; instead it goes straight to proscription.\nBefore I get to the executive order, though, I want to go back to Gates: that video at the top, where he blamed the Department of Justice for Microsoft having missed mobile, was the first thing I thought of during my interview with Allen. The fact of the matter is that Gates is the single most unreliable narrator about why Microsoft missed mobile, precisely because he was so intimately involved in the effort.\nBy the time that interview happened in 2019, it was obvious to everyone that Microsoft had utterly failed in mobile, and that it cost the company billions of dollars along the way. It is exceptionally difficult, particularly for someone as intelligent and successful as Gates, to admit the obvious truth: Microsoft missed mobile because Microsoft approached the space with the entirely wrong paradigm in mind. Or, to be more blunt, Gates got it wrong. It is much easier to blame someone else than to face that failure, particularly when the federal government is sitting right there!\nIn short, it is always necessary to carefully examine the motivations of a self-interested actor, and that certainly applies to the letter Allen referenced.\n\nTo rewind just a bit, last January I wrote AI and the Big Five, which posited that the initial wave of generative AI would largely benefit the dominant tech companies. Apple’s strategy was unclear, but it controlled the devices via which AI would be accessed, and had the potential to benefit even more if AI could be run locally. Amazon had AWS, which held much of the data over which companies might wish to apply AI, but also lacked its own foundational models. Google likely had the greatest capabilities, but also the greatest business model challenges. Meta controlled the apps through which consumers might be most likely to encounter AI generated content. Microsoft, meanwhile, thanks to its partnership with OpenAI, was the best placed to ride the initial wave generated by ChatGPT.\nNine months later and the Article holds up well: Apple is releasing ever more powerful devices, but still lacks a clear strategy; Amazon spent its last earnings call trying to convince investors that AI applications would come to their data, and talking up its partnership with Anthropic, OpenAI’s biggest competitor; Google has demonstrated great technology but has been slow to ship; Meta is pushing ahead with generative AI in its apps; and Microsoft is actually registering meaningful financial impact from its OpenAI partnership.\nWith this as context, it’s interesting to consider who signed that letter Allen referred to, which stated:\n\n  Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\n\nThere are 30 signatories from OpenAI, including the aforementioned CEO Sam Altman. There are 15 signatories from Anthropic, including CEO Dario Amodei. There are seven signatories from Microsoft, including CTO Kevin Scott. There are 81 signatories from Google, including Google DeepMind CEO Demis Hassabis. There are none from Apple or Amazon, and two low-level employees from Meta.\nWhat is striking about this tally is the extent to which the totals and prominence align to the relative companies’ current position in the market. OpenAI has the lead, at least in terms of consumer and developer mindshare, and the company is deriving real revenue from ChatGPT; Anthropic is second, and has signed deals with both Google and Amazon. Google has great products and an internal paralysis around shipping them for business model reasons; urging caution is very much in their interest. Microsoft is in the middle: it is making money from AI, but it doesn’t control its own models; Apple and Amazon are both waiting for the market to come to them.\nIn this ultra-cynical analysis the biggest surprise is probably Meta: the company has its own models, but no one of prominence has signed. These models, though, have been gradually open-sourced: Meta is betting on distributed innovation to generate value that will best be captured via the consumer touchpoints the the company controls.\nThe point is this: if you accept the premise that regulation locks in incumbents, then it sure is notable that the early AI winners seem the most invested in generating alarm in Washington, D.C. about AI. This despite the fact that their concern is apparently not sufficiently high to, you know, stop their work. No, they are the responsible ones, the ones who care enough to call for regulation; all the better if concerns about imagined harms kneecap inevitable competitors.\nAn Executive Order on Attenuating Innovation\nThere is another quote I thought of this week. It was delivered by Senator Amy Klobuchar in a tweet:\n\nI wrote at the time in an Update:\n\n  In 1991 — assuming that the “dawn of the Internet” was the launch of the World Wide Web — the following were the biggest companies by market cap:\n\n$88 billion — General Electric \n$80 billion — Exxon Mobil\n$62 billion — Walmart\n$54 billion — Coca-Cola\n$42 billion — Merck\n\n  The only tech company in the top 10 was IBM, with a $31 billion market cap. Imagine proposing a bill then targeting companies with greater than $550 billion market caps, knowing that it is nothing but tech companies!\n  What doesn’t occur to Senator Klobuchar is the possibility that the relationship between the massive increase in wealth, and even greater gain in consumer welfare, produced by tech companies since the “dawn of the Internet” may in fact be related to the fact that there hasn’t been any major regulation (the most important piece of regulation, Section 230, protected the Internet from lawsuits; this legislation invites them). I’m not saying that the lack of regulation is causal, but I am exceptionally skeptical that we would have had more growth with more regulation.\n  More broadly, tech sure seems like the only area where innovation and building is happening anywhere in the West. This isn’t to deny that the big tech companies aren’t sometimes bad actors, and that platforms in particular do, at least in theory, need regulation. But given the sclerosis present everywhere but tech it sure seems like it would be prudent to be exceptionally skeptical about the prospect of new regulation; I definitely wouldn’t be celebrating it as if it were some sort of overdue accomplishment.\n\nUnfortunately this week’s Executive Order takes the exact opposite approach to AI that we took to technology previously. As Steven Sinofsky explains in this excellent article:\n\n  This document is the work of aggregating policy inputs from an extended committee of interested constituencies while also navigating the law — literally what is it that can be done to throttle artificial intelligence legally without passing any new laws that might throttle artificial intelligence. There is no clear owner of this document. There is no leading science consensus or direction that we can discern. It is impossible to separate out the document from the process and approach used to “govern” AI innovation. Govern is quoted because it is the word used in the EO. This is so much less a document of what should be done with the potential of technology than it is a document pushing the limits of what can be done legally to slow innovation.\n\nMuch attention has been focused on the Executive Order’s ultra-specific limits on model sizes and attributes (you can exceed those limits if you are registered and approved, a game best played by large established companies like the list I just detailed); unfortunately that is only the beginning of the issues with this Executive Order, but again, I urge you to read Sinofsky’s post.\nWhat is so disappointing to me is how utterly opposed this executive order is to how innovation actually happens:\n\nThe Biden administration is not embracing uncertainty: it is operating from an assumption that AI is dangerous, despite the fact that many of the listed harms, like learning how to build a bomb or synthesize dangerous chemicals or conduct cyber attacks, are already trivially accomplished on today’s Internet. What is completely lacking is anything other than the briefest of hand waves at AI’s potential upside. The government is Bill Gates, imagining what might be possible, when it ought to be Steve Jobs, humble enough to know it cannot predict the future.\nThe Biden administration is operating with a fundamental lack of trust in the capability of humans to invent new things, not just technologies, but also use cases, many of which will create new jobs. It can envision how the spreadsheet might imperil bookkeepers, but it can’t imagine how that same tool might unlock entire new industries.\nThe Biden administration is arrogantly insisting that it ought have a role in dictating the outcomes of an innovation that few if any of its members understand, and almost certainly could not invent. There is, to be sure, a role for oversight and regulation, but that is a blunt instrument best applied after the invention, like an editor.\n\nIn short, this Executive Order is a lot like Gates’ approach to mobile: rooted in the past, yet arrogant about an unknowable future; proscriptive instead of adaptive; and, worst of all, trivially influenced by motivated reasoning best understood as some of the most cynical attempts at regulatory capture the tech industry has ever seen.\nThe Sclerotic Shiggoth\nI fully endorse Sinofsky’s conclusion:\n\n  This approach to regulation is not about innovation despite all the verbiage proclaiming it to be. This Order is about stifling innovation and turning the next platform over to incumbents in the US and far more likely new companies in other countries that did not see it as a priority to halt innovation before it even happens.\n  I am by no means certain if AI is the next technology platform the likes of which will make the smartphone revolution that has literally benefitted every human on earth look small. I don’t know sitting here today if the AI products just in market less than a year are the next biggest thing ever. They may turn out to be a way stop on the trajectory of innovation. They may turn out to be ingredients that everyone incorporates into existing products. There are so many things that we do not yet know.\n  What we do know is that we are at the very earliest stages. We simply have no in-market products, and that means no in-market problems, upon which to base such concerns of fear and need to “govern” regulation. Alarmists or “existentialists” say they have enough evidence. If that’s the case then then so be it, but then the only way to truly make that case is to embark on the legislative process and use democracy to validate those concerns. I just know that we have plenty of past evidence that every technology has come with its alarmists and concerns and somehow optimism prevailed. Why should the pessimists prevail now?\n\nThey should not. We should accelerate innovation, not attenuate it. Innovation — technology, broadly speaking — is the only way to grow the pie, and to solve the problems we face that actually exist in any sort of knowable way, from climate change to China, from pandemics to poverty, and from diseases to demographics. To attack the solution is denialism at best, outright sabotage at worst. Indeed, the shoggoth to fear is our societal sclerosis seeking to drag the most exciting new technology in years into an innovation anti-pattern.\nPhoto generated by Dall-E 3, with the following prompt: “Photo of a radiant, downscaled city teetering on the brink of an expansive abyss, with a dark, murky quagmire below containing decayed structures reminiscent of historic landmarks. The city is a beacon of the future, with flying cars, green buildings, and residents in futuristic attire. The influence of AI is subtly interwoven, with robots helping citizens and digital screens integrated into the environment. Below, the haunting silhouette of a shoggoth, with its eerie tendrils, endeavors to pull the city into the depths, illustrating the clash between forward-moving evolution and outdated forces.”\n\n\t\t\t",
      "subsections": [
        {
          "subtitle": "A Cynical Read on AI Alarm",
          "contentHtml": "<p>Last week in a <a href=\"https://stratechery.com/2023/an-interview-with-gregory-allen-about-the-updated-china-chip-ban/\">Stratechery Interview with Gregory Allen about the chip ban</a> we discussed why Washington D.C. suddenly had so much urgency about AI. The first reason was of course ChatGPT; it was the second, though, that set off alarm bells in my head. Here’s Allen:</p><blockquote><p>\n  The other thing that’s happened that I do think is important just for folks to understand is, that&nbsp;<a href=\"https://www.safe.ai/press-release\">Center for AI Safety letter that came out</a>, that was signed by Sam Altman, that was signed by a bunch of other folks that said, “The risks of AI, including the risks of human extinction, should be viewed in the same light as nuclear weapons and pandemics.” The list of signatories to that letter was quite illustrious and quite long, and it’s really difficult to overstate the impact that that letter had on Washington, D. C. When you have the CEO of all these companies…when you get that kind of roster saying, “When you think of my technology, think of nuclear weapons,” you definitely get Washington’s attention.\n</p></blockquote><p>It turns out you get more than that: on Monday the Biden administration released an <a href=\"https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/\">Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence</a>. This Executive Order goes far beyond setting up a commission or study about AI, a field that is obviously still under rapid development; instead it goes straight to proscription.</p><p>Before I get to the executive order, though, I want to go back to Gates: that video at the top, where he blamed the Department of Justice for Microsoft having missed mobile, was the first thing I thought of during my interview with Allen. The fact of the matter is that Gates is the single most <em>unreliable</em> narrator about why Microsoft missed mobile, precisely because he was so intimately involved in the effort.</p><p>By the time that interview happened in 2019, it was obvious to everyone that Microsoft had utterly failed in mobile, and that it cost the company billions of dollars along the way. It is exceptionally difficult, particularly for someone as intelligent and successful as Gates, to admit the obvious truth: Microsoft missed mobile because Microsoft approached the space with the entirely wrong paradigm in mind. Or, to be more blunt, Gates got it wrong. It is much easier to blame someone else than to face that failure, particularly when the federal government is sitting right there!</p><p>In short, it is always necessary to carefully examine the motivations of a self-interested actor, and that certainly applies to the letter Allen referenced.</p><hr><p>To rewind just a bit, last January I wrote <a href=\"https://stratechery.com/2023/ai-and-the-big-five/\">AI and the Big Five</a>, which posited that the initial wave of generative AI would largely benefit the dominant tech companies. Apple’s strategy was unclear, but it controlled the devices via which AI would be accessed, and had the potential to benefit even more if AI could be run locally. Amazon had AWS, which held much of the data over which companies might wish to apply AI, but also lacked its own foundational models. Google likely had the greatest capabilities, but also the greatest business model challenges. Meta controlled the apps through which consumers might be most likely to encounter AI generated content. Microsoft, meanwhile, thanks to its partnership with OpenAI, was the best placed to ride the initial wave generated by ChatGPT.</p><p>Nine months later and the Article holds up well: Apple is releasing ever more powerful devices, but still lacks a clear strategy; Amazon spent <a href=\"https://stratechery.com/2023/amazon-earnings-the-logistics-virtuous-cycle-amazon-aggregator-ads/\">its last earnings call</a> trying to convince investors that AI applications would come to their data, and talking up its partnership with Anthropic, OpenAI’s biggest competitor; Google has demonstrated great technology but has been slow to ship; Meta is pushing ahead with generative AI in its apps; and Microsoft is actually registering <a href=\"https://stratechery.com/2023/google-earnings-microsoft-earnings-ai-leverage/\">meaningful financial impact</a> from its OpenAI partnership.</p><p>With this as context, it’s interesting to consider who signed that letter Allen referred to, which stated:</p><blockquote><p>\n  Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\n</p></blockquote><p>There are 30 signatories from OpenAI, including the aforementioned CEO Sam Altman. There are 15 signatories from Anthropic, including CEO Dario Amodei. There are seven signatories from Microsoft, including CTO Kevin Scott. There are 81 signatories from Google, including Google DeepMind CEO Demis Hassabis. There are none from Apple or Amazon, and two low-level employees from Meta.</p><p>What is striking about this tally is the extent to which the totals and prominence align to the relative companies’ current position in the market. OpenAI has the lead, at least in terms of consumer and developer mindshare, and the company is deriving real revenue from ChatGPT; Anthropic is second, and has signed deals with both Google and Amazon. Google has great products and an internal paralysis around shipping them for business model reasons; urging caution is very much in their interest. Microsoft is in the middle: it is making money from AI, but it doesn’t control its own models; Apple and Amazon are both waiting for the market to come to them.</p><p>In this ultra-cynical analysis the biggest surprise is probably Meta: the company has its own models, but no one of prominence has signed. These models, though, have been <a href=\"https://stratechery.com/2023/free-meta-open-sources-another-ai-model-moats-and-open-source-apple-and-meta/\">gradually open-sourced</a>: Meta is betting on distributed innovation to generate value that will best be captured via the consumer touchpoints the the company controls.</p><p>The point is this: if you accept the premise that regulation locks in incumbents, then it sure is notable that the early AI winners seem the most invested in generating alarm in Washington, D.C. about AI. This despite the fact that their concern is apparently not sufficiently high to, you know, stop their work. No, they are the responsible ones, the ones who care enough to call for regulation; all the better if concerns about imagined harms kneecap inevitable competitors.</p>",
          "contentText": "Last week in a Stratechery Interview with Gregory Allen about the chip ban we discussed why Washington D.C. suddenly had so much urgency about AI. The first reason was of course ChatGPT; it was the second, though, that set off alarm bells in my head. Here’s Allen:\n  The other thing that’s happened that I do think is important just for folks to understand is, that Center for AI Safety letter that came out, that was signed by Sam Altman, that was signed by a bunch of other folks that said, “The risks of AI, including the risks of human extinction, should be viewed in the same light as nuclear weapons and pandemics.” The list of signatories to that letter was quite illustrious and quite long, and it’s really difficult to overstate the impact that that letter had on Washington, D. C. When you have the CEO of all these companies…when you get that kind of roster saying, “When you think of my technology, think of nuclear weapons,” you definitely get Washington’s attention.\nIt turns out you get more than that: on Monday the Biden administration released an Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. This Executive Order goes far beyond setting up a commission or study about AI, a field that is obviously still under rapid development; instead it goes straight to proscription.Before I get to the executive order, though, I want to go back to Gates: that video at the top, where he blamed the Department of Justice for Microsoft having missed mobile, was the first thing I thought of during my interview with Allen. The fact of the matter is that Gates is the single most unreliable narrator about why Microsoft missed mobile, precisely because he was so intimately involved in the effort.By the time that interview happened in 2019, it was obvious to everyone that Microsoft had utterly failed in mobile, and that it cost the company billions of dollars along the way. It is exceptionally difficult, particularly for someone as intelligent and successful as Gates, to admit the obvious truth: Microsoft missed mobile because Microsoft approached the space with the entirely wrong paradigm in mind. Or, to be more blunt, Gates got it wrong. It is much easier to blame someone else than to face that failure, particularly when the federal government is sitting right there!In short, it is always necessary to carefully examine the motivations of a self-interested actor, and that certainly applies to the letter Allen referenced.To rewind just a bit, last January I wrote AI and the Big Five, which posited that the initial wave of generative AI would largely benefit the dominant tech companies. Apple’s strategy was unclear, but it controlled the devices via which AI would be accessed, and had the potential to benefit even more if AI could be run locally. Amazon had AWS, which held much of the data over which companies might wish to apply AI, but also lacked its own foundational models. Google likely had the greatest capabilities, but also the greatest business model challenges. Meta controlled the apps through which consumers might be most likely to encounter AI generated content. Microsoft, meanwhile, thanks to its partnership with OpenAI, was the best placed to ride the initial wave generated by ChatGPT.Nine months later and the Article holds up well: Apple is releasing ever more powerful devices, but still lacks a clear strategy; Amazon spent its last earnings call trying to convince investors that AI applications would come to their data, and talking up its partnership with Anthropic, OpenAI’s biggest competitor; Google has demonstrated great technology but has been slow to ship; Meta is pushing ahead with generative AI in its apps; and Microsoft is actually registering meaningful financial impact from its OpenAI partnership.With this as context, it’s interesting to consider who signed that letter Allen referred to, which stated:\n  Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\nThere are 30 signatories from OpenAI, including the aforementioned CEO Sam Altman. There are 15 signatories from Anthropic, including CEO Dario Amodei. There are seven signatories from Microsoft, including CTO Kevin Scott. There are 81 signatories from Google, including Google DeepMind CEO Demis Hassabis. There are none from Apple or Amazon, and two low-level employees from Meta.What is striking about this tally is the extent to which the totals and prominence align to the relative companies’ current position in the market. OpenAI has the lead, at least in terms of consumer and developer mindshare, and the company is deriving real revenue from ChatGPT; Anthropic is second, and has signed deals with both Google and Amazon. Google has great products and an internal paralysis around shipping them for business model reasons; urging caution is very much in their interest. Microsoft is in the middle: it is making money from AI, but it doesn’t control its own models; Apple and Amazon are both waiting for the market to come to them.In this ultra-cynical analysis the biggest surprise is probably Meta: the company has its own models, but no one of prominence has signed. These models, though, have been gradually open-sourced: Meta is betting on distributed innovation to generate value that will best be captured via the consumer touchpoints the the company controls.The point is this: if you accept the premise that regulation locks in incumbents, then it sure is notable that the early AI winners seem the most invested in generating alarm in Washington, D.C. about AI. This despite the fact that their concern is apparently not sufficiently high to, you know, stop their work. No, they are the responsible ones, the ones who care enough to call for regulation; all the better if concerns about imagined harms kneecap inevitable competitors."
        },
        {
          "subtitle": "An Executive Order on Attenuating Innovation",
          "contentHtml": "<p>There is another quote I thought of this week. It was delivered by Senator Amy Klobuchar <a href=\"https://twitter.com/amyklobuchar/status/1484566074104426497\">in a tweet</a>:</p><p><a href=\"https://twitter.com/amyklobuchar/status/1484566074104426497\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?resize=640%2C372&amp;ssl=1\" alt=\"A tweet from Senator Klobuchar celebrating tech regulation\" width=\"640\" height=\"372\" class=\"aligncenter size-full wp-image-11821\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?resize=300%2C174&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?resize=1024%2C594&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?resize=768%2C446&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-1.png?resize=1085%2C630&amp;ssl=1 1085w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p><p>I wrote at the time <a href=\"https://stratechery.com/2022/free-netflix-follow-up-the-senate-tech-bill/\">in an Update</a>:</p><blockquote><p>\n  In 1991 — assuming that the “dawn of the Internet” was the launch of the World Wide Web — the following were the biggest companies by market cap:</p>\n<ol>\n<li><strong>$88 billion</strong> — General Electric </li>\n<li><strong>$80 billion</strong> — Exxon Mobil</li>\n<li><strong>$62 billion</strong> — Walmart</li>\n<li><strong>$54 billion</strong> — Coca-Cola</li>\n<li><strong>$42 billion</strong> —&nbsp;Merck</li>\n</ol>\n<p>  The only tech company in the top 10 was IBM, with a $31 billion market cap. Imagine proposing a bill then targeting companies with greater than $550 billion market caps, knowing that it is nothing but tech companies!</p>\n<p>  What doesn’t occur to Senator Klobuchar is the possibility that the relationship between the massive increase in wealth, and even greater gain in consumer welfare, produced by tech companies since the “dawn of the Internet” may in fact be related to the fact that there hasn’t been any major regulation (the most important piece of regulation, <a href=\"https://stratechery.com/topic/regulation/section-230/\">Section 230</a>, protected the Internet from lawsuits; this legislation invites them). I’m not saying that the lack of regulation is causal, but I am exceptionally skeptical that we would have had <em>more</em> growth with <em>more</em> regulation.</p>\n<p>  More broadly, tech sure seems like the only area where innovation and building is happening anywhere in the West. This isn’t to deny that the big tech companies aren’t sometimes bad actors, and that platforms in particular do, at least in theory, need regulation. But given the sclerosis present everywhere <em>but</em> tech it sure seems like it would be prudent to be exceptionally skeptical about the prospect of new regulation; I definitely wouldn’t be celebrating it as if it were some sort of overdue accomplishment.\n</p></blockquote><p>Unfortunately this week’s Executive Order takes the exact opposite approach to AI that we took to technology previously. As Steven Sinofsky explains in <a href=\"https://hardcoresoftware.learningbyshipping.com/p/211-regulating-ai-by-executive-order\">this excellent article</a>:</p><blockquote><p>\n  This document is the work of aggregating policy inputs from an extended committee of interested constituencies while also navigating the law — literally what is it that can be done to throttle artificial intelligence legally without passing any new laws that might throttle artificial intelligence. There is no clear owner of this document. There is no leading science consensus or direction that we can discern. It is impossible to separate out the document from the process and approach used to “govern” AI innovation. Govern is quoted because it is the word used in the EO. This is so much less a document of what should be done with the potential of technology than it is a document pushing the limits of what can be done legally to slow innovation.\n</p></blockquote><p>Much attention has been focused on the Executive Order’s ultra-specific limits on model sizes and attributes (you can exceed those limits if you are registered and approved, a game best played by large established companies like the list I just detailed); unfortunately that is only the beginning of the issues with this Executive Order, but again, I urge you to read <a href=\"https://hardcoresoftware.learningbyshipping.com/p/211-regulating-ai-by-executive-order\">Sinofsky’s post</a>.</p><p>What is so disappointing to me is how utterly opposed this executive order is to how innovation actually happens:</p><ul>\n<li>The Biden administration is not embracing uncertainty: it is operating from an assumption that AI is dangerous, despite the fact that many of the listed harms, like learning how to build a bomb or synthesize dangerous chemicals or conduct cyber attacks, are already trivially accomplished on today’s Internet. What is completely lacking is anything other than the briefest of hand waves at AI’s potential upside. The government is Bill Gates, imagining what might be possible, when it ought to be Steve Jobs, humble enough to know it cannot predict the future.</li>\n<li>The Biden administration is operating with a fundamental lack of trust in the capability of humans to invent new things, not just technologies, but also use cases, many of which will create new jobs. It can envision how the spreadsheet might imperil bookkeepers, but it can’t imagine how that same tool might unlock entire new industries.</li>\n<li>The Biden administration is arrogantly insisting that it ought have a role in dictating the outcomes of an innovation that few if any of its members understand, and almost certainly could not invent. There is, to be sure, a role for oversight and regulation, but that is a blunt instrument best applied after the invention, like an editor.</li>\n</ul><p>In short, this Executive Order is a lot like Gates’ approach to mobile: rooted in the past, yet arrogant about an unknowable future; proscriptive instead of adaptive; and, worst of all, trivially influenced by motivated reasoning best understood as some of the most cynical attempts at regulatory capture the tech industry has ever seen.</p>",
          "contentText": "There is another quote I thought of this week. It was delivered by Senator Amy Klobuchar in a tweet:I wrote at the time in an Update:\n  In 1991 — assuming that the “dawn of the Internet” was the launch of the World Wide Web — the following were the biggest companies by market cap:\n\n$88 billion — General Electric \n$80 billion — Exxon Mobil\n$62 billion — Walmart\n$54 billion — Coca-Cola\n$42 billion — Merck\n\n  The only tech company in the top 10 was IBM, with a $31 billion market cap. Imagine proposing a bill then targeting companies with greater than $550 billion market caps, knowing that it is nothing but tech companies!\n  What doesn’t occur to Senator Klobuchar is the possibility that the relationship between the massive increase in wealth, and even greater gain in consumer welfare, produced by tech companies since the “dawn of the Internet” may in fact be related to the fact that there hasn’t been any major regulation (the most important piece of regulation, Section 230, protected the Internet from lawsuits; this legislation invites them). I’m not saying that the lack of regulation is causal, but I am exceptionally skeptical that we would have had more growth with more regulation.\n  More broadly, tech sure seems like the only area where innovation and building is happening anywhere in the West. This isn’t to deny that the big tech companies aren’t sometimes bad actors, and that platforms in particular do, at least in theory, need regulation. But given the sclerosis present everywhere but tech it sure seems like it would be prudent to be exceptionally skeptical about the prospect of new regulation; I definitely wouldn’t be celebrating it as if it were some sort of overdue accomplishment.\nUnfortunately this week’s Executive Order takes the exact opposite approach to AI that we took to technology previously. As Steven Sinofsky explains in this excellent article:\n  This document is the work of aggregating policy inputs from an extended committee of interested constituencies while also navigating the law — literally what is it that can be done to throttle artificial intelligence legally without passing any new laws that might throttle artificial intelligence. There is no clear owner of this document. There is no leading science consensus or direction that we can discern. It is impossible to separate out the document from the process and approach used to “govern” AI innovation. Govern is quoted because it is the word used in the EO. This is so much less a document of what should be done with the potential of technology than it is a document pushing the limits of what can be done legally to slow innovation.\nMuch attention has been focused on the Executive Order’s ultra-specific limits on model sizes and attributes (you can exceed those limits if you are registered and approved, a game best played by large established companies like the list I just detailed); unfortunately that is only the beginning of the issues with this Executive Order, but again, I urge you to read Sinofsky’s post.What is so disappointing to me is how utterly opposed this executive order is to how innovation actually happens:\nThe Biden administration is not embracing uncertainty: it is operating from an assumption that AI is dangerous, despite the fact that many of the listed harms, like learning how to build a bomb or synthesize dangerous chemicals or conduct cyber attacks, are already trivially accomplished on today’s Internet. What is completely lacking is anything other than the briefest of hand waves at AI’s potential upside. The government is Bill Gates, imagining what might be possible, when it ought to be Steve Jobs, humble enough to know it cannot predict the future.\nThe Biden administration is operating with a fundamental lack of trust in the capability of humans to invent new things, not just technologies, but also use cases, many of which will create new jobs. It can envision how the spreadsheet might imperil bookkeepers, but it can’t imagine how that same tool might unlock entire new industries.\nThe Biden administration is arrogantly insisting that it ought have a role in dictating the outcomes of an innovation that few if any of its members understand, and almost certainly could not invent. There is, to be sure, a role for oversight and regulation, but that is a blunt instrument best applied after the invention, like an editor.\nIn short, this Executive Order is a lot like Gates’ approach to mobile: rooted in the past, yet arrogant about an unknowable future; proscriptive instead of adaptive; and, worst of all, trivially influenced by motivated reasoning best understood as some of the most cynical attempts at regulatory capture the tech industry has ever seen."
        },
        {
          "subtitle": "The Sclerotic Shiggoth",
          "contentHtml": "<p>I fully endorse Sinofsky’s conclusion:</p><blockquote><p>\n  This approach to regulation is not about innovation despite all the verbiage proclaiming it to be. This Order is about stifling innovation and turning the next platform over to incumbents in the US and far more likely new companies in other countries that did not see it as a priority to halt innovation before it even happens.</p>\n<p>  I am by no means certain if AI is the next technology platform the likes of which will make the smartphone revolution that has literally benefitted every human on earth look small. I don’t know sitting here today if the AI products just in market less than a year are the next biggest thing ever. They may turn out to be a way stop on the trajectory of innovation. They may turn out to be ingredients that everyone incorporates into existing products. There are so many things that we do not yet know.</p>\n<p>  What we do know is that we are at the very earliest stages. We simply have no in-market products, and that means no in-market problems, upon which to base such concerns of fear and need to “govern” regulation. Alarmists or “existentialists” say they have enough evidence. If that’s the case then then so be it, but then the only way to truly make that case is to embark on the legislative process and use democracy to validate those concerns. I just know that we have plenty of past evidence that every technology has come with its alarmists and concerns and somehow optimism prevailed. Why should the pessimists prevail now?\n</p></blockquote><p>They should not. We should accelerate innovation, not attenuate it. Innovation —&nbsp;technology, broadly speaking — is the only way to grow the pie, and to solve the problems we face that actually exist in any sort of knowable way, from climate change to China, from pandemics to poverty, and from diseases to demographics. To attack the solution is denialism at best, outright sabotage at worst. Indeed, the shoggoth to fear is our societal sclerosis seeking to drag the most exciting new technology in years into an innovation anti-pattern.</p><div align=\"center\"><figure id=\"attachment_11831\" aria-describedby=\"caption-attachment-11831\" style=\"width: 1024px\" class=\"wp-caption alignnone\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?resize=640%2C640&amp;ssl=1\" alt=\"Photo of a radiant, downscaled city teetering on the brink of an expansive abyss, with a dark, murky quagmire below containing decayed structures reminiscent of historic landmarks. The city is a beacon of the future, with flying cars, green buildings, and residents in futuristic attire. The influence of AI is subtly interwoven, with robots helping citizens and digital screens integrated into the environment. Below, the haunting silhouette of a shoggoth, with its eerie tendrils, endeavors to pull the city into the depths, illustrating the clash between forward-moving evolution and outdated forces.\" width=\"640\" height=\"640\" class=\"size-full wp-image-11831\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?w=1024&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/11/attentuatinginnovation-4.png?resize=630%2C630&amp;ssl=1 630w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"><figcaption id=\"caption-attachment-11831\" class=\"wp-caption-text\">Photo generated by Dall-E 3, with the following prompt: “Photo of a radiant, downscaled city teetering on the brink of an expansive abyss, with a dark, murky quagmire below containing decayed structures reminiscent of historic landmarks. The city is a beacon of the future, with flying cars, green buildings, and residents in futuristic attire. The influence of AI is subtly interwoven, with robots helping citizens and digital screens integrated into the environment. Below, the haunting silhouette of a shoggoth, with its eerie tendrils, endeavors to pull the city into the depths, illustrating the clash between forward-moving evolution and outdated forces.”</figcaption></figure></div>",
          "contentText": "I fully endorse Sinofsky’s conclusion:\n  This approach to regulation is not about innovation despite all the verbiage proclaiming it to be. This Order is about stifling innovation and turning the next platform over to incumbents in the US and far more likely new companies in other countries that did not see it as a priority to halt innovation before it even happens.\n  I am by no means certain if AI is the next technology platform the likes of which will make the smartphone revolution that has literally benefitted every human on earth look small. I don’t know sitting here today if the AI products just in market less than a year are the next biggest thing ever. They may turn out to be a way stop on the trajectory of innovation. They may turn out to be ingredients that everyone incorporates into existing products. There are so many things that we do not yet know.\n  What we do know is that we are at the very earliest stages. We simply have no in-market products, and that means no in-market problems, upon which to base such concerns of fear and need to “govern” regulation. Alarmists or “existentialists” say they have enough evidence. If that’s the case then then so be it, but then the only way to truly make that case is to embark on the legislative process and use democracy to validate those concerns. I just know that we have plenty of past evidence that every technology has come with its alarmists and concerns and somehow optimism prevailed. Why should the pessimists prevail now?\nThey should not. We should accelerate innovation, not attenuate it. Innovation — technology, broadly speaking — is the only way to grow the pie, and to solve the problems we face that actually exist in any sort of knowable way, from climate change to China, from pandemics to poverty, and from diseases to demographics. To attack the solution is denialism at best, outright sabotage at worst. Indeed, the shoggoth to fear is our societal sclerosis seeking to drag the most exciting new technology in years into an innovation anti-pattern.Photo generated by Dall-E 3, with the following prompt: “Photo of a radiant, downscaled city teetering on the brink of an expansive abyss, with a dark, murky quagmire below containing decayed structures reminiscent of historic landmarks. The city is a beacon of the future, with flying cars, green buildings, and residents in futuristic attire. The influence of AI is subtly interwoven, with robots helping citizens and digital screens integrated into the environment. Below, the haunting silhouette of a shoggoth, with its eerie tendrils, endeavors to pull the city into the depths, illustrating the clash between forward-moving evolution and outdated forces.”"
        }
      ]
    },
    {
      "title": "China Chips and Moore’s Law",
      "publishedDate": "2023-10-18T08:18:14-07:00",
      "updatedDate": "2023-10-18T09:58:18-07:00",
      "contentHtml": "\n\t\t<div>\n<hr>\n</div>\n<div align=\"center\"><strong>The complexity for minimum component costs has increased at a rate of roughly a factor of two per year (see graph on next page). Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for at least 10 years.</strong><br>\n— <em>Gordon Moore, <a href=\"http://cva.stanford.edu/classes/cs99s/papers/moore-crammingmorecomponents.pdf\">Cramming More Components Onto Integrated Circuits</a></em></div>\n<div></div>\n<p><a href=\"http://cva.stanford.edu/classes/cs99s/papers/moore-crammingmorecomponents.pdf\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/10/CleanShot-2023-10-18-at-22.43.12@2x.png?resize=640%2C335&amp;ssl=1\" alt=\"Gordon Moore's illustration of Moore's Law\" width=\"640\" height=\"335\" class=\"aligncenter size-full wp-image-11729\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/10/CleanShot-2023-10-18-at-22.43.12@2x.png?w=2548&amp;ssl=1 2548w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/10/CleanShot-2023-10-18-at-22.43.12@2x.png?resize=300%2C157&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/10/CleanShot-2023-10-18-at-22.43.12@2x.png?resize=1024%2C535&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/10/CleanShot-2023-10-18-at-22.43.12@2x.png?resize=768%2C401&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/10/CleanShot-2023-10-18-at-22.43.12@2x.png?resize=1536%2C803&amp;ssl=1 1536w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/10/CleanShot-2023-10-18-at-22.43.12@2x.png?resize=2048%2C1071&amp;ssl=1 2048w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/10/CleanShot-2023-10-18-at-22.43.12@2x.png?resize=1200%2C627&amp;ssl=1 1200w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/10/CleanShot-2023-10-18-at-22.43.12@2x.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/10/CleanShot-2023-10-18-at-22.43.12@2x.png?w=1920&amp;ssl=1 1920w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<div>\n<hr>\n</div>\n<div align=\"center\"><strong>Moore’s law is dead.</strong><br>\n– <em>Jensen Huang</em></div>\n<div>\n<hr>\n</div>\n<p>On Tuesday <a href=\"https://www.wsj.com/tech/u-s-tightens-curbs-on-ai-chip-exports-to-china-widening-rift-with-u-s-businesses-3b9983df\">the Biden administration tightened export controls</a> for advanced AI chips being sold to China; the primary target was Nvidia’s H800 and A800 chips, which were specifically designed <a href=\"https://stratechery.com/2022/china-chip-ban-clarifications-nvidias-a800-china-intel-and-tower/\">to skirt controls</a> put in place last year. The primary difference between the H800/A800 and H100/A100 is the bandwidth of their interconnects: the A100 had 600 Gb/s interconnects (the H100 has 900GB/s), which just so happened to be the limit proscribed by last year’s export controls; the A800 and H800 were limited to 400 Gb/s interconnects.</p>\n<p>The reason why interconnect speed matters is tied up with Nvidia CEO Jensen Huang’s thesis that Moore’s Law is dead. Moore’s Law, as originally stated in 1965, states that the number of transistors in an integrated circuit would double every year. Moore revised his prediction 10 years later to be a doubling every two years, which held until the last decade or so, when it has slowed to a doubling about every three years.</p>\n<p>In practice, though, Moore’s Law has become something more akin to a fundamental precept underlying the tech industry: computing power will both increase <em>and</em> get cheaper over time. This precept — which I will call Moore’s Precept, for clarity — is connected to Moore’s technical prediction: smaller transistors can switch faster, and use less energy in the switching, even as more of them fit on a single wafer; this means that you can either get more chips per wafer or larger chips, either decreasing price or increasing power for the same price. In practice we got both.</p>\n<p>What is critical is that the rest of the tech industry didn’t need to understand the technical or economic details of Moore’s Law: for 60 years it has been safe to simply assume that computers would get faster, which meant the optimal approach was always to build for the cutting edge or just beyond, and trust that processor speed would catch up to your use case. From an analyst perspective, it is Moore’s Precept that enables me to write speculative articles like <a href=\"https://stratechery.com/2023/ai-hardware-and-virtual-reality/\">AI, Hardware, and Virtual Reality</a>: it is enough to see that a use case is possible, if not yet optimal; Moore’s Precept will provide the optimization.</p>\n<h3>The End of Moore’s Precept?</h3>\n<p>This distinction between Moore’s Law and Moore’s Precept is the key to understanding Nvidia CEO Jensen Huang’s repeated declarations that Moore’s Law is dead. From a technical perspective, it has certainly slowed, but density continues to increase; here is TSMC’s transistor density by node size, using the first (i.e. worse) iteration of each node size:<sup id=\"rf1-11727\"><a href=\"#fn1-11727\" title=\"I am not certain I have the exact right numbers for older nodes, but I have confirmed that the numbers are in the right ballpark\" rel=\"footnote\">1</a></sup></p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">TSMC</th>\n<th align=\"center\">Transistor Density (MTr/mm)</th>\n<th align=\"center\">Year Introduced</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">90 nm</td>\n<td align=\"center\">3.4</td>\n<td align=\"center\">2004</td>\n</tr>\n<tr>\n<td align=\"center\">65 nm</td>\n<td align=\"center\">5.6</td>\n<td align=\"center\">2006</td>\n</tr>\n<tr>\n<td align=\"center\">40 nm</td>\n<td align=\"center\">9.8</td>\n<td align=\"center\">2008</td>\n</tr>\n<tr>\n<td align=\"center\">28 nm</td>\n<td align=\"center\">16.6</td>\n<td align=\"center\">2011</td>\n</tr>\n<tr>\n<td align=\"center\">20 nm</td>\n<td align=\"center\">20.9</td>\n<td align=\"center\">2014</td>\n</tr>\n<tr>\n<td align=\"center\">16 nm</td>\n<td align=\"center\">28.9</td>\n<td align=\"center\">2015</td>\n</tr>\n<tr>\n<td align=\"center\">10 nm</td>\n<td align=\"center\">52.5</td>\n<td align=\"center\">2017</td>\n</tr>\n<tr>\n<td align=\"center\">7 nm</td>\n<td align=\"center\">91.2</td>\n<td align=\"center\">2019</td>\n</tr>\n<tr>\n<td align=\"center\">5 nm</td>\n<td align=\"center\">138.2</td>\n<td align=\"center\">2020</td>\n</tr>\n</tbody>\n</table>\n<p>Remember, though, that cost matters; here is the same table with TSMC’s introductory price/wafer, and what that translates to in terms of price/billion transistors:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">TSMC</th>\n<th align=\"center\">MTr/mm</th>\n<th align=\"center\">Year Introduced</th>\n<th align=\"center\">Price/Wafer</th>\n<th align=\"center\">Price/BTr</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">90 nm</td>\n<td align=\"center\">3.4</td>\n<td align=\"center\">2004</td>\n<td align=\"center\">$1,650</td>\n<td align=\"center\">$6.87</td>\n</tr>\n<tr>\n<td align=\"center\">65 nm</td>\n<td align=\"center\">5.6</td>\n<td align=\"center\">2006</td>\n<td align=\"center\">$1,937</td>\n<td align=\"center\">$4.89</td>\n</tr>\n<tr>\n<td align=\"center\">40 nm</td>\n<td align=\"center\">9.8</td>\n<td align=\"center\">2008</td>\n<td align=\"center\">$2,274</td>\n<td align=\"center\">$3.28</td>\n</tr>\n<tr>\n<td align=\"center\">28 nm</td>\n<td align=\"center\">16.6</td>\n<td align=\"center\">2011</td>\n<td align=\"center\">$2,891</td>\n<td align=\"center\">$2.46</td>\n</tr>\n<tr>\n<td align=\"center\">20 nm</td>\n<td align=\"center\">20.9</td>\n<td align=\"center\">2014</td>\n<td align=\"center\">$3,677</td>\n<td align=\"center\">$2.49</td>\n</tr>\n<tr>\n<td align=\"center\">16 nm</td>\n<td align=\"center\">28.9</td>\n<td align=\"center\">2015</td>\n<td align=\"center\">$3,984</td>\n<td align=\"center\">$1.95</td>\n</tr>\n<tr>\n<td align=\"center\">10 nm</td>\n<td align=\"center\">52.5</td>\n<td align=\"center\">2017</td>\n<td align=\"center\">$5,992</td>\n<td align=\"center\">$1.61</td>\n</tr>\n<tr>\n<td align=\"center\">7 nm</td>\n<td align=\"center\">91.2</td>\n<td align=\"center\">2019</td>\n<td align=\"center\">$9,346</td>\n<td align=\"center\">$1.45</td>\n</tr>\n<tr>\n<td align=\"center\">5 nm</td>\n<td align=\"center\">138.2</td>\n<td align=\"center\">2020</td>\n<td align=\"center\">$16,988</td>\n<td align=\"center\">$1.74</td>\n</tr>\n</tbody>\n</table>\n<p>Notice that number on the bottom right: with TSMC’s 5 nm process the price per transistor increased — and it increased <em>a lot</em> (20%). The reason was obvious: 5 nm was the first process that required ASML’s extreme ultraviolet (EUV) lithography, and EUV machines were hugely expensive — around $150 million each.<sup id=\"rf2-11727\"><a href=\"#fn2-11727\" title=\"TSMC first used EUV with latter iterations of its 7nm process, but that was primarily to move down the learning curve; EUV was not strictly necessary, and the original 7nm process used immersion DUV lithography exclusively\" rel=\"footnote\">2</a></sup> In other words, it appeared that while the technical definition of Moore’s Law would continue, the precept that chips would always get both faster and cheaper would not.</p>\n<h3>GPUs and Embarrassing Parallelism</h3>\n<p>Huang’s argument, to be clear, does not simply rest on the cost of 5 nm chips; remember Moore’s Precept is about speed as well as cost, and the truth is that a lot of those density gains have primarily gone towards power efficiency as energy became a constraint in everything from mobile to PCs to data centers. Huang’s thesis for several years now is that Nvidia has the solution to making computing <em>faster</em>: use GPUs.</p>\n<p>GPUs are much less complex than CPUs; that means they can execute instructions much more quickly, but those instructions have to be much simpler. At the same time, you can run a lot of them at the same time to achieve outsized results. Graphics is, unsurprisingly, the most obvious example: every “shader” — the primary processing component of a GPU — calculates what will be displayed on a single portion of the screen; the size of the portion is a function of how many shaders you have available. If you have 1,024 shaders, each shader draws 1/1,024 of the screen. Ergo, if you have 2,048 shaders, you can draw the screen twice as fast. Graphics performance is “embarrassingly parallel”, which is to say it scales with the number of processors you apply to the problem.</p>\n<p>This “embarrassing parallelism” is the key to GPUs outsized performance relative to CPUs, but the challenge is that not all software problems are easily parallel-izable; Nvida’s CUDA ecosystem is predicated on providing the tools to build software applications that can leverage GPU parallelism, and is one of the major moats undergirding Nvidia’s dominance, but most software applications still need the complexity of CPUs to run.</p>\n<p>AI, though, is not most software. It turns out that AI, both in terms of training models and in leveraging them (i.e. inference) is an embarrassingly parallel application. Moreover, the optimum amount of scalability goes far beyond a computer monitor displaying graphics; this is why Nvidia AI chips feature the high-speed interconnects referenced by the chip ban: AI applications run across multiple AI chips at the same time, but the key to making sure those GPUs are busy is feeding them with data, and that requires those high speed interconnects.</p>\n<p>That noted, I’m skeptical about the wholesale shift of traditional data center applications to GPUs; from <a href=\"https://stratechery.com/2023/nvidia-on-the-mountaintop/\">Nvidia On the Mountaintop</a>:</p>\n<blockquote><p>\n  Humans — and companies — are lazy, and not only are CPU-based applications easier to develop, they are also mostly already built. I have a hard time seeing what companies are going to go through the time and effort to port things that already run on CPUs to GPUs; at the end of the day, the applications that run in a cloud are determined by customers who provide the demand for cloud resources, not cloud providers looking to optimize FLOP/rack.\n</p></blockquote>\n<p>There’s another reason to think that traditional CPUs still have some life in them as well: it turns out that Moore’s Precept may be back on track.</p>\n<h3>EUV and Moore’s Precept</h3>\n<p>The table I posted above only ran through 5 nm; the iPhone 15 Pro, though, has an N3 chip, and check out the price/transistor:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">TSMC</th>\n<th align=\"center\">MTr/mm</th>\n<th align=\"center\">Year Introduced</th>\n<th align=\"center\">Price/Wafer</th>\n<th align=\"center\">Price/BTr</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">90 nm</td>\n<td align=\"center\">3.4</td>\n<td align=\"center\">2004</td>\n<td align=\"center\">$1,650</td>\n<td align=\"center\">$6.87</td>\n</tr>\n<tr>\n<td align=\"center\">65 nm</td>\n<td align=\"center\">5.6</td>\n<td align=\"center\">2006</td>\n<td align=\"center\">$1,937</td>\n<td align=\"center\">$4.89</td>\n</tr>\n<tr>\n<td align=\"center\">40 nm</td>\n<td align=\"center\">9.8</td>\n<td align=\"center\">2008</td>\n<td align=\"center\">$2,274</td>\n<td align=\"center\">$3.28</td>\n</tr>\n<tr>\n<td align=\"center\">28 nm</td>\n<td align=\"center\">16.6</td>\n<td align=\"center\">2011</td>\n<td align=\"center\">$2,891</td>\n<td align=\"center\">$2.46</td>\n</tr>\n<tr>\n<td align=\"center\">20 nm</td>\n<td align=\"center\">20.9</td>\n<td align=\"center\">2014</td>\n<td align=\"center\">$3,677</td>\n<td align=\"center\">$2.49</td>\n</tr>\n<tr>\n<td align=\"center\">16 nm</td>\n<td align=\"center\">28.9</td>\n<td align=\"center\">2015</td>\n<td align=\"center\">$3,984</td>\n<td align=\"center\">$1.95</td>\n</tr>\n<tr>\n<td align=\"center\">10 nm</td>\n<td align=\"center\">52.5</td>\n<td align=\"center\">2017</td>\n<td align=\"center\">$5,992</td>\n<td align=\"center\">$1.61</td>\n</tr>\n<tr>\n<td align=\"center\">7 nm</td>\n<td align=\"center\">91.2</td>\n<td align=\"center\">2019</td>\n<td align=\"center\">$9,346</td>\n<td align=\"center\">$1.45</td>\n</tr>\n<tr>\n<td align=\"center\">5 nm</td>\n<td align=\"center\">138.2</td>\n<td align=\"center\">2020</td>\n<td align=\"center\">$16,988</td>\n<td align=\"center\">$1.74</td>\n</tr>\n<tr>\n<td align=\"center\">3 nm (N3B)</td>\n<td align=\"center\">197.0</td>\n<td align=\"center\">2023</td>\n<td align=\"center\">$20,000</td>\n<td align=\"center\">$1.44</td>\n</tr>\n<tr>\n<td align=\"center\">3 nm (N3E)</td>\n<td align=\"center\">215.6</td>\n<td align=\"center\">2023</td>\n<td align=\"center\">$20,000</td>\n<td align=\"center\">$1.31</td>\n</tr>\n</tbody>\n</table>\n<p>While I only included the first version of each node previously, the N3B process, which is used for the iPhone’s A17 Pro chip, <a href=\"https://fuse.wikichip.org/news/7048/n3e-replaces-n3-comes-in-many-flavors/\">is a dead-end</a>; TSMC changed its approach with the N3E, which will be the basis of the N3 family going forward. It also makes the N3 leap even more impressive in terms of price/transistor: while N3B undid the 5 nm backslide, N3E is a marked improvement over 7 nm.</p>\n<p>Moreover, the gains are actually what you would expect: yes, those EUV machines cost a lot, but the price decreases embedded in Moore’s Precept are not a function of equipment getting cheaper — notice that the price/wafer has been increasing continuously. Rather, ever declining prices/transistor are a function of Moore’s Law, which is to say that new equipment, like EUV, lets us “Cram[] More Components Onto Integrated Circuits”.</p>\n<p>What happened at 5 nm was similar to what happened at 20 nm, the last time the price/transistor increased: that was the node where TSMC started to use double-patterning, which means they had to do every lithography step twice; that both doubled the utilization of lithography equipment per wafer and also decreased yield. For that node, at least, the gains from making smaller transistors were outweighed by the costs. A year later, though, and TSMC launched the 16 nm node that re-united Moore’s Law with Moore’s Precept. That is exactly what seems to have happened with 3 nm —&nbsp;the gains of EUV are now significantly outweighing the costs — and early rumors about 2 nm density and price points suggests the gains should continue for another node.</p>\n<h3>Chip Ban Angst</h3>\n<p>All of this is interesting in its own right, but it’s particularly pertinent in light of the recent angst in Washington DC over Huawei’s recent smartphone with a 7 nm chip, seemingly in defiance of those export controls. I already explained why that angst was misguided in <a href=\"https://stratechery.com/2023/the-huawei-mate-60-pro-7nm-background-implications-and-reactions/\">this September Update</a>. To summarize my argument:</p>\n<ul>\n<li>TSMC had already shown that 7 nm chips could be made using deep ultraviolet (DUV)-based immersion lithography, and China had plenty of DUV lithography machines, given that DUV has been the standard for multiple generations of chips.</li>\n<li>China’s Semiconductor Manufacturing International Corp. (SMIC) had <a href=\"https://www.semianalysis.com/p/chinas-smic-is-shipping-7nm-foundry\">already made a 7 nm chip in 2022</a>; sure it was simpler than the one launched in that Huawei phone, but that is the exact sort of progression you should expect from a competent foundry.</li>\n<li>SMIC is almost certainly not producing that 7nm chip economically; Intel, for example, could make a 7nm chip using DUV, they just couldn’t do it economically, which is why they ultimately switched to EUV.</li>\n</ul>\n<p>In short, the problem with the chip ban was drawing the line at 10 nm: that line was arbitrary given that the equipment needed to make 10 nm chips had already been shown to be capable of producing 7 nm chips; that SMIC managed to do just that isn’t a surprise, and, crucially, is not evidence that the chip ban was a failure.</p>\n<p>The line that actually matters is 5 nm, which is another way to say that the export control that will actually limit China’s long-term development is EUV. Fortunately <a href=\"https://www.reuters.com/article/us-asml-holding-usa-china-insight-idUSKBN1Z50HN\">the Trump administration had already persuaded the Netherlands</a> to not allow the export of EUV machines, which the Biden administration further locked down with its chip ban and further coordination with the Netherlands. The reality is that a lot of chip-making equipment is “multi-nodal”; much of the machinery can be used at multiple nodes, but you must have EUV machines to realize Moore’s Precept, because it is the key piece of technology driving Moore’s Law.</p>\n<p>By the same token, the A800/H800 loophole was a real one: the H800 is made on TSMC’s third-generation 5 nm process (confusingly called N4), which is to say it is made with EUV; the interconnect limits were meaningful, and would make AI development slower and more costly (because those GPUs would be starved of data more of the time), but it didn’t halt it. This matters because AI is the military application the U.S. should be the most concerned with: a lot of military applications run perfectly fine on existing chips (or even, in the case of guided weaponry, chips that were made decades ago); wars of the future, though, will almost certainly be undergirded by AI, a field that is only just now getting started.</p>\n<p>This leads to a further point: the payoff from this chip ban will not come immediately. The only way the entire idea makes sense is if Moore’s Law continues to exist, because that means the chips that will be available in five or ten years will be that much faster and cheaper than the ones that exist today, increasing the gap. And, at the same time, the idea also depends on taking Huang’s argument seriously, because AI needs not just power but scale. Fortunately movement on both fronts is headed in the right direction.</p>\n<p>There remain good arguments against the entire concept of the chip ban, including the obvious fact that China is heavily incentivized to built up replacements from scratch (and could <a href=\"https://stratechery.com/2022/chips-and-china/\">have leverage on the U.S. on the trailing edge</a>): perhaps in 20 years the U.S. will not only have lost its most potent point of leverage but will also see its most cutting edge companies undercut by Chinese competition. That die, though, has long since been cast; the results that matter are not a smartphone in 2023, but the capabilities of 2030 and beyond.</p>\n<hr class=\"footnotes\"><ol class=\"footnotes\" style=\"list-style-type:decimal\"><li id=\"fn1-11727\"><p>I am not certain I have the exact right numbers for older nodes, but I have confirmed that the numbers are in the right ballpark&nbsp;<a href=\"#rf1-11727\" class=\"backlink\" title=\"Return to footnote 1.\">↩</a></p></li><li id=\"fn2-11727\"><p>TSMC first used EUV with latter iterations of its 7nm process, but that was primarily to move down the learning curve; EUV was not strictly necessary, and the original 7nm process used immersion DUV lithography exclusively&nbsp;<a href=\"#rf2-11727\" class=\"backlink\" title=\"Return to footnote 2.\">↩</a></p></li></ol>\n\t\t\t",
      "contentText": "\n\t\t\n\n\nThe complexity for minimum component costs has increased at a rate of roughly a factor of two per year (see graph on next page). Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for at least 10 years.\n— Gordon Moore, Cramming More Components Onto Integrated Circuits\n\n\n\n\n\nMoore’s law is dead.\n– Jensen Huang\n\n\n\nOn Tuesday the Biden administration tightened export controls for advanced AI chips being sold to China; the primary target was Nvidia’s H800 and A800 chips, which were specifically designed to skirt controls put in place last year. The primary difference between the H800/A800 and H100/A100 is the bandwidth of their interconnects: the A100 had 600 Gb/s interconnects (the H100 has 900GB/s), which just so happened to be the limit proscribed by last year’s export controls; the A800 and H800 were limited to 400 Gb/s interconnects.\nThe reason why interconnect speed matters is tied up with Nvidia CEO Jensen Huang’s thesis that Moore’s Law is dead. Moore’s Law, as originally stated in 1965, states that the number of transistors in an integrated circuit would double every year. Moore revised his prediction 10 years later to be a doubling every two years, which held until the last decade or so, when it has slowed to a doubling about every three years.\nIn practice, though, Moore’s Law has become something more akin to a fundamental precept underlying the tech industry: computing power will both increase and get cheaper over time. This precept — which I will call Moore’s Precept, for clarity — is connected to Moore’s technical prediction: smaller transistors can switch faster, and use less energy in the switching, even as more of them fit on a single wafer; this means that you can either get more chips per wafer or larger chips, either decreasing price or increasing power for the same price. In practice we got both.\nWhat is critical is that the rest of the tech industry didn’t need to understand the technical or economic details of Moore’s Law: for 60 years it has been safe to simply assume that computers would get faster, which meant the optimal approach was always to build for the cutting edge or just beyond, and trust that processor speed would catch up to your use case. From an analyst perspective, it is Moore’s Precept that enables me to write speculative articles like AI, Hardware, and Virtual Reality: it is enough to see that a use case is possible, if not yet optimal; Moore’s Precept will provide the optimization.\nThe End of Moore’s Precept?\nThis distinction between Moore’s Law and Moore’s Precept is the key to understanding Nvidia CEO Jensen Huang’s repeated declarations that Moore’s Law is dead. From a technical perspective, it has certainly slowed, but density continues to increase; here is TSMC’s transistor density by node size, using the first (i.e. worse) iteration of each node size:1\n\n\n\nTSMC\nTransistor Density (MTr/mm)\nYear Introduced\n\n\n\n\n90 nm\n3.4\n2004\n\n\n65 nm\n5.6\n2006\n\n\n40 nm\n9.8\n2008\n\n\n28 nm\n16.6\n2011\n\n\n20 nm\n20.9\n2014\n\n\n16 nm\n28.9\n2015\n\n\n10 nm\n52.5\n2017\n\n\n7 nm\n91.2\n2019\n\n\n5 nm\n138.2\n2020\n\n\n\nRemember, though, that cost matters; here is the same table with TSMC’s introductory price/wafer, and what that translates to in terms of price/billion transistors:\n\n\n\nTSMC\nMTr/mm\nYear Introduced\nPrice/Wafer\nPrice/BTr\n\n\n\n\n90 nm\n3.4\n2004\n$1,650\n$6.87\n\n\n65 nm\n5.6\n2006\n$1,937\n$4.89\n\n\n40 nm\n9.8\n2008\n$2,274\n$3.28\n\n\n28 nm\n16.6\n2011\n$2,891\n$2.46\n\n\n20 nm\n20.9\n2014\n$3,677\n$2.49\n\n\n16 nm\n28.9\n2015\n$3,984\n$1.95\n\n\n10 nm\n52.5\n2017\n$5,992\n$1.61\n\n\n7 nm\n91.2\n2019\n$9,346\n$1.45\n\n\n5 nm\n138.2\n2020\n$16,988\n$1.74\n\n\n\nNotice that number on the bottom right: with TSMC’s 5 nm process the price per transistor increased — and it increased a lot (20%). The reason was obvious: 5 nm was the first process that required ASML’s extreme ultraviolet (EUV) lithography, and EUV machines were hugely expensive — around $150 million each.2 In other words, it appeared that while the technical definition of Moore’s Law would continue, the precept that chips would always get both faster and cheaper would not.\nGPUs and Embarrassing Parallelism\nHuang’s argument, to be clear, does not simply rest on the cost of 5 nm chips; remember Moore’s Precept is about speed as well as cost, and the truth is that a lot of those density gains have primarily gone towards power efficiency as energy became a constraint in everything from mobile to PCs to data centers. Huang’s thesis for several years now is that Nvidia has the solution to making computing faster: use GPUs.\nGPUs are much less complex than CPUs; that means they can execute instructions much more quickly, but those instructions have to be much simpler. At the same time, you can run a lot of them at the same time to achieve outsized results. Graphics is, unsurprisingly, the most obvious example: every “shader” — the primary processing component of a GPU — calculates what will be displayed on a single portion of the screen; the size of the portion is a function of how many shaders you have available. If you have 1,024 shaders, each shader draws 1/1,024 of the screen. Ergo, if you have 2,048 shaders, you can draw the screen twice as fast. Graphics performance is “embarrassingly parallel”, which is to say it scales with the number of processors you apply to the problem.\nThis “embarrassing parallelism” is the key to GPUs outsized performance relative to CPUs, but the challenge is that not all software problems are easily parallel-izable; Nvida’s CUDA ecosystem is predicated on providing the tools to build software applications that can leverage GPU parallelism, and is one of the major moats undergirding Nvidia’s dominance, but most software applications still need the complexity of CPUs to run.\nAI, though, is not most software. It turns out that AI, both in terms of training models and in leveraging them (i.e. inference) is an embarrassingly parallel application. Moreover, the optimum amount of scalability goes far beyond a computer monitor displaying graphics; this is why Nvidia AI chips feature the high-speed interconnects referenced by the chip ban: AI applications run across multiple AI chips at the same time, but the key to making sure those GPUs are busy is feeding them with data, and that requires those high speed interconnects.\nThat noted, I’m skeptical about the wholesale shift of traditional data center applications to GPUs; from Nvidia On the Mountaintop:\n\n  Humans — and companies — are lazy, and not only are CPU-based applications easier to develop, they are also mostly already built. I have a hard time seeing what companies are going to go through the time and effort to port things that already run on CPUs to GPUs; at the end of the day, the applications that run in a cloud are determined by customers who provide the demand for cloud resources, not cloud providers looking to optimize FLOP/rack.\n\nThere’s another reason to think that traditional CPUs still have some life in them as well: it turns out that Moore’s Precept may be back on track.\nEUV and Moore’s Precept\nThe table I posted above only ran through 5 nm; the iPhone 15 Pro, though, has an N3 chip, and check out the price/transistor:\n\n\n\nTSMC\nMTr/mm\nYear Introduced\nPrice/Wafer\nPrice/BTr\n\n\n\n\n90 nm\n3.4\n2004\n$1,650\n$6.87\n\n\n65 nm\n5.6\n2006\n$1,937\n$4.89\n\n\n40 nm\n9.8\n2008\n$2,274\n$3.28\n\n\n28 nm\n16.6\n2011\n$2,891\n$2.46\n\n\n20 nm\n20.9\n2014\n$3,677\n$2.49\n\n\n16 nm\n28.9\n2015\n$3,984\n$1.95\n\n\n10 nm\n52.5\n2017\n$5,992\n$1.61\n\n\n7 nm\n91.2\n2019\n$9,346\n$1.45\n\n\n5 nm\n138.2\n2020\n$16,988\n$1.74\n\n\n3 nm (N3B)\n197.0\n2023\n$20,000\n$1.44\n\n\n3 nm (N3E)\n215.6\n2023\n$20,000\n$1.31\n\n\n\nWhile I only included the first version of each node previously, the N3B process, which is used for the iPhone’s A17 Pro chip, is a dead-end; TSMC changed its approach with the N3E, which will be the basis of the N3 family going forward. It also makes the N3 leap even more impressive in terms of price/transistor: while N3B undid the 5 nm backslide, N3E is a marked improvement over 7 nm.\nMoreover, the gains are actually what you would expect: yes, those EUV machines cost a lot, but the price decreases embedded in Moore’s Precept are not a function of equipment getting cheaper — notice that the price/wafer has been increasing continuously. Rather, ever declining prices/transistor are a function of Moore’s Law, which is to say that new equipment, like EUV, lets us “Cram[] More Components Onto Integrated Circuits”.\nWhat happened at 5 nm was similar to what happened at 20 nm, the last time the price/transistor increased: that was the node where TSMC started to use double-patterning, which means they had to do every lithography step twice; that both doubled the utilization of lithography equipment per wafer and also decreased yield. For that node, at least, the gains from making smaller transistors were outweighed by the costs. A year later, though, and TSMC launched the 16 nm node that re-united Moore’s Law with Moore’s Precept. That is exactly what seems to have happened with 3 nm — the gains of EUV are now significantly outweighing the costs — and early rumors about 2 nm density and price points suggests the gains should continue for another node.\nChip Ban Angst\nAll of this is interesting in its own right, but it’s particularly pertinent in light of the recent angst in Washington DC over Huawei’s recent smartphone with a 7 nm chip, seemingly in defiance of those export controls. I already explained why that angst was misguided in this September Update. To summarize my argument:\n\nTSMC had already shown that 7 nm chips could be made using deep ultraviolet (DUV)-based immersion lithography, and China had plenty of DUV lithography machines, given that DUV has been the standard for multiple generations of chips.\nChina’s Semiconductor Manufacturing International Corp. (SMIC) had already made a 7 nm chip in 2022; sure it was simpler than the one launched in that Huawei phone, but that is the exact sort of progression you should expect from a competent foundry.\nSMIC is almost certainly not producing that 7nm chip economically; Intel, for example, could make a 7nm chip using DUV, they just couldn’t do it economically, which is why they ultimately switched to EUV.\n\nIn short, the problem with the chip ban was drawing the line at 10 nm: that line was arbitrary given that the equipment needed to make 10 nm chips had already been shown to be capable of producing 7 nm chips; that SMIC managed to do just that isn’t a surprise, and, crucially, is not evidence that the chip ban was a failure.\nThe line that actually matters is 5 nm, which is another way to say that the export control that will actually limit China’s long-term development is EUV. Fortunately the Trump administration had already persuaded the Netherlands to not allow the export of EUV machines, which the Biden administration further locked down with its chip ban and further coordination with the Netherlands. The reality is that a lot of chip-making equipment is “multi-nodal”; much of the machinery can be used at multiple nodes, but you must have EUV machines to realize Moore’s Precept, because it is the key piece of technology driving Moore’s Law.\nBy the same token, the A800/H800 loophole was a real one: the H800 is made on TSMC’s third-generation 5 nm process (confusingly called N4), which is to say it is made with EUV; the interconnect limits were meaningful, and would make AI development slower and more costly (because those GPUs would be starved of data more of the time), but it didn’t halt it. This matters because AI is the military application the U.S. should be the most concerned with: a lot of military applications run perfectly fine on existing chips (or even, in the case of guided weaponry, chips that were made decades ago); wars of the future, though, will almost certainly be undergirded by AI, a field that is only just now getting started.\nThis leads to a further point: the payoff from this chip ban will not come immediately. The only way the entire idea makes sense is if Moore’s Law continues to exist, because that means the chips that will be available in five or ten years will be that much faster and cheaper than the ones that exist today, increasing the gap. And, at the same time, the idea also depends on taking Huang’s argument seriously, because AI needs not just power but scale. Fortunately movement on both fronts is headed in the right direction.\nThere remain good arguments against the entire concept of the chip ban, including the obvious fact that China is heavily incentivized to built up replacements from scratch (and could have leverage on the U.S. on the trailing edge): perhaps in 20 years the U.S. will not only have lost its most potent point of leverage but will also see its most cutting edge companies undercut by Chinese competition. That die, though, has long since been cast; the results that matter are not a smartphone in 2023, but the capabilities of 2030 and beyond.\nI am not certain I have the exact right numbers for older nodes, but I have confirmed that the numbers are in the right ballpark ↩TSMC first used EUV with latter iterations of its 7nm process, but that was primarily to move down the learning curve; EUV was not strictly necessary, and the original 7nm process used immersion DUV lithography exclusively ↩\n\t\t\t",
      "subsections": [
        {
          "subtitle": "The End of Moore’s Precept?",
          "contentHtml": "<p>This distinction between Moore’s Law and Moore’s Precept is the key to understanding Nvidia CEO Jensen Huang’s repeated declarations that Moore’s Law is dead. From a technical perspective, it has certainly slowed, but density continues to increase; here is TSMC’s transistor density by node size, using the first (i.e. worse) iteration of each node size:<sup id=\"rf1-11727\"><a href=\"#fn1-11727\" title=\"I am not certain I have the exact right numbers for older nodes, but I have confirmed that the numbers are in the right ballpark\" rel=\"footnote\">1</a></sup></p><table>\n<thead>\n<tr>\n<th align=\"center\">TSMC</th>\n<th align=\"center\">Transistor Density (MTr/mm)</th>\n<th align=\"center\">Year Introduced</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">90 nm</td>\n<td align=\"center\">3.4</td>\n<td align=\"center\">2004</td>\n</tr>\n<tr>\n<td align=\"center\">65 nm</td>\n<td align=\"center\">5.6</td>\n<td align=\"center\">2006</td>\n</tr>\n<tr>\n<td align=\"center\">40 nm</td>\n<td align=\"center\">9.8</td>\n<td align=\"center\">2008</td>\n</tr>\n<tr>\n<td align=\"center\">28 nm</td>\n<td align=\"center\">16.6</td>\n<td align=\"center\">2011</td>\n</tr>\n<tr>\n<td align=\"center\">20 nm</td>\n<td align=\"center\">20.9</td>\n<td align=\"center\">2014</td>\n</tr>\n<tr>\n<td align=\"center\">16 nm</td>\n<td align=\"center\">28.9</td>\n<td align=\"center\">2015</td>\n</tr>\n<tr>\n<td align=\"center\">10 nm</td>\n<td align=\"center\">52.5</td>\n<td align=\"center\">2017</td>\n</tr>\n<tr>\n<td align=\"center\">7 nm</td>\n<td align=\"center\">91.2</td>\n<td align=\"center\">2019</td>\n</tr>\n<tr>\n<td align=\"center\">5 nm</td>\n<td align=\"center\">138.2</td>\n<td align=\"center\">2020</td>\n</tr>\n</tbody>\n</table><p>Remember, though, that cost matters; here is the same table with TSMC’s introductory price/wafer, and what that translates to in terms of price/billion transistors:</p><table>\n<thead>\n<tr>\n<th align=\"center\">TSMC</th>\n<th align=\"center\">MTr/mm</th>\n<th align=\"center\">Year Introduced</th>\n<th align=\"center\">Price/Wafer</th>\n<th align=\"center\">Price/BTr</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">90 nm</td>\n<td align=\"center\">3.4</td>\n<td align=\"center\">2004</td>\n<td align=\"center\">$1,650</td>\n<td align=\"center\">$6.87</td>\n</tr>\n<tr>\n<td align=\"center\">65 nm</td>\n<td align=\"center\">5.6</td>\n<td align=\"center\">2006</td>\n<td align=\"center\">$1,937</td>\n<td align=\"center\">$4.89</td>\n</tr>\n<tr>\n<td align=\"center\">40 nm</td>\n<td align=\"center\">9.8</td>\n<td align=\"center\">2008</td>\n<td align=\"center\">$2,274</td>\n<td align=\"center\">$3.28</td>\n</tr>\n<tr>\n<td align=\"center\">28 nm</td>\n<td align=\"center\">16.6</td>\n<td align=\"center\">2011</td>\n<td align=\"center\">$2,891</td>\n<td align=\"center\">$2.46</td>\n</tr>\n<tr>\n<td align=\"center\">20 nm</td>\n<td align=\"center\">20.9</td>\n<td align=\"center\">2014</td>\n<td align=\"center\">$3,677</td>\n<td align=\"center\">$2.49</td>\n</tr>\n<tr>\n<td align=\"center\">16 nm</td>\n<td align=\"center\">28.9</td>\n<td align=\"center\">2015</td>\n<td align=\"center\">$3,984</td>\n<td align=\"center\">$1.95</td>\n</tr>\n<tr>\n<td align=\"center\">10 nm</td>\n<td align=\"center\">52.5</td>\n<td align=\"center\">2017</td>\n<td align=\"center\">$5,992</td>\n<td align=\"center\">$1.61</td>\n</tr>\n<tr>\n<td align=\"center\">7 nm</td>\n<td align=\"center\">91.2</td>\n<td align=\"center\">2019</td>\n<td align=\"center\">$9,346</td>\n<td align=\"center\">$1.45</td>\n</tr>\n<tr>\n<td align=\"center\">5 nm</td>\n<td align=\"center\">138.2</td>\n<td align=\"center\">2020</td>\n<td align=\"center\">$16,988</td>\n<td align=\"center\">$1.74</td>\n</tr>\n</tbody>\n</table><p>Notice that number on the bottom right: with TSMC’s 5 nm process the price per transistor increased — and it increased <em>a lot</em> (20%). The reason was obvious: 5 nm was the first process that required ASML’s extreme ultraviolet (EUV) lithography, and EUV machines were hugely expensive — around $150 million each.<sup id=\"rf2-11727\"><a href=\"#fn2-11727\" title=\"TSMC first used EUV with latter iterations of its 7nm process, but that was primarily to move down the learning curve; EUV was not strictly necessary, and the original 7nm process used immersion DUV lithography exclusively\" rel=\"footnote\">2</a></sup> In other words, it appeared that while the technical definition of Moore’s Law would continue, the precept that chips would always get both faster and cheaper would not.</p>",
          "contentText": "This distinction between Moore’s Law and Moore’s Precept is the key to understanding Nvidia CEO Jensen Huang’s repeated declarations that Moore’s Law is dead. From a technical perspective, it has certainly slowed, but density continues to increase; here is TSMC’s transistor density by node size, using the first (i.e. worse) iteration of each node size:1\n\n\nTSMC\nTransistor Density (MTr/mm)\nYear Introduced\n\n\n\n\n90 nm\n3.4\n2004\n\n\n65 nm\n5.6\n2006\n\n\n40 nm\n9.8\n2008\n\n\n28 nm\n16.6\n2011\n\n\n20 nm\n20.9\n2014\n\n\n16 nm\n28.9\n2015\n\n\n10 nm\n52.5\n2017\n\n\n7 nm\n91.2\n2019\n\n\n5 nm\n138.2\n2020\n\n\nRemember, though, that cost matters; here is the same table with TSMC’s introductory price/wafer, and what that translates to in terms of price/billion transistors:\n\n\nTSMC\nMTr/mm\nYear Introduced\nPrice/Wafer\nPrice/BTr\n\n\n\n\n90 nm\n3.4\n2004\n$1,650\n$6.87\n\n\n65 nm\n5.6\n2006\n$1,937\n$4.89\n\n\n40 nm\n9.8\n2008\n$2,274\n$3.28\n\n\n28 nm\n16.6\n2011\n$2,891\n$2.46\n\n\n20 nm\n20.9\n2014\n$3,677\n$2.49\n\n\n16 nm\n28.9\n2015\n$3,984\n$1.95\n\n\n10 nm\n52.5\n2017\n$5,992\n$1.61\n\n\n7 nm\n91.2\n2019\n$9,346\n$1.45\n\n\n5 nm\n138.2\n2020\n$16,988\n$1.74\n\n\nNotice that number on the bottom right: with TSMC’s 5 nm process the price per transistor increased — and it increased a lot (20%). The reason was obvious: 5 nm was the first process that required ASML’s extreme ultraviolet (EUV) lithography, and EUV machines were hugely expensive — around $150 million each.2 In other words, it appeared that while the technical definition of Moore’s Law would continue, the precept that chips would always get both faster and cheaper would not."
        },
        {
          "subtitle": "GPUs and Embarrassing Parallelism",
          "contentHtml": "<p>Huang’s argument, to be clear, does not simply rest on the cost of 5 nm chips; remember Moore’s Precept is about speed as well as cost, and the truth is that a lot of those density gains have primarily gone towards power efficiency as energy became a constraint in everything from mobile to PCs to data centers. Huang’s thesis for several years now is that Nvidia has the solution to making computing <em>faster</em>: use GPUs.</p><p>GPUs are much less complex than CPUs; that means they can execute instructions much more quickly, but those instructions have to be much simpler. At the same time, you can run a lot of them at the same time to achieve outsized results. Graphics is, unsurprisingly, the most obvious example: every “shader” — the primary processing component of a GPU — calculates what will be displayed on a single portion of the screen; the size of the portion is a function of how many shaders you have available. If you have 1,024 shaders, each shader draws 1/1,024 of the screen. Ergo, if you have 2,048 shaders, you can draw the screen twice as fast. Graphics performance is “embarrassingly parallel”, which is to say it scales with the number of processors you apply to the problem.</p><p>This “embarrassing parallelism” is the key to GPUs outsized performance relative to CPUs, but the challenge is that not all software problems are easily parallel-izable; Nvida’s CUDA ecosystem is predicated on providing the tools to build software applications that can leverage GPU parallelism, and is one of the major moats undergirding Nvidia’s dominance, but most software applications still need the complexity of CPUs to run.</p><p>AI, though, is not most software. It turns out that AI, both in terms of training models and in leveraging them (i.e. inference) is an embarrassingly parallel application. Moreover, the optimum amount of scalability goes far beyond a computer monitor displaying graphics; this is why Nvidia AI chips feature the high-speed interconnects referenced by the chip ban: AI applications run across multiple AI chips at the same time, but the key to making sure those GPUs are busy is feeding them with data, and that requires those high speed interconnects.</p><p>That noted, I’m skeptical about the wholesale shift of traditional data center applications to GPUs; from <a href=\"https://stratechery.com/2023/nvidia-on-the-mountaintop/\">Nvidia On the Mountaintop</a>:</p><blockquote><p>\n  Humans — and companies — are lazy, and not only are CPU-based applications easier to develop, they are also mostly already built. I have a hard time seeing what companies are going to go through the time and effort to port things that already run on CPUs to GPUs; at the end of the day, the applications that run in a cloud are determined by customers who provide the demand for cloud resources, not cloud providers looking to optimize FLOP/rack.\n</p></blockquote><p>There’s another reason to think that traditional CPUs still have some life in them as well: it turns out that Moore’s Precept may be back on track.</p>",
          "contentText": "Huang’s argument, to be clear, does not simply rest on the cost of 5 nm chips; remember Moore’s Precept is about speed as well as cost, and the truth is that a lot of those density gains have primarily gone towards power efficiency as energy became a constraint in everything from mobile to PCs to data centers. Huang’s thesis for several years now is that Nvidia has the solution to making computing faster: use GPUs.GPUs are much less complex than CPUs; that means they can execute instructions much more quickly, but those instructions have to be much simpler. At the same time, you can run a lot of them at the same time to achieve outsized results. Graphics is, unsurprisingly, the most obvious example: every “shader” — the primary processing component of a GPU — calculates what will be displayed on a single portion of the screen; the size of the portion is a function of how many shaders you have available. If you have 1,024 shaders, each shader draws 1/1,024 of the screen. Ergo, if you have 2,048 shaders, you can draw the screen twice as fast. Graphics performance is “embarrassingly parallel”, which is to say it scales with the number of processors you apply to the problem.This “embarrassing parallelism” is the key to GPUs outsized performance relative to CPUs, but the challenge is that not all software problems are easily parallel-izable; Nvida’s CUDA ecosystem is predicated on providing the tools to build software applications that can leverage GPU parallelism, and is one of the major moats undergirding Nvidia’s dominance, but most software applications still need the complexity of CPUs to run.AI, though, is not most software. It turns out that AI, both in terms of training models and in leveraging them (i.e. inference) is an embarrassingly parallel application. Moreover, the optimum amount of scalability goes far beyond a computer monitor displaying graphics; this is why Nvidia AI chips feature the high-speed interconnects referenced by the chip ban: AI applications run across multiple AI chips at the same time, but the key to making sure those GPUs are busy is feeding them with data, and that requires those high speed interconnects.That noted, I’m skeptical about the wholesale shift of traditional data center applications to GPUs; from Nvidia On the Mountaintop:\n  Humans — and companies — are lazy, and not only are CPU-based applications easier to develop, they are also mostly already built. I have a hard time seeing what companies are going to go through the time and effort to port things that already run on CPUs to GPUs; at the end of the day, the applications that run in a cloud are determined by customers who provide the demand for cloud resources, not cloud providers looking to optimize FLOP/rack.\nThere’s another reason to think that traditional CPUs still have some life in them as well: it turns out that Moore’s Precept may be back on track."
        },
        {
          "subtitle": "EUV and Moore’s Precept",
          "contentHtml": "<p>The table I posted above only ran through 5 nm; the iPhone 15 Pro, though, has an N3 chip, and check out the price/transistor:</p><table>\n<thead>\n<tr>\n<th align=\"center\">TSMC</th>\n<th align=\"center\">MTr/mm</th>\n<th align=\"center\">Year Introduced</th>\n<th align=\"center\">Price/Wafer</th>\n<th align=\"center\">Price/BTr</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">90 nm</td>\n<td align=\"center\">3.4</td>\n<td align=\"center\">2004</td>\n<td align=\"center\">$1,650</td>\n<td align=\"center\">$6.87</td>\n</tr>\n<tr>\n<td align=\"center\">65 nm</td>\n<td align=\"center\">5.6</td>\n<td align=\"center\">2006</td>\n<td align=\"center\">$1,937</td>\n<td align=\"center\">$4.89</td>\n</tr>\n<tr>\n<td align=\"center\">40 nm</td>\n<td align=\"center\">9.8</td>\n<td align=\"center\">2008</td>\n<td align=\"center\">$2,274</td>\n<td align=\"center\">$3.28</td>\n</tr>\n<tr>\n<td align=\"center\">28 nm</td>\n<td align=\"center\">16.6</td>\n<td align=\"center\">2011</td>\n<td align=\"center\">$2,891</td>\n<td align=\"center\">$2.46</td>\n</tr>\n<tr>\n<td align=\"center\">20 nm</td>\n<td align=\"center\">20.9</td>\n<td align=\"center\">2014</td>\n<td align=\"center\">$3,677</td>\n<td align=\"center\">$2.49</td>\n</tr>\n<tr>\n<td align=\"center\">16 nm</td>\n<td align=\"center\">28.9</td>\n<td align=\"center\">2015</td>\n<td align=\"center\">$3,984</td>\n<td align=\"center\">$1.95</td>\n</tr>\n<tr>\n<td align=\"center\">10 nm</td>\n<td align=\"center\">52.5</td>\n<td align=\"center\">2017</td>\n<td align=\"center\">$5,992</td>\n<td align=\"center\">$1.61</td>\n</tr>\n<tr>\n<td align=\"center\">7 nm</td>\n<td align=\"center\">91.2</td>\n<td align=\"center\">2019</td>\n<td align=\"center\">$9,346</td>\n<td align=\"center\">$1.45</td>\n</tr>\n<tr>\n<td align=\"center\">5 nm</td>\n<td align=\"center\">138.2</td>\n<td align=\"center\">2020</td>\n<td align=\"center\">$16,988</td>\n<td align=\"center\">$1.74</td>\n</tr>\n<tr>\n<td align=\"center\">3 nm (N3B)</td>\n<td align=\"center\">197.0</td>\n<td align=\"center\">2023</td>\n<td align=\"center\">$20,000</td>\n<td align=\"center\">$1.44</td>\n</tr>\n<tr>\n<td align=\"center\">3 nm (N3E)</td>\n<td align=\"center\">215.6</td>\n<td align=\"center\">2023</td>\n<td align=\"center\">$20,000</td>\n<td align=\"center\">$1.31</td>\n</tr>\n</tbody>\n</table><p>While I only included the first version of each node previously, the N3B process, which is used for the iPhone’s A17 Pro chip, <a href=\"https://fuse.wikichip.org/news/7048/n3e-replaces-n3-comes-in-many-flavors/\">is a dead-end</a>; TSMC changed its approach with the N3E, which will be the basis of the N3 family going forward. It also makes the N3 leap even more impressive in terms of price/transistor: while N3B undid the 5 nm backslide, N3E is a marked improvement over 7 nm.</p><p>Moreover, the gains are actually what you would expect: yes, those EUV machines cost a lot, but the price decreases embedded in Moore’s Precept are not a function of equipment getting cheaper — notice that the price/wafer has been increasing continuously. Rather, ever declining prices/transistor are a function of Moore’s Law, which is to say that new equipment, like EUV, lets us “Cram[] More Components Onto Integrated Circuits”.</p><p>What happened at 5 nm was similar to what happened at 20 nm, the last time the price/transistor increased: that was the node where TSMC started to use double-patterning, which means they had to do every lithography step twice; that both doubled the utilization of lithography equipment per wafer and also decreased yield. For that node, at least, the gains from making smaller transistors were outweighed by the costs. A year later, though, and TSMC launched the 16 nm node that re-united Moore’s Law with Moore’s Precept. That is exactly what seems to have happened with 3 nm —&nbsp;the gains of EUV are now significantly outweighing the costs — and early rumors about 2 nm density and price points suggests the gains should continue for another node.</p>",
          "contentText": "The table I posted above only ran through 5 nm; the iPhone 15 Pro, though, has an N3 chip, and check out the price/transistor:\n\n\nTSMC\nMTr/mm\nYear Introduced\nPrice/Wafer\nPrice/BTr\n\n\n\n\n90 nm\n3.4\n2004\n$1,650\n$6.87\n\n\n65 nm\n5.6\n2006\n$1,937\n$4.89\n\n\n40 nm\n9.8\n2008\n$2,274\n$3.28\n\n\n28 nm\n16.6\n2011\n$2,891\n$2.46\n\n\n20 nm\n20.9\n2014\n$3,677\n$2.49\n\n\n16 nm\n28.9\n2015\n$3,984\n$1.95\n\n\n10 nm\n52.5\n2017\n$5,992\n$1.61\n\n\n7 nm\n91.2\n2019\n$9,346\n$1.45\n\n\n5 nm\n138.2\n2020\n$16,988\n$1.74\n\n\n3 nm (N3B)\n197.0\n2023\n$20,000\n$1.44\n\n\n3 nm (N3E)\n215.6\n2023\n$20,000\n$1.31\n\n\nWhile I only included the first version of each node previously, the N3B process, which is used for the iPhone’s A17 Pro chip, is a dead-end; TSMC changed its approach with the N3E, which will be the basis of the N3 family going forward. It also makes the N3 leap even more impressive in terms of price/transistor: while N3B undid the 5 nm backslide, N3E is a marked improvement over 7 nm.Moreover, the gains are actually what you would expect: yes, those EUV machines cost a lot, but the price decreases embedded in Moore’s Precept are not a function of equipment getting cheaper — notice that the price/wafer has been increasing continuously. Rather, ever declining prices/transistor are a function of Moore’s Law, which is to say that new equipment, like EUV, lets us “Cram[] More Components Onto Integrated Circuits”.What happened at 5 nm was similar to what happened at 20 nm, the last time the price/transistor increased: that was the node where TSMC started to use double-patterning, which means they had to do every lithography step twice; that both doubled the utilization of lithography equipment per wafer and also decreased yield. For that node, at least, the gains from making smaller transistors were outweighed by the costs. A year later, though, and TSMC launched the 16 nm node that re-united Moore’s Law with Moore’s Precept. That is exactly what seems to have happened with 3 nm — the gains of EUV are now significantly outweighing the costs — and early rumors about 2 nm density and price points suggests the gains should continue for another node."
        },
        {
          "subtitle": "Chip Ban Angst",
          "contentHtml": "<p>All of this is interesting in its own right, but it’s particularly pertinent in light of the recent angst in Washington DC over Huawei’s recent smartphone with a 7 nm chip, seemingly in defiance of those export controls. I already explained why that angst was misguided in <a href=\"https://stratechery.com/2023/the-huawei-mate-60-pro-7nm-background-implications-and-reactions/\">this September Update</a>. To summarize my argument:</p><ul>\n<li>TSMC had already shown that 7 nm chips could be made using deep ultraviolet (DUV)-based immersion lithography, and China had plenty of DUV lithography machines, given that DUV has been the standard for multiple generations of chips.</li>\n<li>China’s Semiconductor Manufacturing International Corp. (SMIC) had <a href=\"https://www.semianalysis.com/p/chinas-smic-is-shipping-7nm-foundry\">already made a 7 nm chip in 2022</a>; sure it was simpler than the one launched in that Huawei phone, but that is the exact sort of progression you should expect from a competent foundry.</li>\n<li>SMIC is almost certainly not producing that 7nm chip economically; Intel, for example, could make a 7nm chip using DUV, they just couldn’t do it economically, which is why they ultimately switched to EUV.</li>\n</ul><p>In short, the problem with the chip ban was drawing the line at 10 nm: that line was arbitrary given that the equipment needed to make 10 nm chips had already been shown to be capable of producing 7 nm chips; that SMIC managed to do just that isn’t a surprise, and, crucially, is not evidence that the chip ban was a failure.</p><p>The line that actually matters is 5 nm, which is another way to say that the export control that will actually limit China’s long-term development is EUV. Fortunately <a href=\"https://www.reuters.com/article/us-asml-holding-usa-china-insight-idUSKBN1Z50HN\">the Trump administration had already persuaded the Netherlands</a> to not allow the export of EUV machines, which the Biden administration further locked down with its chip ban and further coordination with the Netherlands. The reality is that a lot of chip-making equipment is “multi-nodal”; much of the machinery can be used at multiple nodes, but you must have EUV machines to realize Moore’s Precept, because it is the key piece of technology driving Moore’s Law.</p><p>By the same token, the A800/H800 loophole was a real one: the H800 is made on TSMC’s third-generation 5 nm process (confusingly called N4), which is to say it is made with EUV; the interconnect limits were meaningful, and would make AI development slower and more costly (because those GPUs would be starved of data more of the time), but it didn’t halt it. This matters because AI is the military application the U.S. should be the most concerned with: a lot of military applications run perfectly fine on existing chips (or even, in the case of guided weaponry, chips that were made decades ago); wars of the future, though, will almost certainly be undergirded by AI, a field that is only just now getting started.</p><p>This leads to a further point: the payoff from this chip ban will not come immediately. The only way the entire idea makes sense is if Moore’s Law continues to exist, because that means the chips that will be available in five or ten years will be that much faster and cheaper than the ones that exist today, increasing the gap. And, at the same time, the idea also depends on taking Huang’s argument seriously, because AI needs not just power but scale. Fortunately movement on both fronts is headed in the right direction.</p><p>There remain good arguments against the entire concept of the chip ban, including the obvious fact that China is heavily incentivized to built up replacements from scratch (and could <a href=\"https://stratechery.com/2022/chips-and-china/\">have leverage on the U.S. on the trailing edge</a>): perhaps in 20 years the U.S. will not only have lost its most potent point of leverage but will also see its most cutting edge companies undercut by Chinese competition. That die, though, has long since been cast; the results that matter are not a smartphone in 2023, but the capabilities of 2030 and beyond.</p><hr class=\"footnotes\"><ol class=\"footnotes\" style=\"list-style-type:decimal\"><li id=\"fn1-11727\"><p>I am not certain I have the exact right numbers for older nodes, but I have confirmed that the numbers are in the right ballpark&nbsp;<a href=\"#rf1-11727\" class=\"backlink\" title=\"Return to footnote 1.\">↩</a></p></li><li id=\"fn2-11727\"><p>TSMC first used EUV with latter iterations of its 7nm process, but that was primarily to move down the learning curve; EUV was not strictly necessary, and the original 7nm process used immersion DUV lithography exclusively&nbsp;<a href=\"#rf2-11727\" class=\"backlink\" title=\"Return to footnote 2.\">↩</a></p></li></ol>",
          "contentText": "All of this is interesting in its own right, but it’s particularly pertinent in light of the recent angst in Washington DC over Huawei’s recent smartphone with a 7 nm chip, seemingly in defiance of those export controls. I already explained why that angst was misguided in this September Update. To summarize my argument:\nTSMC had already shown that 7 nm chips could be made using deep ultraviolet (DUV)-based immersion lithography, and China had plenty of DUV lithography machines, given that DUV has been the standard for multiple generations of chips.\nChina’s Semiconductor Manufacturing International Corp. (SMIC) had already made a 7 nm chip in 2022; sure it was simpler than the one launched in that Huawei phone, but that is the exact sort of progression you should expect from a competent foundry.\nSMIC is almost certainly not producing that 7nm chip economically; Intel, for example, could make a 7nm chip using DUV, they just couldn’t do it economically, which is why they ultimately switched to EUV.\nIn short, the problem with the chip ban was drawing the line at 10 nm: that line was arbitrary given that the equipment needed to make 10 nm chips had already been shown to be capable of producing 7 nm chips; that SMIC managed to do just that isn’t a surprise, and, crucially, is not evidence that the chip ban was a failure.The line that actually matters is 5 nm, which is another way to say that the export control that will actually limit China’s long-term development is EUV. Fortunately the Trump administration had already persuaded the Netherlands to not allow the export of EUV machines, which the Biden administration further locked down with its chip ban and further coordination with the Netherlands. The reality is that a lot of chip-making equipment is “multi-nodal”; much of the machinery can be used at multiple nodes, but you must have EUV machines to realize Moore’s Precept, because it is the key piece of technology driving Moore’s Law.By the same token, the A800/H800 loophole was a real one: the H800 is made on TSMC’s third-generation 5 nm process (confusingly called N4), which is to say it is made with EUV; the interconnect limits were meaningful, and would make AI development slower and more costly (because those GPUs would be starved of data more of the time), but it didn’t halt it. This matters because AI is the military application the U.S. should be the most concerned with: a lot of military applications run perfectly fine on existing chips (or even, in the case of guided weaponry, chips that were made decades ago); wars of the future, though, will almost certainly be undergirded by AI, a field that is only just now getting started.This leads to a further point: the payoff from this chip ban will not come immediately. The only way the entire idea makes sense is if Moore’s Law continues to exist, because that means the chips that will be available in five or ten years will be that much faster and cheaper than the ones that exist today, increasing the gap. And, at the same time, the idea also depends on taking Huang’s argument seriously, because AI needs not just power but scale. Fortunately movement on both fronts is headed in the right direction.There remain good arguments against the entire concept of the chip ban, including the obvious fact that China is heavily incentivized to built up replacements from scratch (and could have leverage on the U.S. on the trailing edge): perhaps in 20 years the U.S. will not only have lost its most potent point of leverage but will also see its most cutting edge companies undercut by Chinese competition. That die, though, has long since been cast; the results that matter are not a smartphone in 2023, but the capabilities of 2030 and beyond.I am not certain I have the exact right numbers for older nodes, but I have confirmed that the numbers are in the right ballpark ↩TSMC first used EUV with latter iterations of its 7nm process, but that was primarily to move down the learning curve; EUV was not strictly necessary, and the original 7nm process used immersion DUV lithography exclusively ↩"
        }
      ]
    },
    {
      "title": "AI, Hardware, and Virtual Reality",
      "publishedDate": "2023-09-28T11:12:01-07:00",
      "updatedDate": "2023-10-02T07:19:40-07:00",
      "contentHtml": "\n\t\t<p>In <a href=\"https://stratechery.com/2023/an-interview-with-craig-moffett-about-charter-vs-disney-and-the-path-dependency-of-the-communications-industry/\">a recent interview I did with Craig Moffett</a> we discussed why there is a “TMT” sector when it comes to industry classifications. TMT stands for technology, media, and telecoms, and what unifies them is that all deal in a world of massive up-front investment — i.e. huge fixed costs — and then near perfect scalability once deployed — zero marginal costs.</p>\n<p>Each of these three categories, though, is distinct in the experience they provide:</p>\n<ul>\n<li>Media is a recording or publication that enables a shift in time between production and consumption.</li>\n<li>Telecoms enables a shift in place when it comes to communication.</li>\n<li>Technology, which generally means software, enables interactivity at scale.</li>\n</ul>\n<p>Another way to think about these categories is that if reality is the time and place in which one currently exists, each provides a form of virtual reality:</p>\n<ul>\n<li>Media consumption entails consuming content that was created at a different time.</li>\n<li>Communication entails talking to someone who is in a different place.</li>\n<li>Software entails manipulating bits on a computer in a manner that doesn’t actually change anything about your physical space, just the virtual one.</li>\n</ul>\n<p>The constraint on each of these is the same: human time and attention. Media needs to be created, software needs to be manipulated, and communication depends on there being someone to communicate with. That human constraint, by extension, is perhaps why we don’t actually call media, communication, or software “virtual reality”, despite the defiance of reality I noted above. No matter how profound the changes wrought by digitization, the human component remains.</p>\n<p>AI removes the human constraint: media and interactive experiences can be created continuously; the costs may be substantial, particularly compared to general compute, but are practically zero relative to the cost of humans. The most compelling use case to date, though, is communication: there is always someone to talk to.</p>\n<h3>ChatGPT Talks and Sees</h3>\n<p>The first AI announcement of the week was literally AI that can talk: OpenAI announced that you can now converse with ChatGPT, and I found the experience profound.</p>\n<p>You have obviously been able to chat with ChatGPT via text for many months now; what I only truly appreciated after <em>talking</em> with ChatGPT, though, was just how much work it was to type out questions and read answers. There was, in other words, a human constraint in our conversations that made it feel like I was using a tool; small wonder that the vast majority of my interaction with ChatGPT has been to do some sort of research, or try to remember something on the edge of my memory, too fuzzy to type a clear search term into Google.</p>\n<p>Simply talking, though, removed that barrier: I quickly found myself having philosophical discussions including, for example, the nature of virtual reality. It was the discussion itself that provided a clue: virtual reality feels real, but something can only feel real if human constraints are no longer apparent. In the case of conversation, there is no effort required to talk to another human in person, or on the phone; to talk to them via chat is certainly convenient, but there is a much more tangible separation. So it is with ChatGPT.<sup id=\"rf1-11652\"><a href=\"#fn1-11652\" title=\"The experience wasn’t perfect, both from a usability standpoint, and from the overly verbose answers, delivered in ChatGPT’s sterile style; you can see the seeds of something very compelling though\" rel=\"footnote\">1</a></sup></p>\n<p>The second AI announcement was that ChatGPT now has vision: you can take a picture of an object or a math problem, and ask ChatGPT about it. It’s a very powerful capability, particularly because it seems that GPT-4 is truly multi-modal: there isn’t some sort of translation layer in-between. The limitation, though, was effort: I had to open up the camera, take a picture, and then ask some sort of question. To put it another way, the impressiveness of the vision capability was, at least for me, somewhat diminished by the fact said capability was released at the same time as voice chat, which impressed precisely because it was easy.</p>\n<p>What is interesting is that I had the opposite reaction during a demo last week: when I watched someone from OpenAI demonstrate vision, it seemed like the more impressive feature by far. The context in which I observed that demo, though, was a Zoom call, which meant I was engaging with the feature on a distant and more intellectual level —&nbsp;a level not dissimilar from how I might have interacted with ChatGPT when I had to type my questions and read the answers. To simply talk, meanwhile, wasn’t very impressive to observe, but was much more impressive when I was the one interacting.</p>\n<h3>Meta AI and Emu</h3>\n<p>The next set of AI announcements happened yesterday at Meta Connect. Meta is releasing its own chatbot, called Meta AI, and a fleet of celebrity-based AI characters, with the promise of more to come, and a developer platform to boot. I haven’t used any of these products, which are, for now, limited to text interactions. What the releases point to, though, is the removal of another kind of human constraint: in the very near future literally billions of people can talk to Tom Brady or Snoop Dogg, all at the same time.</p>\n<p>I doubt, for the record, that celebrity chat bots will ever be much more than a novelty and cool demo, but that is only because they will be superceded by bots that are actually tuned much more explicitly to every individual;<sup id=\"rf2-11652\"><a href=\"#fn2-11652\" title=\"And startups like <a href=&quot;https://beta.character.ai/&quot;>Character.AI</a> are reportedly doing extremely well\" rel=\"footnote\">2</a></sup> each individual bot, though, will have the same absence of constraint inherent in conversations with real people: the bot will always be available, no matter what.</p>\n<p>Meta also announced that you will be able to use Emu, its image generation model, to create custom stickers in chat, and to edit photos in Instagram. Both seem immediately useful, not because Emu is particularly good — that remains to be seen — but because its capabilities are being applied in an immediately useful use case, in a pre-existing channel. The existence of these channels, whether they be Meta’s messaging apps or its social networks, is why Meta was always destined to be a force in AI: it is one thing to build a product that people choose to use, and another, significantly easier thing, to augment a product people already use every day. Less friction is key!</p>\n<h3>Meta Smart Glasses</h3>\n<p>The most compelling announcement for me, though, was a hardware product, specifically the updated Meta Smart Glasses. Here is the key part of the introduction:</p>\n<div align=\"center\"><div id=\"v-pmocaNTS-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/pmocaNTS?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div>\n<blockquote><p>\n  The most interesting thing about this isn’t any of those specs. It’s that these are the first smart glasses that are built and shipping with Meta AI in them. Starting in the US you’re going to get a state-of-the-art AI that you can interact with hands-free wherever you go…</p>\n<p>  This is just the beginning, because this is just audio. It’s basically just text. Starting next year, we’re going to be issuing a free software update to the glasses that makes them multi-modal. So the glasses are going to be able to understand what you’re looking at when you ask them questions. So if you want to know what the building is that you’re standing in front of, or if you want to translate a sign that’s in front of you to know what it’s saying, or if you need help fixing this sad leaky faucet, you can just talk to Meta AI and look at it and it will walk you through it step-by-step how to do it.</p>\n<p>  I think that smart glasses are going to be an important platform for the future, not only because they’re the natural way to put holograms in the world, so we can put digital objects in our physical space, but also — if you think about it, smart glasses are the ideal form factor for you to let an AI assistant see what you’re seeing and hear what you’re hearing.\n</p></blockquote>\n<p>I wonder what my reaction would have been to this announcement had I not experienced the new OpenAI features above, because I basically just made the case for smart glasses: there is a step-change in usability when the human constraint is removed, which is to say that ChatGPT’s vision capabilities seem less useful to me because it takes effort to invoke and interact with it, which is to further say I agree with Zuckerberg that smart glasses are an ideal form factor for this sort of capability.</p>\n<h3>The Hardware Key</h3>\n<p>What was most remarkable about this announcement, though, is the admission that followed:</p>\n<div align=\"center\"><div id=\"v-8SvIWITf-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/8SvIWITf?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div>\n<blockquote><p>\n  Before this last year’s AI breakthroughs, I kind of thought that smart glasses were only really only going to become ubiquitous once we dialed in the holograms and the displays and all that stuff, which we’re making progress on, but is somewhat longer. But now, I think that the AI part of this is going to be just as important in smart glasses being widely adopted as any of the other augmented reality features.\n</p></blockquote>\n<p>It was just 11 months ago that Meta’s stock was plummeting thanks to investor angst about its business, exacerbated by the perception that Meta had shifted to the Metaverse in a desperate attempt to find new growth. This was an incorrect perception, of course, which I explained in <a href=\"https://stratechery.com/2022/meta-myths/\">Meta Myths</a>: users were not deserting Facebook, Instagram engagement was not plummeting, TikTok’s growth had been arrested, advertising was not dying, and Meta’s spending, particularly on AI, was not a waste. At the end, though, I said that one thing was maybe true: the Metaverse might be a waste of time and money.</p>\n<p>However, it seems possible that AI — to Zuckerberg’s surprise — may save the day. This smart glasses announcement is — more than the Quest 3 — evidence that Meta’s bet on hardware might pay off. AI is truly something new and revolutionary and capable of being something more than just a homework aid, but I don’t think the existing interfaces are the right ones. Talking to ChatGPT is better than typing, but I still have to launch the app and set the mode; vision is an amazing capability, but it requires even more intent and friction to invoke. I could see a scenario where Meta’s AI is inferior technically to OpenAI, but more useful simply because it comes in a better form factor.</p>\n<p>This is why I wasn’t surprised by this week’s final piece of AI news, first reported by <a href=\"https://www.theinformation.com/articles/designer-jony-ive-and-open-ais-sam-altman-discuss-ai-hardware-project\">The Information</a>:</p>\n<blockquote><p>\n  Jony Ive, the renowned designer of the iPhone, and OpenAI CEO Sam Altman have been discussing building a new AI hardware device, according to two people familiar with the conversations. SoftBank CEO and investor Masayoshi Son has talked to both about the idea, according to one of these people, but it is unclear if he will remain involved.\n</p></blockquote>\n<p>The <a href=\"https://www.ft.com/content/4c64ffc1-f57b-4e22-a4a5-f9f90a7419b7\">Financial Times</a> added more details:</p>\n<blockquote><p>\n  Sam Altman, OpenAI’s chief, has tapped Ive’s company LoveFrom, which the designer founded when he left Apple in 2019, to develop the ChatGPT creator’s first consumer device, according to three people familiar with the plan. Altman and Ive have held brainstorming sessions at the designer’s San Francisco studio about what a new consumer product centred on OpenAI’s technology would look like, the people said. They hope to create a more natural and intuitive user experience for interacting with AI, in the way that the iPhone’s innovations in touchscreen computing unleashed the mass-market potential of the mobile internet. The process of identifying a design or device remains at an early stage with many different ideas on the table, they said.</p>\n<p>  Son, SoftBank’s founder and chief executive, has also been involved in some of the discussions, pitching a central role for Arm — the chip designer in which the Japanese conglomerate holds a 90 per cent stake — as well as offering financial backing. Son, Altman and Ive have discussed creating a company that would draw on talent and technology from their three groups, the people said, with SoftBank investing more than $1bn in the venture.\n</p></blockquote>\n<p>There are obviously many steps before a potential hardware product, including actually agreeing to build one. And there is, of course, the fact that Apple and Google already make devices everyone carries, with the latter in particular investing heavily in its own AI capabilities; betting on the hardware in market winning the hardware opportunity in AI is the safest bet.</p>\n<p>That may not be a reason for either OpenAI or Meta to abandon their efforts, though: waging a hardware battle against Google and Apple would be difficult, but it might be even worse to be “just an app” if the full realization of AI’s capabilities depend on fully removing human friction from the process.</p>\n<h3>Virtual Reality</h3>\n<p>I should, I suppose, mention the Quest 3, which was formally announced at Meta’s event, given that I opened this Article with allusions to “Virtual Reality.” I have used a prototype Quest 3 device, but not a release version, and so can’t fully comment on its capabilities or the experience; what I will note is that the mixed reality gaming experiences were incredibly fun, particularly a Zombie shooter that is set in the room you are located in.</p>\n<p>That’s the thing, though: Quest 3 still strikes me mostly as a console, while the <a href=\"https://stratechery.com/2023/apple-vision/\">Apple Vision</a> strikes me mostly as a productivity device. Both are interesting niches, but niches nonetheless. What seems essential for both to fully realize the vision of virtual reality is to lose their current sense of boundedness and friction, which is to say that <a href=\"https://stratechery.com/2022/dall-e-the-metaverse-and-zero-marginal-content/\">both need AI generation</a>.</p>\n<p>In fact, I would argue that defining “virtual reality” to mean an immersive headset is to miss the point: virtual reality is a digital experience that has fully broken the bounds of human constraints, and in that experience the hardware is a means, not an end. Moreover, a virtual reality experience need not involve vision at all: talking with ChatGPT, for example, is an aural experience that feels more like virtual reality than the majority of experiences I’ve had in a headset.</p>\n<p>True virtual reality shifts time like media, place like communications, and, crucially, does so with perfect availability and infinite capacity. In this view, virtual reality is AI, and AI is virtual reality. Hardware does matter — that has been the focus of this Article — but it matters as a means to an end, to enable an interactive experience without the constraints of human capacity or the friction of actual reality.</p>\n<p><em>I wrote a follow-up to this Article in <a href=\"https://stratechery.com/2023/ai-and-ambient-computing-zuckerberg-and-the-verge-amazon-and-anthropic/\">this Daily Update</a>.</em></p>\n<hr class=\"footnotes\"><ol class=\"footnotes\" style=\"list-style-type:decimal\"><li id=\"fn1-11652\"><p>The experience wasn’t perfect, both from a usability standpoint, and from the overly verbose answers, delivered in ChatGPT’s sterile style; you can see the seeds of something very compelling though&nbsp;<a href=\"#rf1-11652\" class=\"backlink\" title=\"Return to footnote 1.\">↩</a></p></li><li id=\"fn2-11652\"><p>And startups like <a href=\"https://beta.character.ai/\">Character.AI</a> are reportedly doing extremely well&nbsp;<a href=\"#rf2-11652\" class=\"backlink\" title=\"Return to footnote 2.\">↩</a></p></li></ol>\n\t\t\t",
      "contentText": "\n\t\tIn a recent interview I did with Craig Moffett we discussed why there is a “TMT” sector when it comes to industry classifications. TMT stands for technology, media, and telecoms, and what unifies them is that all deal in a world of massive up-front investment — i.e. huge fixed costs — and then near perfect scalability once deployed — zero marginal costs.\nEach of these three categories, though, is distinct in the experience they provide:\n\nMedia is a recording or publication that enables a shift in time between production and consumption.\nTelecoms enables a shift in place when it comes to communication.\nTechnology, which generally means software, enables interactivity at scale.\n\nAnother way to think about these categories is that if reality is the time and place in which one currently exists, each provides a form of virtual reality:\n\nMedia consumption entails consuming content that was created at a different time.\nCommunication entails talking to someone who is in a different place.\nSoftware entails manipulating bits on a computer in a manner that doesn’t actually change anything about your physical space, just the virtual one.\n\nThe constraint on each of these is the same: human time and attention. Media needs to be created, software needs to be manipulated, and communication depends on there being someone to communicate with. That human constraint, by extension, is perhaps why we don’t actually call media, communication, or software “virtual reality”, despite the defiance of reality I noted above. No matter how profound the changes wrought by digitization, the human component remains.\nAI removes the human constraint: media and interactive experiences can be created continuously; the costs may be substantial, particularly compared to general compute, but are practically zero relative to the cost of humans. The most compelling use case to date, though, is communication: there is always someone to talk to.\nChatGPT Talks and Sees\nThe first AI announcement of the week was literally AI that can talk: OpenAI announced that you can now converse with ChatGPT, and I found the experience profound.\nYou have obviously been able to chat with ChatGPT via text for many months now; what I only truly appreciated after talking with ChatGPT, though, was just how much work it was to type out questions and read answers. There was, in other words, a human constraint in our conversations that made it feel like I was using a tool; small wonder that the vast majority of my interaction with ChatGPT has been to do some sort of research, or try to remember something on the edge of my memory, too fuzzy to type a clear search term into Google.\nSimply talking, though, removed that barrier: I quickly found myself having philosophical discussions including, for example, the nature of virtual reality. It was the discussion itself that provided a clue: virtual reality feels real, but something can only feel real if human constraints are no longer apparent. In the case of conversation, there is no effort required to talk to another human in person, or on the phone; to talk to them via chat is certainly convenient, but there is a much more tangible separation. So it is with ChatGPT.1\nThe second AI announcement was that ChatGPT now has vision: you can take a picture of an object or a math problem, and ask ChatGPT about it. It’s a very powerful capability, particularly because it seems that GPT-4 is truly multi-modal: there isn’t some sort of translation layer in-between. The limitation, though, was effort: I had to open up the camera, take a picture, and then ask some sort of question. To put it another way, the impressiveness of the vision capability was, at least for me, somewhat diminished by the fact said capability was released at the same time as voice chat, which impressed precisely because it was easy.\nWhat is interesting is that I had the opposite reaction during a demo last week: when I watched someone from OpenAI demonstrate vision, it seemed like the more impressive feature by far. The context in which I observed that demo, though, was a Zoom call, which meant I was engaging with the feature on a distant and more intellectual level — a level not dissimilar from how I might have interacted with ChatGPT when I had to type my questions and read the answers. To simply talk, meanwhile, wasn’t very impressive to observe, but was much more impressive when I was the one interacting.\nMeta AI and Emu\nThe next set of AI announcements happened yesterday at Meta Connect. Meta is releasing its own chatbot, called Meta AI, and a fleet of celebrity-based AI characters, with the promise of more to come, and a developer platform to boot. I haven’t used any of these products, which are, for now, limited to text interactions. What the releases point to, though, is the removal of another kind of human constraint: in the very near future literally billions of people can talk to Tom Brady or Snoop Dogg, all at the same time.\nI doubt, for the record, that celebrity chat bots will ever be much more than a novelty and cool demo, but that is only because they will be superceded by bots that are actually tuned much more explicitly to every individual;2 each individual bot, though, will have the same absence of constraint inherent in conversations with real people: the bot will always be available, no matter what.\nMeta also announced that you will be able to use Emu, its image generation model, to create custom stickers in chat, and to edit photos in Instagram. Both seem immediately useful, not because Emu is particularly good — that remains to be seen — but because its capabilities are being applied in an immediately useful use case, in a pre-existing channel. The existence of these channels, whether they be Meta’s messaging apps or its social networks, is why Meta was always destined to be a force in AI: it is one thing to build a product that people choose to use, and another, significantly easier thing, to augment a product people already use every day. Less friction is key!\nMeta Smart Glasses\nThe most compelling announcement for me, though, was a hardware product, specifically the updated Meta Smart Glasses. Here is the key part of the introduction:\n\n\n  The most interesting thing about this isn’t any of those specs. It’s that these are the first smart glasses that are built and shipping with Meta AI in them. Starting in the US you’re going to get a state-of-the-art AI that you can interact with hands-free wherever you go…\n  This is just the beginning, because this is just audio. It’s basically just text. Starting next year, we’re going to be issuing a free software update to the glasses that makes them multi-modal. So the glasses are going to be able to understand what you’re looking at when you ask them questions. So if you want to know what the building is that you’re standing in front of, or if you want to translate a sign that’s in front of you to know what it’s saying, or if you need help fixing this sad leaky faucet, you can just talk to Meta AI and look at it and it will walk you through it step-by-step how to do it.\n  I think that smart glasses are going to be an important platform for the future, not only because they’re the natural way to put holograms in the world, so we can put digital objects in our physical space, but also — if you think about it, smart glasses are the ideal form factor for you to let an AI assistant see what you’re seeing and hear what you’re hearing.\n\nI wonder what my reaction would have been to this announcement had I not experienced the new OpenAI features above, because I basically just made the case for smart glasses: there is a step-change in usability when the human constraint is removed, which is to say that ChatGPT’s vision capabilities seem less useful to me because it takes effort to invoke and interact with it, which is to further say I agree with Zuckerberg that smart glasses are an ideal form factor for this sort of capability.\nThe Hardware Key\nWhat was most remarkable about this announcement, though, is the admission that followed:\n\n\n  Before this last year’s AI breakthroughs, I kind of thought that smart glasses were only really only going to become ubiquitous once we dialed in the holograms and the displays and all that stuff, which we’re making progress on, but is somewhat longer. But now, I think that the AI part of this is going to be just as important in smart glasses being widely adopted as any of the other augmented reality features.\n\nIt was just 11 months ago that Meta’s stock was plummeting thanks to investor angst about its business, exacerbated by the perception that Meta had shifted to the Metaverse in a desperate attempt to find new growth. This was an incorrect perception, of course, which I explained in Meta Myths: users were not deserting Facebook, Instagram engagement was not plummeting, TikTok’s growth had been arrested, advertising was not dying, and Meta’s spending, particularly on AI, was not a waste. At the end, though, I said that one thing was maybe true: the Metaverse might be a waste of time and money.\nHowever, it seems possible that AI — to Zuckerberg’s surprise — may save the day. This smart glasses announcement is — more than the Quest 3 — evidence that Meta’s bet on hardware might pay off. AI is truly something new and revolutionary and capable of being something more than just a homework aid, but I don’t think the existing interfaces are the right ones. Talking to ChatGPT is better than typing, but I still have to launch the app and set the mode; vision is an amazing capability, but it requires even more intent and friction to invoke. I could see a scenario where Meta’s AI is inferior technically to OpenAI, but more useful simply because it comes in a better form factor.\nThis is why I wasn’t surprised by this week’s final piece of AI news, first reported by The Information:\n\n  Jony Ive, the renowned designer of the iPhone, and OpenAI CEO Sam Altman have been discussing building a new AI hardware device, according to two people familiar with the conversations. SoftBank CEO and investor Masayoshi Son has talked to both about the idea, according to one of these people, but it is unclear if he will remain involved.\n\nThe Financial Times added more details:\n\n  Sam Altman, OpenAI’s chief, has tapped Ive’s company LoveFrom, which the designer founded when he left Apple in 2019, to develop the ChatGPT creator’s first consumer device, according to three people familiar with the plan. Altman and Ive have held brainstorming sessions at the designer’s San Francisco studio about what a new consumer product centred on OpenAI’s technology would look like, the people said. They hope to create a more natural and intuitive user experience for interacting with AI, in the way that the iPhone’s innovations in touchscreen computing unleashed the mass-market potential of the mobile internet. The process of identifying a design or device remains at an early stage with many different ideas on the table, they said.\n  Son, SoftBank’s founder and chief executive, has also been involved in some of the discussions, pitching a central role for Arm — the chip designer in which the Japanese conglomerate holds a 90 per cent stake — as well as offering financial backing. Son, Altman and Ive have discussed creating a company that would draw on talent and technology from their three groups, the people said, with SoftBank investing more than $1bn in the venture.\n\nThere are obviously many steps before a potential hardware product, including actually agreeing to build one. And there is, of course, the fact that Apple and Google already make devices everyone carries, with the latter in particular investing heavily in its own AI capabilities; betting on the hardware in market winning the hardware opportunity in AI is the safest bet.\nThat may not be a reason for either OpenAI or Meta to abandon their efforts, though: waging a hardware battle against Google and Apple would be difficult, but it might be even worse to be “just an app” if the full realization of AI’s capabilities depend on fully removing human friction from the process.\nVirtual Reality\nI should, I suppose, mention the Quest 3, which was formally announced at Meta’s event, given that I opened this Article with allusions to “Virtual Reality.” I have used a prototype Quest 3 device, but not a release version, and so can’t fully comment on its capabilities or the experience; what I will note is that the mixed reality gaming experiences were incredibly fun, particularly a Zombie shooter that is set in the room you are located in.\nThat’s the thing, though: Quest 3 still strikes me mostly as a console, while the Apple Vision strikes me mostly as a productivity device. Both are interesting niches, but niches nonetheless. What seems essential for both to fully realize the vision of virtual reality is to lose their current sense of boundedness and friction, which is to say that both need AI generation.\nIn fact, I would argue that defining “virtual reality” to mean an immersive headset is to miss the point: virtual reality is a digital experience that has fully broken the bounds of human constraints, and in that experience the hardware is a means, not an end. Moreover, a virtual reality experience need not involve vision at all: talking with ChatGPT, for example, is an aural experience that feels more like virtual reality than the majority of experiences I’ve had in a headset.\nTrue virtual reality shifts time like media, place like communications, and, crucially, does so with perfect availability and infinite capacity. In this view, virtual reality is AI, and AI is virtual reality. Hardware does matter — that has been the focus of this Article — but it matters as a means to an end, to enable an interactive experience without the constraints of human capacity or the friction of actual reality.\nI wrote a follow-up to this Article in this Daily Update.\nThe experience wasn’t perfect, both from a usability standpoint, and from the overly verbose answers, delivered in ChatGPT’s sterile style; you can see the seeds of something very compelling though ↩And startups like Character.AI are reportedly doing extremely well ↩\n\t\t\t",
      "subsections": [
        {
          "subtitle": "ChatGPT Talks and Sees",
          "contentHtml": "<p>The first AI announcement of the week was literally AI that can talk: OpenAI announced that you can now converse with ChatGPT, and I found the experience profound.</p><p>You have obviously been able to chat with ChatGPT via text for many months now; what I only truly appreciated after <em>talking</em> with ChatGPT, though, was just how much work it was to type out questions and read answers. There was, in other words, a human constraint in our conversations that made it feel like I was using a tool; small wonder that the vast majority of my interaction with ChatGPT has been to do some sort of research, or try to remember something on the edge of my memory, too fuzzy to type a clear search term into Google.</p><p>Simply talking, though, removed that barrier: I quickly found myself having philosophical discussions including, for example, the nature of virtual reality. It was the discussion itself that provided a clue: virtual reality feels real, but something can only feel real if human constraints are no longer apparent. In the case of conversation, there is no effort required to talk to another human in person, or on the phone; to talk to them via chat is certainly convenient, but there is a much more tangible separation. So it is with ChatGPT.<sup id=\"rf1-11652\"><a href=\"#fn1-11652\" title=\"The experience wasn’t perfect, both from a usability standpoint, and from the overly verbose answers, delivered in ChatGPT’s sterile style; you can see the seeds of something very compelling though\" rel=\"footnote\">1</a></sup></p><p>The second AI announcement was that ChatGPT now has vision: you can take a picture of an object or a math problem, and ask ChatGPT about it. It’s a very powerful capability, particularly because it seems that GPT-4 is truly multi-modal: there isn’t some sort of translation layer in-between. The limitation, though, was effort: I had to open up the camera, take a picture, and then ask some sort of question. To put it another way, the impressiveness of the vision capability was, at least for me, somewhat diminished by the fact said capability was released at the same time as voice chat, which impressed precisely because it was easy.</p><p>What is interesting is that I had the opposite reaction during a demo last week: when I watched someone from OpenAI demonstrate vision, it seemed like the more impressive feature by far. The context in which I observed that demo, though, was a Zoom call, which meant I was engaging with the feature on a distant and more intellectual level —&nbsp;a level not dissimilar from how I might have interacted with ChatGPT when I had to type my questions and read the answers. To simply talk, meanwhile, wasn’t very impressive to observe, but was much more impressive when I was the one interacting.</p>",
          "contentText": "The first AI announcement of the week was literally AI that can talk: OpenAI announced that you can now converse with ChatGPT, and I found the experience profound.You have obviously been able to chat with ChatGPT via text for many months now; what I only truly appreciated after talking with ChatGPT, though, was just how much work it was to type out questions and read answers. There was, in other words, a human constraint in our conversations that made it feel like I was using a tool; small wonder that the vast majority of my interaction with ChatGPT has been to do some sort of research, or try to remember something on the edge of my memory, too fuzzy to type a clear search term into Google.Simply talking, though, removed that barrier: I quickly found myself having philosophical discussions including, for example, the nature of virtual reality. It was the discussion itself that provided a clue: virtual reality feels real, but something can only feel real if human constraints are no longer apparent. In the case of conversation, there is no effort required to talk to another human in person, or on the phone; to talk to them via chat is certainly convenient, but there is a much more tangible separation. So it is with ChatGPT.1The second AI announcement was that ChatGPT now has vision: you can take a picture of an object or a math problem, and ask ChatGPT about it. It’s a very powerful capability, particularly because it seems that GPT-4 is truly multi-modal: there isn’t some sort of translation layer in-between. The limitation, though, was effort: I had to open up the camera, take a picture, and then ask some sort of question. To put it another way, the impressiveness of the vision capability was, at least for me, somewhat diminished by the fact said capability was released at the same time as voice chat, which impressed precisely because it was easy.What is interesting is that I had the opposite reaction during a demo last week: when I watched someone from OpenAI demonstrate vision, it seemed like the more impressive feature by far. The context in which I observed that demo, though, was a Zoom call, which meant I was engaging with the feature on a distant and more intellectual level — a level not dissimilar from how I might have interacted with ChatGPT when I had to type my questions and read the answers. To simply talk, meanwhile, wasn’t very impressive to observe, but was much more impressive when I was the one interacting."
        },
        {
          "subtitle": "Meta AI and Emu",
          "contentHtml": "<p>The next set of AI announcements happened yesterday at Meta Connect. Meta is releasing its own chatbot, called Meta AI, and a fleet of celebrity-based AI characters, with the promise of more to come, and a developer platform to boot. I haven’t used any of these products, which are, for now, limited to text interactions. What the releases point to, though, is the removal of another kind of human constraint: in the very near future literally billions of people can talk to Tom Brady or Snoop Dogg, all at the same time.</p><p>I doubt, for the record, that celebrity chat bots will ever be much more than a novelty and cool demo, but that is only because they will be superceded by bots that are actually tuned much more explicitly to every individual;<sup id=\"rf2-11652\"><a href=\"#fn2-11652\" title=\"And startups like <a href=&quot;https://beta.character.ai/&quot;>Character.AI</a> are reportedly doing extremely well\" rel=\"footnote\">2</a></sup> each individual bot, though, will have the same absence of constraint inherent in conversations with real people: the bot will always be available, no matter what.</p><p>Meta also announced that you will be able to use Emu, its image generation model, to create custom stickers in chat, and to edit photos in Instagram. Both seem immediately useful, not because Emu is particularly good — that remains to be seen — but because its capabilities are being applied in an immediately useful use case, in a pre-existing channel. The existence of these channels, whether they be Meta’s messaging apps or its social networks, is why Meta was always destined to be a force in AI: it is one thing to build a product that people choose to use, and another, significantly easier thing, to augment a product people already use every day. Less friction is key!</p>",
          "contentText": "The next set of AI announcements happened yesterday at Meta Connect. Meta is releasing its own chatbot, called Meta AI, and a fleet of celebrity-based AI characters, with the promise of more to come, and a developer platform to boot. I haven’t used any of these products, which are, for now, limited to text interactions. What the releases point to, though, is the removal of another kind of human constraint: in the very near future literally billions of people can talk to Tom Brady or Snoop Dogg, all at the same time.I doubt, for the record, that celebrity chat bots will ever be much more than a novelty and cool demo, but that is only because they will be superceded by bots that are actually tuned much more explicitly to every individual;2 each individual bot, though, will have the same absence of constraint inherent in conversations with real people: the bot will always be available, no matter what.Meta also announced that you will be able to use Emu, its image generation model, to create custom stickers in chat, and to edit photos in Instagram. Both seem immediately useful, not because Emu is particularly good — that remains to be seen — but because its capabilities are being applied in an immediately useful use case, in a pre-existing channel. The existence of these channels, whether they be Meta’s messaging apps or its social networks, is why Meta was always destined to be a force in AI: it is one thing to build a product that people choose to use, and another, significantly easier thing, to augment a product people already use every day. Less friction is key!"
        },
        {
          "subtitle": "Meta Smart Glasses",
          "contentHtml": "<p>The most compelling announcement for me, though, was a hardware product, specifically the updated Meta Smart Glasses. Here is the key part of the introduction:</p><div align=\"center\"><div id=\"v-pmocaNTS-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/pmocaNTS?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div><blockquote><p>\n  The most interesting thing about this isn’t any of those specs. It’s that these are the first smart glasses that are built and shipping with Meta AI in them. Starting in the US you’re going to get a state-of-the-art AI that you can interact with hands-free wherever you go…</p>\n<p>  This is just the beginning, because this is just audio. It’s basically just text. Starting next year, we’re going to be issuing a free software update to the glasses that makes them multi-modal. So the glasses are going to be able to understand what you’re looking at when you ask them questions. So if you want to know what the building is that you’re standing in front of, or if you want to translate a sign that’s in front of you to know what it’s saying, or if you need help fixing this sad leaky faucet, you can just talk to Meta AI and look at it and it will walk you through it step-by-step how to do it.</p>\n<p>  I think that smart glasses are going to be an important platform for the future, not only because they’re the natural way to put holograms in the world, so we can put digital objects in our physical space, but also — if you think about it, smart glasses are the ideal form factor for you to let an AI assistant see what you’re seeing and hear what you’re hearing.\n</p></blockquote><p>I wonder what my reaction would have been to this announcement had I not experienced the new OpenAI features above, because I basically just made the case for smart glasses: there is a step-change in usability when the human constraint is removed, which is to say that ChatGPT’s vision capabilities seem less useful to me because it takes effort to invoke and interact with it, which is to further say I agree with Zuckerberg that smart glasses are an ideal form factor for this sort of capability.</p>",
          "contentText": "The most compelling announcement for me, though, was a hardware product, specifically the updated Meta Smart Glasses. Here is the key part of the introduction:\n  The most interesting thing about this isn’t any of those specs. It’s that these are the first smart glasses that are built and shipping with Meta AI in them. Starting in the US you’re going to get a state-of-the-art AI that you can interact with hands-free wherever you go…\n  This is just the beginning, because this is just audio. It’s basically just text. Starting next year, we’re going to be issuing a free software update to the glasses that makes them multi-modal. So the glasses are going to be able to understand what you’re looking at when you ask them questions. So if you want to know what the building is that you’re standing in front of, or if you want to translate a sign that’s in front of you to know what it’s saying, or if you need help fixing this sad leaky faucet, you can just talk to Meta AI and look at it and it will walk you through it step-by-step how to do it.\n  I think that smart glasses are going to be an important platform for the future, not only because they’re the natural way to put holograms in the world, so we can put digital objects in our physical space, but also — if you think about it, smart glasses are the ideal form factor for you to let an AI assistant see what you’re seeing and hear what you’re hearing.\nI wonder what my reaction would have been to this announcement had I not experienced the new OpenAI features above, because I basically just made the case for smart glasses: there is a step-change in usability when the human constraint is removed, which is to say that ChatGPT’s vision capabilities seem less useful to me because it takes effort to invoke and interact with it, which is to further say I agree with Zuckerberg that smart glasses are an ideal form factor for this sort of capability."
        },
        {
          "subtitle": "The Hardware Key",
          "contentHtml": "<p>What was most remarkable about this announcement, though, is the admission that followed:</p><div align=\"center\"><div id=\"v-8SvIWITf-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/8SvIWITf?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div><blockquote><p>\n  Before this last year’s AI breakthroughs, I kind of thought that smart glasses were only really only going to become ubiquitous once we dialed in the holograms and the displays and all that stuff, which we’re making progress on, but is somewhat longer. But now, I think that the AI part of this is going to be just as important in smart glasses being widely adopted as any of the other augmented reality features.\n</p></blockquote><p>It was just 11 months ago that Meta’s stock was plummeting thanks to investor angst about its business, exacerbated by the perception that Meta had shifted to the Metaverse in a desperate attempt to find new growth. This was an incorrect perception, of course, which I explained in <a href=\"https://stratechery.com/2022/meta-myths/\">Meta Myths</a>: users were not deserting Facebook, Instagram engagement was not plummeting, TikTok’s growth had been arrested, advertising was not dying, and Meta’s spending, particularly on AI, was not a waste. At the end, though, I said that one thing was maybe true: the Metaverse might be a waste of time and money.</p><p>However, it seems possible that AI — to Zuckerberg’s surprise — may save the day. This smart glasses announcement is — more than the Quest 3 — evidence that Meta’s bet on hardware might pay off. AI is truly something new and revolutionary and capable of being something more than just a homework aid, but I don’t think the existing interfaces are the right ones. Talking to ChatGPT is better than typing, but I still have to launch the app and set the mode; vision is an amazing capability, but it requires even more intent and friction to invoke. I could see a scenario where Meta’s AI is inferior technically to OpenAI, but more useful simply because it comes in a better form factor.</p><p>This is why I wasn’t surprised by this week’s final piece of AI news, first reported by <a href=\"https://www.theinformation.com/articles/designer-jony-ive-and-open-ais-sam-altman-discuss-ai-hardware-project\">The Information</a>:</p><blockquote><p>\n  Jony Ive, the renowned designer of the iPhone, and OpenAI CEO Sam Altman have been discussing building a new AI hardware device, according to two people familiar with the conversations. SoftBank CEO and investor Masayoshi Son has talked to both about the idea, according to one of these people, but it is unclear if he will remain involved.\n</p></blockquote><p>The <a href=\"https://www.ft.com/content/4c64ffc1-f57b-4e22-a4a5-f9f90a7419b7\">Financial Times</a> added more details:</p><blockquote><p>\n  Sam Altman, OpenAI’s chief, has tapped Ive’s company LoveFrom, which the designer founded when he left Apple in 2019, to develop the ChatGPT creator’s first consumer device, according to three people familiar with the plan. Altman and Ive have held brainstorming sessions at the designer’s San Francisco studio about what a new consumer product centred on OpenAI’s technology would look like, the people said. They hope to create a more natural and intuitive user experience for interacting with AI, in the way that the iPhone’s innovations in touchscreen computing unleashed the mass-market potential of the mobile internet. The process of identifying a design or device remains at an early stage with many different ideas on the table, they said.</p>\n<p>  Son, SoftBank’s founder and chief executive, has also been involved in some of the discussions, pitching a central role for Arm — the chip designer in which the Japanese conglomerate holds a 90 per cent stake — as well as offering financial backing. Son, Altman and Ive have discussed creating a company that would draw on talent and technology from their three groups, the people said, with SoftBank investing more than $1bn in the venture.\n</p></blockquote><p>There are obviously many steps before a potential hardware product, including actually agreeing to build one. And there is, of course, the fact that Apple and Google already make devices everyone carries, with the latter in particular investing heavily in its own AI capabilities; betting on the hardware in market winning the hardware opportunity in AI is the safest bet.</p><p>That may not be a reason for either OpenAI or Meta to abandon their efforts, though: waging a hardware battle against Google and Apple would be difficult, but it might be even worse to be “just an app” if the full realization of AI’s capabilities depend on fully removing human friction from the process.</p>",
          "contentText": "What was most remarkable about this announcement, though, is the admission that followed:\n  Before this last year’s AI breakthroughs, I kind of thought that smart glasses were only really only going to become ubiquitous once we dialed in the holograms and the displays and all that stuff, which we’re making progress on, but is somewhat longer. But now, I think that the AI part of this is going to be just as important in smart glasses being widely adopted as any of the other augmented reality features.\nIt was just 11 months ago that Meta’s stock was plummeting thanks to investor angst about its business, exacerbated by the perception that Meta had shifted to the Metaverse in a desperate attempt to find new growth. This was an incorrect perception, of course, which I explained in Meta Myths: users were not deserting Facebook, Instagram engagement was not plummeting, TikTok’s growth had been arrested, advertising was not dying, and Meta’s spending, particularly on AI, was not a waste. At the end, though, I said that one thing was maybe true: the Metaverse might be a waste of time and money.However, it seems possible that AI — to Zuckerberg’s surprise — may save the day. This smart glasses announcement is — more than the Quest 3 — evidence that Meta’s bet on hardware might pay off. AI is truly something new and revolutionary and capable of being something more than just a homework aid, but I don’t think the existing interfaces are the right ones. Talking to ChatGPT is better than typing, but I still have to launch the app and set the mode; vision is an amazing capability, but it requires even more intent and friction to invoke. I could see a scenario where Meta’s AI is inferior technically to OpenAI, but more useful simply because it comes in a better form factor.This is why I wasn’t surprised by this week’s final piece of AI news, first reported by The Information:\n  Jony Ive, the renowned designer of the iPhone, and OpenAI CEO Sam Altman have been discussing building a new AI hardware device, according to two people familiar with the conversations. SoftBank CEO and investor Masayoshi Son has talked to both about the idea, according to one of these people, but it is unclear if he will remain involved.\nThe Financial Times added more details:\n  Sam Altman, OpenAI’s chief, has tapped Ive’s company LoveFrom, which the designer founded when he left Apple in 2019, to develop the ChatGPT creator’s first consumer device, according to three people familiar with the plan. Altman and Ive have held brainstorming sessions at the designer’s San Francisco studio about what a new consumer product centred on OpenAI’s technology would look like, the people said. They hope to create a more natural and intuitive user experience for interacting with AI, in the way that the iPhone’s innovations in touchscreen computing unleashed the mass-market potential of the mobile internet. The process of identifying a design or device remains at an early stage with many different ideas on the table, they said.\n  Son, SoftBank’s founder and chief executive, has also been involved in some of the discussions, pitching a central role for Arm — the chip designer in which the Japanese conglomerate holds a 90 per cent stake — as well as offering financial backing. Son, Altman and Ive have discussed creating a company that would draw on talent and technology from their three groups, the people said, with SoftBank investing more than $1bn in the venture.\nThere are obviously many steps before a potential hardware product, including actually agreeing to build one. And there is, of course, the fact that Apple and Google already make devices everyone carries, with the latter in particular investing heavily in its own AI capabilities; betting on the hardware in market winning the hardware opportunity in AI is the safest bet.That may not be a reason for either OpenAI or Meta to abandon their efforts, though: waging a hardware battle against Google and Apple would be difficult, but it might be even worse to be “just an app” if the full realization of AI’s capabilities depend on fully removing human friction from the process."
        },
        {
          "subtitle": "Virtual Reality",
          "contentHtml": "<p>I should, I suppose, mention the Quest 3, which was formally announced at Meta’s event, given that I opened this Article with allusions to “Virtual Reality.” I have used a prototype Quest 3 device, but not a release version, and so can’t fully comment on its capabilities or the experience; what I will note is that the mixed reality gaming experiences were incredibly fun, particularly a Zombie shooter that is set in the room you are located in.</p><p>That’s the thing, though: Quest 3 still strikes me mostly as a console, while the <a href=\"https://stratechery.com/2023/apple-vision/\">Apple Vision</a> strikes me mostly as a productivity device. Both are interesting niches, but niches nonetheless. What seems essential for both to fully realize the vision of virtual reality is to lose their current sense of boundedness and friction, which is to say that <a href=\"https://stratechery.com/2022/dall-e-the-metaverse-and-zero-marginal-content/\">both need AI generation</a>.</p><p>In fact, I would argue that defining “virtual reality” to mean an immersive headset is to miss the point: virtual reality is a digital experience that has fully broken the bounds of human constraints, and in that experience the hardware is a means, not an end. Moreover, a virtual reality experience need not involve vision at all: talking with ChatGPT, for example, is an aural experience that feels more like virtual reality than the majority of experiences I’ve had in a headset.</p><p>True virtual reality shifts time like media, place like communications, and, crucially, does so with perfect availability and infinite capacity. In this view, virtual reality is AI, and AI is virtual reality. Hardware does matter — that has been the focus of this Article — but it matters as a means to an end, to enable an interactive experience without the constraints of human capacity or the friction of actual reality.</p><p><em>I wrote a follow-up to this Article in <a href=\"https://stratechery.com/2023/ai-and-ambient-computing-zuckerberg-and-the-verge-amazon-and-anthropic/\">this Daily Update</a>.</em></p><hr class=\"footnotes\"><ol class=\"footnotes\" style=\"list-style-type:decimal\"><li id=\"fn1-11652\"><p>The experience wasn’t perfect, both from a usability standpoint, and from the overly verbose answers, delivered in ChatGPT’s sterile style; you can see the seeds of something very compelling though&nbsp;<a href=\"#rf1-11652\" class=\"backlink\" title=\"Return to footnote 1.\">↩</a></p></li><li id=\"fn2-11652\"><p>And startups like <a href=\"https://beta.character.ai/\">Character.AI</a> are reportedly doing extremely well&nbsp;<a href=\"#rf2-11652\" class=\"backlink\" title=\"Return to footnote 2.\">↩</a></p></li></ol>",
          "contentText": "I should, I suppose, mention the Quest 3, which was formally announced at Meta’s event, given that I opened this Article with allusions to “Virtual Reality.” I have used a prototype Quest 3 device, but not a release version, and so can’t fully comment on its capabilities or the experience; what I will note is that the mixed reality gaming experiences were incredibly fun, particularly a Zombie shooter that is set in the room you are located in.That’s the thing, though: Quest 3 still strikes me mostly as a console, while the Apple Vision strikes me mostly as a productivity device. Both are interesting niches, but niches nonetheless. What seems essential for both to fully realize the vision of virtual reality is to lose their current sense of boundedness and friction, which is to say that both need AI generation.In fact, I would argue that defining “virtual reality” to mean an immersive headset is to miss the point: virtual reality is a digital experience that has fully broken the bounds of human constraints, and in that experience the hardware is a means, not an end. Moreover, a virtual reality experience need not involve vision at all: talking with ChatGPT, for example, is an aural experience that feels more like virtual reality than the majority of experiences I’ve had in a headset.True virtual reality shifts time like media, place like communications, and, crucially, does so with perfect availability and infinite capacity. In this view, virtual reality is AI, and AI is virtual reality. Hardware does matter — that has been the focus of this Article — but it matters as a means to an end, to enable an interactive experience without the constraints of human capacity or the friction of actual reality.I wrote a follow-up to this Article in this Daily Update.The experience wasn’t perfect, both from a usability standpoint, and from the overly verbose answers, delivered in ChatGPT’s sterile style; you can see the seeds of something very compelling though ↩And startups like Character.AI are reportedly doing extremely well ↩"
        }
      ]
    },
    {
      "title": "FTC Sues Amazon",
      "publishedDate": "2023-09-27T05:59:38-07:00",
      "updatedDate": "2023-10-31T05:00:38-07:00",
      "contentHtml": "\n\t\t<p>From the <a href=\"https://www.wsj.com/tech/ftc-sues-amazon-alleging-illegal-online-marketplace-monopoly-6bd9af23\">Wall Street Journal</a>:</p>\n<blockquote><p>\n  The Federal Trade Commission and 17 states on Tuesday sued Amazon, alleging the online retailer illegally wields monopoly power that keeps prices artificially high, locks sellers into its platform and harms its rivals. The FTC’s lawsuit, filed in Seattle federal court, marks a milestone in the Biden administration’s aggressive approach to enforcing antitrust laws and has been anticipated for months. The agency’s chair, Lina Khan, is a longtime critic of Amazon who wrote in the Yale Law Journal in 2017 that earlier generations of competition cops and courts abandoned the law’s concerns over conglomerates such as Amazon. Khan has had trouble convincing courts of her antitrust views, however. Having earlier lost cases against both Microsoft and Meta Platforms, she and her agency now face a crucial test in taking on Amazon.</p>\n<p>  The federal agency and the states alleged that Amazon violated antitrust laws by using anti-discounting measures that punished merchants for offering lower prices elsewhere. The government also said sellers on Amazon were compelled to use its logistics service if they want their goods to appear in Amazon Prime, the subscription program whose perks include faster shipping times. Such “tying,” the complaint says, illegally “restricts sellers’ choices” and “reduces product selection available to Amazon’s rivals.”</p>\n<p>  The FTC also said sellers feel they must use Amazon’s services such as advertising to be successful on the platform. Between being paid for its logistics program, advertising and other services, “Amazon now takes one of every $2 that a seller makes,” Khan said at a briefing with the media Tuesday.\n</p></blockquote>\n<p>This is the key paragraph of the <a href=\"https://www.ftc.gov/system/files/ftc_gov/pdf/1910129AmazoneCommerceComplaintPublic.pdf\">FTC’s (heavily redacted) complaint</a>:</p>\n<blockquote><p>\n  This case is about the illegal course of exclusionary conduct Amazon deploys to block competition, stunt rivals’ growth, and cement its dominance. The elements of this strategy are mutually reinforcing. Amazon uses a set of anti-discounting tactics to prevent rivals from growing by offering lower prices, and it uses coercive tactics involving its order fulfillment service to prevent rivals from gaining the scale they need to meaningfully compete. Amazon deploys this interconnected strategy to block off every major avenue of competition — including price, product selection, quality, and innovation — in the relevant markets for online superstores and online marketplace services.\n</p></blockquote>\n<p>I will, for the sake of space, focus on these two complaints; I will note, though, that the extreme suspicion with which things like a subscriptions-based loyalty program (Prime), bundling (Prime), store-branded goods (Amazon Basics et al.), and advertising are presented hardly does the FTC’s case any good. Characterizing practices that have been common tactics in retail for literally decades as some sort of nefarious plot makes me question this paragraph from the <a href=\"https://www.ftc.gov/news-events/news/press-releases/2023/09/ftc-sues-amazon-illegally-maintaining-monopoly-power\">press release</a>:</p>\n<blockquote><p>\n  The complaint alleges that Amazon violates the law not because it is big, but because it engages in a course of exclusionary conduct that prevents current competitors from growing and new competitors from emerging. By stifling competition on price, product selection, quality, and by preventing its current or future rivals from attracting a critical mass of shoppers and sellers, Amazon ensures that no current or future rival can threaten its dominance. Amazon’s far-reaching schemes impact hundreds of billions of dollars in retail sales every year, touch hundreds of thousands of products sold by businesses big and small and affect over a hundred million shoppers.\n</p></blockquote>\n<p>That first sentence in particular made me think of this meme:</p>\n<p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-3.png?resize=640%2C295&amp;ssl=1\" alt=\"A meme about the FTC claiming it isn't suing Amazon for being big\" width=\"640\" height=\"295\" class=\"aligncenter size-full wp-image-11640\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-3.png?w=1144&amp;ssl=1 1144w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-3.png?resize=300%2C138&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-3.png?resize=1024%2C473&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-3.png?resize=768%2C354&amp;ssl=1 768w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p>\n<p>Set that aside for now, though; I actually think at least one of the complaints is compelling, if not convincing.</p>\n<h3>FBA and Prime</h3>\n<p>This complaint is not the compelling one; from the complaint:</p>\n<blockquote><p>\n  Amazon deploys yet another tactic as part of its monopolistic course of conduct. Amazon conditions sellers’ ability to be “Prime eligible” on their use of Amazon’s order fulfillment service. As with Amazon’s anti-discounting tactics, this coercive conduct forecloses Amazon’s rivals from drawing a critical mass of sellers or shoppers – thereby depriving them of the scale needed to compete effectively online.</p>\n<p>  Amazon makes Prime eligibility critical for sellers to fully reach Amazon’s enormous base of shoppers. In 2021, more than ██% of all units sold on Amazon in the United States were Prime eligible. Prime eligibility is critical for sellers in part because of the enormous reach of Amazon’s Prime subscription program. According to public reports, Mr. Bezos told Amazon executives that Prime was created in 2005 to “draw a moat around [Amazon’s] best customers.” Prime now blankets more than ██% of all U.S. households, with its reach extending as far as ██% in some zip codes.</p>\n<p>  Amazon requires sellers who want their products to be Prime eligible to use Amazon’s fulfillment service, Fulfillment by Amazon (“FBA”), even though many sellers would rather use an alternative fulfillment method to store and package customer orders.\n</p></blockquote>\n<p>I find this charge ridiculous on its face. The core offering of Prime — the feature that it launched with 18 years ago — was a shipping guarantee. From the <a href=\"https://press.aboutamazon.com/2005/2/amazon-com-announces-record-free-cash-flow-fueled-by-lower-prices-and-free-shipping-introduces-new-express-shipping-program-amazon-prime\">February 2005 press release</a>:</p>\n<blockquote><p>\n  Today the Company also introduced “Amazon Prime,” Amazon.com’s first ever membership program. For a flat membership fee of $79 per year, members get unlimited, express two-day shipping for free, with no minimum purchase requirement. Members also get one-day, overnight shipping for only $3.99 per item — order as late as 6:30PM ET.</p>\n<p>  “Amazon Prime is ‘all-you-can-eat’ express shipping,” said Jeff Bezos, founder and CEO of Amazon.com. “Though expensive for the Company in the short-term, it’s a significant benefit and more convenient for customers. With Amazon Prime, there’s no minimum purchase to think about, and no consolidating orders — two-day shipping becomes an everyday experience rather than an occasional indulgence.”\n</p></blockquote>\n<p>It seems eminently reasonable to me that Amazon predicate inclusion in a program defined by a shipping guarantee on letting Amazon deliver your products. Prime was a massive risk at the time, dwarfed only by the many billions of dollars that Amazon has spent since then building out its logistics network. I see no basis on which a government regulator ought to demand that Amazon give out access to the Prime label and bear the reputation risk for 3rd-party delivery services that did not take those risks or make those investments. It’s absurd.</p>\n<p>The FTC’s argument seems to be mostly based on the existence of an Amazon program called “Seller-Fulfilled Prime” that launched in 2015, before enrollment was shuttered in 2019, and suspended in 2020; Amazon <a href=\"https://sellercentral.amazon.com/seller-forums/discussions/t/b84ba574c99c39b7cda6566fddfe8375\">announced it was coming back in 2023</a> (perhaps because of this case). Seller-Fulfilled Prime let sellers participate in the Prime program, as long as they delivered the goods themselves (i.e. didn’t use a 3rd-party fulfillment service) and passed Amazon’s stringent requirements. The FTC, based on internal emails (which are redacted), claims that Amazon killed the program because it reduced the company’s hold on merchants. A few points on this:</p>\n<ul>\n<li>First, Prime is Amazon’s brand and program; just because Amazon opened it up once doesn’t mean it ought be compelled to keep it open.</li>\n<li>Second, this charge definitely feels downstream from a fishing expedition; I imagine those redacted emails are pretty spicy, because it’s hard to see any justification for this charge otherwise.</li>\n<li>Three, look carefully at those dates: in 2019 <a href=\"https://stratechery.com/2019/day-two-to-one-day/\">Amazon announced that Prime would shift to a one-day guarantee</a>, and in 2020 Amazon was in the middle of saving the country during lockdowns. Given the reputational risk attached to Prime those seem like relevant reasons to suspend the program.</li>\n</ul>\n<p>Ultimately, though, these arguments pale in comparison to the sheer audacity of the FTC’s insistence it ought to be able to simply take what Amazon has built and distribute it to whoever wants it.</p>\n<h3>Pricing Punishment</h3>\n<p>This charge is more compelling; from the complaint:</p>\n<blockquote><p>\n  One set of tactics stifles the ability of rivals to attract shoppers by offering lower prices. Amazon deploys a sophisticated surveillance network of web crawlers that constantly monitor the internet, searching for discounts that might threaten Amazon’s empire. When Amazon detects elsewhere online a product that is cheaper than a seller’s offer for the same product on Amazon, Amazon punishes that seller. It does so to prevent rivals from gaining business by offering shoppers or sellers lower prices…</p>\n<p>  The sanctions Amazon levies on sellers vary. For example, Amazon knocks these sellers out of the all-important “Buy Box,” the display from which a shopper can “Add to Cart” or “Buy Now” an Amazon-selected offer for a product. Nearly ██% of Amazon sales are made through the Buy Box and, as Amazon internally recognizes, eliminating a seller from the Buy Box causes that seller’s sales to “tank.” Another form of punishment is to bury discounting sellers so far down in Amazon’s search results that they become effectively invisible…</p>\n<p>  Moreover, Amazon’s one-two punch of seller punishments and high seller fees often forces sellers to use their inflated Amazon prices as a price floor everywhere else. As a result, Amazon’s conduct causes online shoppers to face artificially higher prices even when shopping somewhere other than Amazon. Amazon’s punitive regime distorts basic market signals: one of the ways sellers respond to Amazon’s fee hikes is by increasing their own prices off Amazon. An executive from another online retailer sums up this perverse dynamic: Amazon’s anti-discounting conduct █████████████████████████████████. Amazon’s illegal tactics mean that when Amazon raises its fees, others — competitors, sellers, and shoppers – suffer the harms.</p>\n<p>  Amazon’s tactics suppress rival online superstores’ ability to compete for shoppers by offering lower prices, thereby depriving American households of more affordable options. Amazon’s conduct also suppresses rival online marketplace service providers’ ability to compete for sellers by offering lower fees because sellers cannot pass along those savings to shoppers in the form of lower product prices.\n</p></blockquote>\n<p>This all sounds bad and, at first glance, anti-competitive. Consider, though, what the FTC is implicitly asking:</p>\n<ul>\n<li>First, Amazon is <em>replacing</em> the offending merchants in the Buy Box with other merchants who offer lower prices (including, potentially, Amazon itself). It’s difficult to understand how this is bad for consumers.</li>\n<li>Second, insisting that Amazon promote merchants who offer higher prices on Amazon and lower prices elsewhere is, once again, insisting that Amazon offer the fruits of its investments, both in terms of customer acquisition and in delivery speed, to merchants who are actively seeking to develop an Amazon competitor.</li>\n<li>Third, most-favored nation clauses, which this is in practice if not in specifics (in fact, according to the FTC, these practices replaced MFN clauses) have consistently been found to be legal.</li>\n</ul>\n<p>Most importantly, though, this alleged illegality rests on Amazon being a monopoly, which means, as happens with all antitrust cases, we have a question of market definition. In this case the FTC has defined the relevant markets as “the online superstore market” and “the market for online marketplace services.” These definitions, conveniently enough, exclude all brick-and-mortar retailers (the word “omnichannel” doesn’t appear in the complaint), and all independent retailers, such as those hosted by Shopify. The FTC says that this narrow definition makes sense because of the convenience and selection that is exclusive to “online superstores”, thanks to the ability to ship things together; never-mind that other websites are only a click away, and that the entire reason you can ship things together is because most items on Amazon are Fulfilled by Amazon (see the previous complaint).</p>\n<p>This definition is obviously going to be critical to this case: <a href=\"https://www.ben-evans.com/benedictevans/2019/12/amazons-market-share19\">Benedict Evans ran the numbers</a> and, if you consider all of retail, then Amazon only has single-digits worth of marketshare; if you consider all of e-commerce Amazon has about 35% share. What is clear is that just about everything on Amazon is available elsewhere: the power the company has with regard to pricing is a function of the demand it delivers to merchants — demand that is not compelled of customers, but willingly given precisely because customers find Amazon’s service to be valuable.</p>\n<h3>Amazon’s Market-Making</h3>\n<p>That’s not to say that there aren’t merchants wholly beholden to Amazon; in 2019 an Amazon reseller named Molson Hart <a href=\"https://medium.com/@molson_hart/amazon-needs-a-competitor-and-walmart-aint-it-5997977b77b2\">wrote an essay on Medium</a> complaining about Amazon’s fulfillment prices, and included this chart:</p>\n<blockquote><p>\n  We sell plush and construction toys on Amazon. Well, technically, we sell toys on our website, on eBay, on Walmart.com, to brick-and-mortar stores, and we sell on Amazon. But, really, we only sell on Amazon. In 2018, we had about $4,000,000 in sales but Amazon.com accounted for over 98% of that.</p>\n<p>  <a href=\"https://medium.com/@molson_hart/amazon-needs-a-competitor-and-walmart-aint-it-5997977b77b2\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-1.png?resize=640%2C395&amp;ssl=1\" alt=\"How Amazon dominates the sale of one merchant\" width=\"640\" height=\"395\" class=\"aligncenter size-full wp-image-11638\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-1.png?w=727&amp;ssl=1 727w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-1.png?resize=300%2C185&amp;ssl=1 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>  Harvard Business School would call this “vendor/customer concentration”. In the e-commerce world, we call it being Amazon’s bitch.</p>\n<p>  While Amazon received $1.95 million from us last year, they are not afraid of losing our business for a couple of reasons. First, there are thousands of companies out there eager to take our place. Second, Amazon had $277 billion in gross merchandise revenue in 2018. Our $3.9 million in sales on Amazon accounted for .0014% of that. Finally, we have nowhere else to go and Amazon knows it.\n</p></blockquote>\n<p>Hart <a href=\"https://www.youtube.com/watch?v=Pan9eNpVqVY\">shared his entrepreneurship story on a podcast</a>, and I think it provides important context:</p>\n<div align=\"center\"><div id=\"v-g4OK1LSH-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/g4OK1LSH?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div>\n<blockquote><p>\n  In 2014-2015, you could sell literally anything you wanted on Amazon and it was profitable. You didn’t really need to do any data analysis. If you were buying in China and you didn’t do an absolutely abominable job sending it to Amazon, you were making money. When you’re selling into retail it’s different. On Amazon you could just sell commodities in 2014-2015 — literally a towel, you didn’t have to have a brand or anything — but to sell into retail, which is a developed market, I can’t call into Walmart and be like, “Hey, I’m this twenty-six year old kid and I’m going to sell you towels.” They’re like “No, we’re going to buy from branded manufacturers of towels and factories for towels and people who have an established business, some credibility in this industry.” So if we wanted to sell into retail we had to bring innovative products to the table, otherwise there was no incentive for them to take the risk on a young company with a young founder, etc.</p>\n<p>  So I figured out at one point, “Maybe instead of coming up with all of these innovative products, because innovation was really hard, maybe we could find stuff that is already popular in China or Japan or Korea and just bring it to America and re-brand it.” So then what I did is I just went onto Taobao — China’s Amazon — and went through tens of thousands of products. I looked at the top sellers in each category, toys, games, bikes, sporting goods, all that stuff, and anytime I saw a product that was selling really well in China that I had never seen before I put it into a bucket and said, “OK, we’re going to launch this in America.” So that’s what we did and by-and-large was a pretty effective strategy. That’s actually how Brain Flakes was born.\n</p></blockquote>\n<p>Brain Flakes is <a href=\"https://viahart.com/\">Hart’s biggest product</a>, and the biggest driver of that Amazon-dominated sales chart up above. The question I have with regards to that chart, though, and Hart’s griping about Amazon’s fees, is hasn’t Amazon earned the right to charge Hart whatever it deems appropriate? Hart himself admits that his entire business was predicated on Amazon’s marketplace model, a model that enabled individual entrepreneurs with smarts and hustle to build big businesses without a reputation or a brand. To put it another way, Hart’s business is dominated by Amazon <em>because Amazon made his entire business possible</em> (and if these complaints sound familiar, they echo <a href=\"https://stratechery.com/2020/email-addresses-and-razor-blades/\">complaints about Facebook from companies built on Facebook</a>, <a href=\"https://stratechery.com/2018/the-bill-gates-line/\">Yelp’s complaints that they have to acquire customers instead of relying on SEO</a>, or <a href=\"https://stratechery.com/2014/economic-power-age-abundance/\">publishers the world over</a> blaming their commodity status on the same companies that made the market for them in the first place).</p>\n<p>I am by no means here to pick on Hart or any of the millions of other 3rd-party merchants on Amazon: I salute their entrepreneurial grit. I fail, though, to see what exactly is anticompetitive in this story. What I see, much like the Prime program above, is massive investment by Amazon to create an entire category that dramatically increased the amount of commerce, and it’s unclear to me why they can’t conduct normal business activity to ensure they have competitive prices in that market.</p>\n<p>That noted, the reason I find this part of the complaint compelling is that I do have unease about the use of enforced price matching and other non-organic means of limiting competition; that, along with acquisitions and digital advertising, was one of the three areas of concern I highlighted in <a href=\"https://stratechery.com/2019/where-warrens-wrong/\">a 2019 Article</a> that explained what antitrust crusaders fail to understand about Aggregators who gain market power not through controlling supply but rather by harnessing demand. The issue, though, <a href=\"https://stratechery.com/2019/a-framework-for-regulating-competition-on-the-internet/\">is that these concerns ought be addressed through new laws</a>; trying to apply antitrust regulations that were created for the analog world in a digital context simply doesn’t make sense, and very likely will, like so many other recent FTC actions, fail in court.</p>\n\n\t\t\t",
      "contentText": "\n\t\tFrom the Wall Street Journal:\n\n  The Federal Trade Commission and 17 states on Tuesday sued Amazon, alleging the online retailer illegally wields monopoly power that keeps prices artificially high, locks sellers into its platform and harms its rivals. The FTC’s lawsuit, filed in Seattle federal court, marks a milestone in the Biden administration’s aggressive approach to enforcing antitrust laws and has been anticipated for months. The agency’s chair, Lina Khan, is a longtime critic of Amazon who wrote in the Yale Law Journal in 2017 that earlier generations of competition cops and courts abandoned the law’s concerns over conglomerates such as Amazon. Khan has had trouble convincing courts of her antitrust views, however. Having earlier lost cases against both Microsoft and Meta Platforms, she and her agency now face a crucial test in taking on Amazon.\n  The federal agency and the states alleged that Amazon violated antitrust laws by using anti-discounting measures that punished merchants for offering lower prices elsewhere. The government also said sellers on Amazon were compelled to use its logistics service if they want their goods to appear in Amazon Prime, the subscription program whose perks include faster shipping times. Such “tying,” the complaint says, illegally “restricts sellers’ choices” and “reduces product selection available to Amazon’s rivals.”\n  The FTC also said sellers feel they must use Amazon’s services such as advertising to be successful on the platform. Between being paid for its logistics program, advertising and other services, “Amazon now takes one of every $2 that a seller makes,” Khan said at a briefing with the media Tuesday.\n\nThis is the key paragraph of the FTC’s (heavily redacted) complaint:\n\n  This case is about the illegal course of exclusionary conduct Amazon deploys to block competition, stunt rivals’ growth, and cement its dominance. The elements of this strategy are mutually reinforcing. Amazon uses a set of anti-discounting tactics to prevent rivals from growing by offering lower prices, and it uses coercive tactics involving its order fulfillment service to prevent rivals from gaining the scale they need to meaningfully compete. Amazon deploys this interconnected strategy to block off every major avenue of competition — including price, product selection, quality, and innovation — in the relevant markets for online superstores and online marketplace services.\n\nI will, for the sake of space, focus on these two complaints; I will note, though, that the extreme suspicion with which things like a subscriptions-based loyalty program (Prime), bundling (Prime), store-branded goods (Amazon Basics et al.), and advertising are presented hardly does the FTC’s case any good. Characterizing practices that have been common tactics in retail for literally decades as some sort of nefarious plot makes me question this paragraph from the press release:\n\n  The complaint alleges that Amazon violates the law not because it is big, but because it engages in a course of exclusionary conduct that prevents current competitors from growing and new competitors from emerging. By stifling competition on price, product selection, quality, and by preventing its current or future rivals from attracting a critical mass of shoppers and sellers, Amazon ensures that no current or future rival can threaten its dominance. Amazon’s far-reaching schemes impact hundreds of billions of dollars in retail sales every year, touch hundreds of thousands of products sold by businesses big and small and affect over a hundred million shoppers.\n\nThat first sentence in particular made me think of this meme:\n\nSet that aside for now, though; I actually think at least one of the complaints is compelling, if not convincing.\nFBA and Prime\nThis complaint is not the compelling one; from the complaint:\n\n  Amazon deploys yet another tactic as part of its monopolistic course of conduct. Amazon conditions sellers’ ability to be “Prime eligible” on their use of Amazon’s order fulfillment service. As with Amazon’s anti-discounting tactics, this coercive conduct forecloses Amazon’s rivals from drawing a critical mass of sellers or shoppers – thereby depriving them of the scale needed to compete effectively online.\n  Amazon makes Prime eligibility critical for sellers to fully reach Amazon’s enormous base of shoppers. In 2021, more than ██% of all units sold on Amazon in the United States were Prime eligible. Prime eligibility is critical for sellers in part because of the enormous reach of Amazon’s Prime subscription program. According to public reports, Mr. Bezos told Amazon executives that Prime was created in 2005 to “draw a moat around [Amazon’s] best customers.” Prime now blankets more than ██% of all U.S. households, with its reach extending as far as ██% in some zip codes.\n  Amazon requires sellers who want their products to be Prime eligible to use Amazon’s fulfillment service, Fulfillment by Amazon (“FBA”), even though many sellers would rather use an alternative fulfillment method to store and package customer orders.\n\nI find this charge ridiculous on its face. The core offering of Prime — the feature that it launched with 18 years ago — was a shipping guarantee. From the February 2005 press release:\n\n  Today the Company also introduced “Amazon Prime,” Amazon.com’s first ever membership program. For a flat membership fee of $79 per year, members get unlimited, express two-day shipping for free, with no minimum purchase requirement. Members also get one-day, overnight shipping for only $3.99 per item — order as late as 6:30PM ET.\n  “Amazon Prime is ‘all-you-can-eat’ express shipping,” said Jeff Bezos, founder and CEO of Amazon.com. “Though expensive for the Company in the short-term, it’s a significant benefit and more convenient for customers. With Amazon Prime, there’s no minimum purchase to think about, and no consolidating orders — two-day shipping becomes an everyday experience rather than an occasional indulgence.”\n\nIt seems eminently reasonable to me that Amazon predicate inclusion in a program defined by a shipping guarantee on letting Amazon deliver your products. Prime was a massive risk at the time, dwarfed only by the many billions of dollars that Amazon has spent since then building out its logistics network. I see no basis on which a government regulator ought to demand that Amazon give out access to the Prime label and bear the reputation risk for 3rd-party delivery services that did not take those risks or make those investments. It’s absurd.\nThe FTC’s argument seems to be mostly based on the existence of an Amazon program called “Seller-Fulfilled Prime” that launched in 2015, before enrollment was shuttered in 2019, and suspended in 2020; Amazon announced it was coming back in 2023 (perhaps because of this case). Seller-Fulfilled Prime let sellers participate in the Prime program, as long as they delivered the goods themselves (i.e. didn’t use a 3rd-party fulfillment service) and passed Amazon’s stringent requirements. The FTC, based on internal emails (which are redacted), claims that Amazon killed the program because it reduced the company’s hold on merchants. A few points on this:\n\nFirst, Prime is Amazon’s brand and program; just because Amazon opened it up once doesn’t mean it ought be compelled to keep it open.\nSecond, this charge definitely feels downstream from a fishing expedition; I imagine those redacted emails are pretty spicy, because it’s hard to see any justification for this charge otherwise.\nThree, look carefully at those dates: in 2019 Amazon announced that Prime would shift to a one-day guarantee, and in 2020 Amazon was in the middle of saving the country during lockdowns. Given the reputational risk attached to Prime those seem like relevant reasons to suspend the program.\n\nUltimately, though, these arguments pale in comparison to the sheer audacity of the FTC’s insistence it ought to be able to simply take what Amazon has built and distribute it to whoever wants it.\nPricing Punishment\nThis charge is more compelling; from the complaint:\n\n  One set of tactics stifles the ability of rivals to attract shoppers by offering lower prices. Amazon deploys a sophisticated surveillance network of web crawlers that constantly monitor the internet, searching for discounts that might threaten Amazon’s empire. When Amazon detects elsewhere online a product that is cheaper than a seller’s offer for the same product on Amazon, Amazon punishes that seller. It does so to prevent rivals from gaining business by offering shoppers or sellers lower prices…\n  The sanctions Amazon levies on sellers vary. For example, Amazon knocks these sellers out of the all-important “Buy Box,” the display from which a shopper can “Add to Cart” or “Buy Now” an Amazon-selected offer for a product. Nearly ██% of Amazon sales are made through the Buy Box and, as Amazon internally recognizes, eliminating a seller from the Buy Box causes that seller’s sales to “tank.” Another form of punishment is to bury discounting sellers so far down in Amazon’s search results that they become effectively invisible…\n  Moreover, Amazon’s one-two punch of seller punishments and high seller fees often forces sellers to use their inflated Amazon prices as a price floor everywhere else. As a result, Amazon’s conduct causes online shoppers to face artificially higher prices even when shopping somewhere other than Amazon. Amazon’s punitive regime distorts basic market signals: one of the ways sellers respond to Amazon’s fee hikes is by increasing their own prices off Amazon. An executive from another online retailer sums up this perverse dynamic: Amazon’s anti-discounting conduct █████████████████████████████████. Amazon’s illegal tactics mean that when Amazon raises its fees, others — competitors, sellers, and shoppers – suffer the harms.\n  Amazon’s tactics suppress rival online superstores’ ability to compete for shoppers by offering lower prices, thereby depriving American households of more affordable options. Amazon’s conduct also suppresses rival online marketplace service providers’ ability to compete for sellers by offering lower fees because sellers cannot pass along those savings to shoppers in the form of lower product prices.\n\nThis all sounds bad and, at first glance, anti-competitive. Consider, though, what the FTC is implicitly asking:\n\nFirst, Amazon is replacing the offending merchants in the Buy Box with other merchants who offer lower prices (including, potentially, Amazon itself). It’s difficult to understand how this is bad for consumers.\nSecond, insisting that Amazon promote merchants who offer higher prices on Amazon and lower prices elsewhere is, once again, insisting that Amazon offer the fruits of its investments, both in terms of customer acquisition and in delivery speed, to merchants who are actively seeking to develop an Amazon competitor.\nThird, most-favored nation clauses, which this is in practice if not in specifics (in fact, according to the FTC, these practices replaced MFN clauses) have consistently been found to be legal.\n\nMost importantly, though, this alleged illegality rests on Amazon being a monopoly, which means, as happens with all antitrust cases, we have a question of market definition. In this case the FTC has defined the relevant markets as “the online superstore market” and “the market for online marketplace services.” These definitions, conveniently enough, exclude all brick-and-mortar retailers (the word “omnichannel” doesn’t appear in the complaint), and all independent retailers, such as those hosted by Shopify. The FTC says that this narrow definition makes sense because of the convenience and selection that is exclusive to “online superstores”, thanks to the ability to ship things together; never-mind that other websites are only a click away, and that the entire reason you can ship things together is because most items on Amazon are Fulfilled by Amazon (see the previous complaint).\nThis definition is obviously going to be critical to this case: Benedict Evans ran the numbers and, if you consider all of retail, then Amazon only has single-digits worth of marketshare; if you consider all of e-commerce Amazon has about 35% share. What is clear is that just about everything on Amazon is available elsewhere: the power the company has with regard to pricing is a function of the demand it delivers to merchants — demand that is not compelled of customers, but willingly given precisely because customers find Amazon’s service to be valuable.\nAmazon’s Market-Making\nThat’s not to say that there aren’t merchants wholly beholden to Amazon; in 2019 an Amazon reseller named Molson Hart wrote an essay on Medium complaining about Amazon’s fulfillment prices, and included this chart:\n\n  We sell plush and construction toys on Amazon. Well, technically, we sell toys on our website, on eBay, on Walmart.com, to brick-and-mortar stores, and we sell on Amazon. But, really, we only sell on Amazon. In 2018, we had about $4,000,000 in sales but Amazon.com accounted for over 98% of that.\n  \n  Harvard Business School would call this “vendor/customer concentration”. In the e-commerce world, we call it being Amazon’s bitch.\n  While Amazon received $1.95 million from us last year, they are not afraid of losing our business for a couple of reasons. First, there are thousands of companies out there eager to take our place. Second, Amazon had $277 billion in gross merchandise revenue in 2018. Our $3.9 million in sales on Amazon accounted for .0014% of that. Finally, we have nowhere else to go and Amazon knows it.\n\nHart shared his entrepreneurship story on a podcast, and I think it provides important context:\n\n\n  In 2014-2015, you could sell literally anything you wanted on Amazon and it was profitable. You didn’t really need to do any data analysis. If you were buying in China and you didn’t do an absolutely abominable job sending it to Amazon, you were making money. When you’re selling into retail it’s different. On Amazon you could just sell commodities in 2014-2015 — literally a towel, you didn’t have to have a brand or anything — but to sell into retail, which is a developed market, I can’t call into Walmart and be like, “Hey, I’m this twenty-six year old kid and I’m going to sell you towels.” They’re like “No, we’re going to buy from branded manufacturers of towels and factories for towels and people who have an established business, some credibility in this industry.” So if we wanted to sell into retail we had to bring innovative products to the table, otherwise there was no incentive for them to take the risk on a young company with a young founder, etc.\n  So I figured out at one point, “Maybe instead of coming up with all of these innovative products, because innovation was really hard, maybe we could find stuff that is already popular in China or Japan or Korea and just bring it to America and re-brand it.” So then what I did is I just went onto Taobao — China’s Amazon — and went through tens of thousands of products. I looked at the top sellers in each category, toys, games, bikes, sporting goods, all that stuff, and anytime I saw a product that was selling really well in China that I had never seen before I put it into a bucket and said, “OK, we’re going to launch this in America.” So that’s what we did and by-and-large was a pretty effective strategy. That’s actually how Brain Flakes was born.\n\nBrain Flakes is Hart’s biggest product, and the biggest driver of that Amazon-dominated sales chart up above. The question I have with regards to that chart, though, and Hart’s griping about Amazon’s fees, is hasn’t Amazon earned the right to charge Hart whatever it deems appropriate? Hart himself admits that his entire business was predicated on Amazon’s marketplace model, a model that enabled individual entrepreneurs with smarts and hustle to build big businesses without a reputation or a brand. To put it another way, Hart’s business is dominated by Amazon because Amazon made his entire business possible (and if these complaints sound familiar, they echo complaints about Facebook from companies built on Facebook, Yelp’s complaints that they have to acquire customers instead of relying on SEO, or publishers the world over blaming their commodity status on the same companies that made the market for them in the first place).\nI am by no means here to pick on Hart or any of the millions of other 3rd-party merchants on Amazon: I salute their entrepreneurial grit. I fail, though, to see what exactly is anticompetitive in this story. What I see, much like the Prime program above, is massive investment by Amazon to create an entire category that dramatically increased the amount of commerce, and it’s unclear to me why they can’t conduct normal business activity to ensure they have competitive prices in that market.\nThat noted, the reason I find this part of the complaint compelling is that I do have unease about the use of enforced price matching and other non-organic means of limiting competition; that, along with acquisitions and digital advertising, was one of the three areas of concern I highlighted in a 2019 Article that explained what antitrust crusaders fail to understand about Aggregators who gain market power not through controlling supply but rather by harnessing demand. The issue, though, is that these concerns ought be addressed through new laws; trying to apply antitrust regulations that were created for the analog world in a digital context simply doesn’t make sense, and very likely will, like so many other recent FTC actions, fail in court.\n\n\t\t\t",
      "subsections": [
        {
          "subtitle": "FBA and Prime",
          "contentHtml": "<p>This complaint is not the compelling one; from the complaint:</p><blockquote><p>\n  Amazon deploys yet another tactic as part of its monopolistic course of conduct. Amazon conditions sellers’ ability to be “Prime eligible” on their use of Amazon’s order fulfillment service. As with Amazon’s anti-discounting tactics, this coercive conduct forecloses Amazon’s rivals from drawing a critical mass of sellers or shoppers – thereby depriving them of the scale needed to compete effectively online.</p>\n<p>  Amazon makes Prime eligibility critical for sellers to fully reach Amazon’s enormous base of shoppers. In 2021, more than ██% of all units sold on Amazon in the United States were Prime eligible. Prime eligibility is critical for sellers in part because of the enormous reach of Amazon’s Prime subscription program. According to public reports, Mr. Bezos told Amazon executives that Prime was created in 2005 to “draw a moat around [Amazon’s] best customers.” Prime now blankets more than ██% of all U.S. households, with its reach extending as far as ██% in some zip codes.</p>\n<p>  Amazon requires sellers who want their products to be Prime eligible to use Amazon’s fulfillment service, Fulfillment by Amazon (“FBA”), even though many sellers would rather use an alternative fulfillment method to store and package customer orders.\n</p></blockquote><p>I find this charge ridiculous on its face. The core offering of Prime — the feature that it launched with 18 years ago — was a shipping guarantee. From the <a href=\"https://press.aboutamazon.com/2005/2/amazon-com-announces-record-free-cash-flow-fueled-by-lower-prices-and-free-shipping-introduces-new-express-shipping-program-amazon-prime\">February 2005 press release</a>:</p><blockquote><p>\n  Today the Company also introduced “Amazon Prime,” Amazon.com’s first ever membership program. For a flat membership fee of $79 per year, members get unlimited, express two-day shipping for free, with no minimum purchase requirement. Members also get one-day, overnight shipping for only $3.99 per item — order as late as 6:30PM ET.</p>\n<p>  “Amazon Prime is ‘all-you-can-eat’ express shipping,” said Jeff Bezos, founder and CEO of Amazon.com. “Though expensive for the Company in the short-term, it’s a significant benefit and more convenient for customers. With Amazon Prime, there’s no minimum purchase to think about, and no consolidating orders — two-day shipping becomes an everyday experience rather than an occasional indulgence.”\n</p></blockquote><p>It seems eminently reasonable to me that Amazon predicate inclusion in a program defined by a shipping guarantee on letting Amazon deliver your products. Prime was a massive risk at the time, dwarfed only by the many billions of dollars that Amazon has spent since then building out its logistics network. I see no basis on which a government regulator ought to demand that Amazon give out access to the Prime label and bear the reputation risk for 3rd-party delivery services that did not take those risks or make those investments. It’s absurd.</p><p>The FTC’s argument seems to be mostly based on the existence of an Amazon program called “Seller-Fulfilled Prime” that launched in 2015, before enrollment was shuttered in 2019, and suspended in 2020; Amazon <a href=\"https://sellercentral.amazon.com/seller-forums/discussions/t/b84ba574c99c39b7cda6566fddfe8375\">announced it was coming back in 2023</a> (perhaps because of this case). Seller-Fulfilled Prime let sellers participate in the Prime program, as long as they delivered the goods themselves (i.e. didn’t use a 3rd-party fulfillment service) and passed Amazon’s stringent requirements. The FTC, based on internal emails (which are redacted), claims that Amazon killed the program because it reduced the company’s hold on merchants. A few points on this:</p><ul>\n<li>First, Prime is Amazon’s brand and program; just because Amazon opened it up once doesn’t mean it ought be compelled to keep it open.</li>\n<li>Second, this charge definitely feels downstream from a fishing expedition; I imagine those redacted emails are pretty spicy, because it’s hard to see any justification for this charge otherwise.</li>\n<li>Three, look carefully at those dates: in 2019 <a href=\"https://stratechery.com/2019/day-two-to-one-day/\">Amazon announced that Prime would shift to a one-day guarantee</a>, and in 2020 Amazon was in the middle of saving the country during lockdowns. Given the reputational risk attached to Prime those seem like relevant reasons to suspend the program.</li>\n</ul><p>Ultimately, though, these arguments pale in comparison to the sheer audacity of the FTC’s insistence it ought to be able to simply take what Amazon has built and distribute it to whoever wants it.</p>",
          "contentText": "This complaint is not the compelling one; from the complaint:\n  Amazon deploys yet another tactic as part of its monopolistic course of conduct. Amazon conditions sellers’ ability to be “Prime eligible” on their use of Amazon’s order fulfillment service. As with Amazon’s anti-discounting tactics, this coercive conduct forecloses Amazon’s rivals from drawing a critical mass of sellers or shoppers – thereby depriving them of the scale needed to compete effectively online.\n  Amazon makes Prime eligibility critical for sellers to fully reach Amazon’s enormous base of shoppers. In 2021, more than ██% of all units sold on Amazon in the United States were Prime eligible. Prime eligibility is critical for sellers in part because of the enormous reach of Amazon’s Prime subscription program. According to public reports, Mr. Bezos told Amazon executives that Prime was created in 2005 to “draw a moat around [Amazon’s] best customers.” Prime now blankets more than ██% of all U.S. households, with its reach extending as far as ██% in some zip codes.\n  Amazon requires sellers who want their products to be Prime eligible to use Amazon’s fulfillment service, Fulfillment by Amazon (“FBA”), even though many sellers would rather use an alternative fulfillment method to store and package customer orders.\nI find this charge ridiculous on its face. The core offering of Prime — the feature that it launched with 18 years ago — was a shipping guarantee. From the February 2005 press release:\n  Today the Company also introduced “Amazon Prime,” Amazon.com’s first ever membership program. For a flat membership fee of $79 per year, members get unlimited, express two-day shipping for free, with no minimum purchase requirement. Members also get one-day, overnight shipping for only $3.99 per item — order as late as 6:30PM ET.\n  “Amazon Prime is ‘all-you-can-eat’ express shipping,” said Jeff Bezos, founder and CEO of Amazon.com. “Though expensive for the Company in the short-term, it’s a significant benefit and more convenient for customers. With Amazon Prime, there’s no minimum purchase to think about, and no consolidating orders — two-day shipping becomes an everyday experience rather than an occasional indulgence.”\nIt seems eminently reasonable to me that Amazon predicate inclusion in a program defined by a shipping guarantee on letting Amazon deliver your products. Prime was a massive risk at the time, dwarfed only by the many billions of dollars that Amazon has spent since then building out its logistics network. I see no basis on which a government regulator ought to demand that Amazon give out access to the Prime label and bear the reputation risk for 3rd-party delivery services that did not take those risks or make those investments. It’s absurd.The FTC’s argument seems to be mostly based on the existence of an Amazon program called “Seller-Fulfilled Prime” that launched in 2015, before enrollment was shuttered in 2019, and suspended in 2020; Amazon announced it was coming back in 2023 (perhaps because of this case). Seller-Fulfilled Prime let sellers participate in the Prime program, as long as they delivered the goods themselves (i.e. didn’t use a 3rd-party fulfillment service) and passed Amazon’s stringent requirements. The FTC, based on internal emails (which are redacted), claims that Amazon killed the program because it reduced the company’s hold on merchants. A few points on this:\nFirst, Prime is Amazon’s brand and program; just because Amazon opened it up once doesn’t mean it ought be compelled to keep it open.\nSecond, this charge definitely feels downstream from a fishing expedition; I imagine those redacted emails are pretty spicy, because it’s hard to see any justification for this charge otherwise.\nThree, look carefully at those dates: in 2019 Amazon announced that Prime would shift to a one-day guarantee, and in 2020 Amazon was in the middle of saving the country during lockdowns. Given the reputational risk attached to Prime those seem like relevant reasons to suspend the program.\nUltimately, though, these arguments pale in comparison to the sheer audacity of the FTC’s insistence it ought to be able to simply take what Amazon has built and distribute it to whoever wants it."
        },
        {
          "subtitle": "Pricing Punishment",
          "contentHtml": "<p>This charge is more compelling; from the complaint:</p><blockquote><p>\n  One set of tactics stifles the ability of rivals to attract shoppers by offering lower prices. Amazon deploys a sophisticated surveillance network of web crawlers that constantly monitor the internet, searching for discounts that might threaten Amazon’s empire. When Amazon detects elsewhere online a product that is cheaper than a seller’s offer for the same product on Amazon, Amazon punishes that seller. It does so to prevent rivals from gaining business by offering shoppers or sellers lower prices…</p>\n<p>  The sanctions Amazon levies on sellers vary. For example, Amazon knocks these sellers out of the all-important “Buy Box,” the display from which a shopper can “Add to Cart” or “Buy Now” an Amazon-selected offer for a product. Nearly ██% of Amazon sales are made through the Buy Box and, as Amazon internally recognizes, eliminating a seller from the Buy Box causes that seller’s sales to “tank.” Another form of punishment is to bury discounting sellers so far down in Amazon’s search results that they become effectively invisible…</p>\n<p>  Moreover, Amazon’s one-two punch of seller punishments and high seller fees often forces sellers to use their inflated Amazon prices as a price floor everywhere else. As a result, Amazon’s conduct causes online shoppers to face artificially higher prices even when shopping somewhere other than Amazon. Amazon’s punitive regime distorts basic market signals: one of the ways sellers respond to Amazon’s fee hikes is by increasing their own prices off Amazon. An executive from another online retailer sums up this perverse dynamic: Amazon’s anti-discounting conduct █████████████████████████████████. Amazon’s illegal tactics mean that when Amazon raises its fees, others — competitors, sellers, and shoppers – suffer the harms.</p>\n<p>  Amazon’s tactics suppress rival online superstores’ ability to compete for shoppers by offering lower prices, thereby depriving American households of more affordable options. Amazon’s conduct also suppresses rival online marketplace service providers’ ability to compete for sellers by offering lower fees because sellers cannot pass along those savings to shoppers in the form of lower product prices.\n</p></blockquote><p>This all sounds bad and, at first glance, anti-competitive. Consider, though, what the FTC is implicitly asking:</p><ul>\n<li>First, Amazon is <em>replacing</em> the offending merchants in the Buy Box with other merchants who offer lower prices (including, potentially, Amazon itself). It’s difficult to understand how this is bad for consumers.</li>\n<li>Second, insisting that Amazon promote merchants who offer higher prices on Amazon and lower prices elsewhere is, once again, insisting that Amazon offer the fruits of its investments, both in terms of customer acquisition and in delivery speed, to merchants who are actively seeking to develop an Amazon competitor.</li>\n<li>Third, most-favored nation clauses, which this is in practice if not in specifics (in fact, according to the FTC, these practices replaced MFN clauses) have consistently been found to be legal.</li>\n</ul><p>Most importantly, though, this alleged illegality rests on Amazon being a monopoly, which means, as happens with all antitrust cases, we have a question of market definition. In this case the FTC has defined the relevant markets as “the online superstore market” and “the market for online marketplace services.” These definitions, conveniently enough, exclude all brick-and-mortar retailers (the word “omnichannel” doesn’t appear in the complaint), and all independent retailers, such as those hosted by Shopify. The FTC says that this narrow definition makes sense because of the convenience and selection that is exclusive to “online superstores”, thanks to the ability to ship things together; never-mind that other websites are only a click away, and that the entire reason you can ship things together is because most items on Amazon are Fulfilled by Amazon (see the previous complaint).</p><p>This definition is obviously going to be critical to this case: <a href=\"https://www.ben-evans.com/benedictevans/2019/12/amazons-market-share19\">Benedict Evans ran the numbers</a> and, if you consider all of retail, then Amazon only has single-digits worth of marketshare; if you consider all of e-commerce Amazon has about 35% share. What is clear is that just about everything on Amazon is available elsewhere: the power the company has with regard to pricing is a function of the demand it delivers to merchants — demand that is not compelled of customers, but willingly given precisely because customers find Amazon’s service to be valuable.</p>",
          "contentText": "This charge is more compelling; from the complaint:\n  One set of tactics stifles the ability of rivals to attract shoppers by offering lower prices. Amazon deploys a sophisticated surveillance network of web crawlers that constantly monitor the internet, searching for discounts that might threaten Amazon’s empire. When Amazon detects elsewhere online a product that is cheaper than a seller’s offer for the same product on Amazon, Amazon punishes that seller. It does so to prevent rivals from gaining business by offering shoppers or sellers lower prices…\n  The sanctions Amazon levies on sellers vary. For example, Amazon knocks these sellers out of the all-important “Buy Box,” the display from which a shopper can “Add to Cart” or “Buy Now” an Amazon-selected offer for a product. Nearly ██% of Amazon sales are made through the Buy Box and, as Amazon internally recognizes, eliminating a seller from the Buy Box causes that seller’s sales to “tank.” Another form of punishment is to bury discounting sellers so far down in Amazon’s search results that they become effectively invisible…\n  Moreover, Amazon’s one-two punch of seller punishments and high seller fees often forces sellers to use their inflated Amazon prices as a price floor everywhere else. As a result, Amazon’s conduct causes online shoppers to face artificially higher prices even when shopping somewhere other than Amazon. Amazon’s punitive regime distorts basic market signals: one of the ways sellers respond to Amazon’s fee hikes is by increasing their own prices off Amazon. An executive from another online retailer sums up this perverse dynamic: Amazon’s anti-discounting conduct █████████████████████████████████. Amazon’s illegal tactics mean that when Amazon raises its fees, others — competitors, sellers, and shoppers – suffer the harms.\n  Amazon’s tactics suppress rival online superstores’ ability to compete for shoppers by offering lower prices, thereby depriving American households of more affordable options. Amazon’s conduct also suppresses rival online marketplace service providers’ ability to compete for sellers by offering lower fees because sellers cannot pass along those savings to shoppers in the form of lower product prices.\nThis all sounds bad and, at first glance, anti-competitive. Consider, though, what the FTC is implicitly asking:\nFirst, Amazon is replacing the offending merchants in the Buy Box with other merchants who offer lower prices (including, potentially, Amazon itself). It’s difficult to understand how this is bad for consumers.\nSecond, insisting that Amazon promote merchants who offer higher prices on Amazon and lower prices elsewhere is, once again, insisting that Amazon offer the fruits of its investments, both in terms of customer acquisition and in delivery speed, to merchants who are actively seeking to develop an Amazon competitor.\nThird, most-favored nation clauses, which this is in practice if not in specifics (in fact, according to the FTC, these practices replaced MFN clauses) have consistently been found to be legal.\nMost importantly, though, this alleged illegality rests on Amazon being a monopoly, which means, as happens with all antitrust cases, we have a question of market definition. In this case the FTC has defined the relevant markets as “the online superstore market” and “the market for online marketplace services.” These definitions, conveniently enough, exclude all brick-and-mortar retailers (the word “omnichannel” doesn’t appear in the complaint), and all independent retailers, such as those hosted by Shopify. The FTC says that this narrow definition makes sense because of the convenience and selection that is exclusive to “online superstores”, thanks to the ability to ship things together; never-mind that other websites are only a click away, and that the entire reason you can ship things together is because most items on Amazon are Fulfilled by Amazon (see the previous complaint).This definition is obviously going to be critical to this case: Benedict Evans ran the numbers and, if you consider all of retail, then Amazon only has single-digits worth of marketshare; if you consider all of e-commerce Amazon has about 35% share. What is clear is that just about everything on Amazon is available elsewhere: the power the company has with regard to pricing is a function of the demand it delivers to merchants — demand that is not compelled of customers, but willingly given precisely because customers find Amazon’s service to be valuable."
        },
        {
          "subtitle": "Amazon’s Market-Making",
          "contentHtml": "<p>That’s not to say that there aren’t merchants wholly beholden to Amazon; in 2019 an Amazon reseller named Molson Hart <a href=\"https://medium.com/@molson_hart/amazon-needs-a-competitor-and-walmart-aint-it-5997977b77b2\">wrote an essay on Medium</a> complaining about Amazon’s fulfillment prices, and included this chart:</p><blockquote><p>\n  We sell plush and construction toys on Amazon. Well, technically, we sell toys on our website, on eBay, on Walmart.com, to brick-and-mortar stores, and we sell on Amazon. But, really, we only sell on Amazon. In 2018, we had about $4,000,000 in sales but Amazon.com accounted for over 98% of that.</p>\n<p>  <a href=\"https://medium.com/@molson_hart/amazon-needs-a-competitor-and-walmart-aint-it-5997977b77b2\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-1.png?resize=640%2C395&amp;ssl=1\" alt=\"How Amazon dominates the sale of one merchant\" width=\"640\" height=\"395\" class=\"aligncenter size-full wp-image-11638\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-1.png?w=727&amp;ssl=1 727w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/amazonftc-1.png?resize=300%2C185&amp;ssl=1 300w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>  Harvard Business School would call this “vendor/customer concentration”. In the e-commerce world, we call it being Amazon’s bitch.</p>\n<p>  While Amazon received $1.95 million from us last year, they are not afraid of losing our business for a couple of reasons. First, there are thousands of companies out there eager to take our place. Second, Amazon had $277 billion in gross merchandise revenue in 2018. Our $3.9 million in sales on Amazon accounted for .0014% of that. Finally, we have nowhere else to go and Amazon knows it.\n</p></blockquote><p>Hart <a href=\"https://www.youtube.com/watch?v=Pan9eNpVqVY\">shared his entrepreneurship story on a podcast</a>, and I think it provides important context:</p><div align=\"center\"><div id=\"v-g4OK1LSH-1\" class=\"video-player\"><iframe title=\"VideoPress Video Player\" aria-label=\"VideoPress Video Player\" width=\"640\" height=\"360\" src=\"https://videopress.com/embed/g4OK1LSH?hd=1&amp;cover=1&amp;loop=0&amp;autoPlay=0&amp;permalink=1&amp;muted=0&amp;controls=1&amp;playsinline=0&amp;useAverageColor=0&amp;preloadContent=metadata\" frameborder=\"0\" allowfullscreen=\"\" data-resize-to-parent=\"true\" allow=\"clipboard-write\"></iframe><script src=\"https://s0.wp.com/wp-content/plugins/video/assets/js/next/videopress-iframe.js\"></script></div></div><blockquote><p>\n  In 2014-2015, you could sell literally anything you wanted on Amazon and it was profitable. You didn’t really need to do any data analysis. If you were buying in China and you didn’t do an absolutely abominable job sending it to Amazon, you were making money. When you’re selling into retail it’s different. On Amazon you could just sell commodities in 2014-2015 — literally a towel, you didn’t have to have a brand or anything — but to sell into retail, which is a developed market, I can’t call into Walmart and be like, “Hey, I’m this twenty-six year old kid and I’m going to sell you towels.” They’re like “No, we’re going to buy from branded manufacturers of towels and factories for towels and people who have an established business, some credibility in this industry.” So if we wanted to sell into retail we had to bring innovative products to the table, otherwise there was no incentive for them to take the risk on a young company with a young founder, etc.</p>\n<p>  So I figured out at one point, “Maybe instead of coming up with all of these innovative products, because innovation was really hard, maybe we could find stuff that is already popular in China or Japan or Korea and just bring it to America and re-brand it.” So then what I did is I just went onto Taobao — China’s Amazon — and went through tens of thousands of products. I looked at the top sellers in each category, toys, games, bikes, sporting goods, all that stuff, and anytime I saw a product that was selling really well in China that I had never seen before I put it into a bucket and said, “OK, we’re going to launch this in America.” So that’s what we did and by-and-large was a pretty effective strategy. That’s actually how Brain Flakes was born.\n</p></blockquote><p>Brain Flakes is <a href=\"https://viahart.com/\">Hart’s biggest product</a>, and the biggest driver of that Amazon-dominated sales chart up above. The question I have with regards to that chart, though, and Hart’s griping about Amazon’s fees, is hasn’t Amazon earned the right to charge Hart whatever it deems appropriate? Hart himself admits that his entire business was predicated on Amazon’s marketplace model, a model that enabled individual entrepreneurs with smarts and hustle to build big businesses without a reputation or a brand. To put it another way, Hart’s business is dominated by Amazon <em>because Amazon made his entire business possible</em> (and if these complaints sound familiar, they echo <a href=\"https://stratechery.com/2020/email-addresses-and-razor-blades/\">complaints about Facebook from companies built on Facebook</a>, <a href=\"https://stratechery.com/2018/the-bill-gates-line/\">Yelp’s complaints that they have to acquire customers instead of relying on SEO</a>, or <a href=\"https://stratechery.com/2014/economic-power-age-abundance/\">publishers the world over</a> blaming their commodity status on the same companies that made the market for them in the first place).</p><p>I am by no means here to pick on Hart or any of the millions of other 3rd-party merchants on Amazon: I salute their entrepreneurial grit. I fail, though, to see what exactly is anticompetitive in this story. What I see, much like the Prime program above, is massive investment by Amazon to create an entire category that dramatically increased the amount of commerce, and it’s unclear to me why they can’t conduct normal business activity to ensure they have competitive prices in that market.</p><p>That noted, the reason I find this part of the complaint compelling is that I do have unease about the use of enforced price matching and other non-organic means of limiting competition; that, along with acquisitions and digital advertising, was one of the three areas of concern I highlighted in <a href=\"https://stratechery.com/2019/where-warrens-wrong/\">a 2019 Article</a> that explained what antitrust crusaders fail to understand about Aggregators who gain market power not through controlling supply but rather by harnessing demand. The issue, though, <a href=\"https://stratechery.com/2019/a-framework-for-regulating-competition-on-the-internet/\">is that these concerns ought be addressed through new laws</a>; trying to apply antitrust regulations that were created for the analog world in a digital context simply doesn’t make sense, and very likely will, like so many other recent FTC actions, fail in court.</p>",
          "contentText": "That’s not to say that there aren’t merchants wholly beholden to Amazon; in 2019 an Amazon reseller named Molson Hart wrote an essay on Medium complaining about Amazon’s fulfillment prices, and included this chart:\n  We sell plush and construction toys on Amazon. Well, technically, we sell toys on our website, on eBay, on Walmart.com, to brick-and-mortar stores, and we sell on Amazon. But, really, we only sell on Amazon. In 2018, we had about $4,000,000 in sales but Amazon.com accounted for over 98% of that.\n  \n  Harvard Business School would call this “vendor/customer concentration”. In the e-commerce world, we call it being Amazon’s bitch.\n  While Amazon received $1.95 million from us last year, they are not afraid of losing our business for a couple of reasons. First, there are thousands of companies out there eager to take our place. Second, Amazon had $277 billion in gross merchandise revenue in 2018. Our $3.9 million in sales on Amazon accounted for .0014% of that. Finally, we have nowhere else to go and Amazon knows it.\nHart shared his entrepreneurship story on a podcast, and I think it provides important context:\n  In 2014-2015, you could sell literally anything you wanted on Amazon and it was profitable. You didn’t really need to do any data analysis. If you were buying in China and you didn’t do an absolutely abominable job sending it to Amazon, you were making money. When you’re selling into retail it’s different. On Amazon you could just sell commodities in 2014-2015 — literally a towel, you didn’t have to have a brand or anything — but to sell into retail, which is a developed market, I can’t call into Walmart and be like, “Hey, I’m this twenty-six year old kid and I’m going to sell you towels.” They’re like “No, we’re going to buy from branded manufacturers of towels and factories for towels and people who have an established business, some credibility in this industry.” So if we wanted to sell into retail we had to bring innovative products to the table, otherwise there was no incentive for them to take the risk on a young company with a young founder, etc.\n  So I figured out at one point, “Maybe instead of coming up with all of these innovative products, because innovation was really hard, maybe we could find stuff that is already popular in China or Japan or Korea and just bring it to America and re-brand it.” So then what I did is I just went onto Taobao — China’s Amazon — and went through tens of thousands of products. I looked at the top sellers in each category, toys, games, bikes, sporting goods, all that stuff, and anytime I saw a product that was selling really well in China that I had never seen before I put it into a bucket and said, “OK, we’re going to launch this in America.” So that’s what we did and by-and-large was a pretty effective strategy. That’s actually how Brain Flakes was born.\nBrain Flakes is Hart’s biggest product, and the biggest driver of that Amazon-dominated sales chart up above. The question I have with regards to that chart, though, and Hart’s griping about Amazon’s fees, is hasn’t Amazon earned the right to charge Hart whatever it deems appropriate? Hart himself admits that his entire business was predicated on Amazon’s marketplace model, a model that enabled individual entrepreneurs with smarts and hustle to build big businesses without a reputation or a brand. To put it another way, Hart’s business is dominated by Amazon because Amazon made his entire business possible (and if these complaints sound familiar, they echo complaints about Facebook from companies built on Facebook, Yelp’s complaints that they have to acquire customers instead of relying on SEO, or publishers the world over blaming their commodity status on the same companies that made the market for them in the first place).I am by no means here to pick on Hart or any of the millions of other 3rd-party merchants on Amazon: I salute their entrepreneurial grit. I fail, though, to see what exactly is anticompetitive in this story. What I see, much like the Prime program above, is massive investment by Amazon to create an entire category that dramatically increased the amount of commerce, and it’s unclear to me why they can’t conduct normal business activity to ensure they have competitive prices in that market.That noted, the reason I find this part of the complaint compelling is that I do have unease about the use of enforced price matching and other non-organic means of limiting competition; that, along with acquisitions and digital advertising, was one of the three areas of concern I highlighted in a 2019 Article that explained what antitrust crusaders fail to understand about Aggregators who gain market power not through controlling supply but rather by harnessing demand. The issue, though, is that these concerns ought be addressed through new laws; trying to apply antitrust regulations that were created for the analog world in a digital context simply doesn’t make sense, and very likely will, like so many other recent FTC actions, fail in court."
        }
      ]
    },
    {
      "title": "Charter-Disney Winners and Losers",
      "publishedDate": "2023-09-12T05:07:01-07:00",
      "updatedDate": "2023-09-20T05:53:11-07:00",
      "contentHtml": "\n\t\t<p>From the <a href=\"https://www.wsj.com/business/media/disney-charter-end-carriage-dispute-restoring-espn-other-channels-to-15-million-spectrum-households-6236560c\">Wall Street Journal</a>:</p>\n<blockquote><p>\n  Disney and Charter Communications have reached an agreement that will restore popular channels, including ESPN and ABC, to the cable operator’s nearly 15 million subscribers, ending a blackout that lasted for more than a week. The agreement comes just hours before ESPN’s coverage of the first “Monday Night Football” game of the season — a highly anticipated matchup between the New York Jets and Buffalo Bills. It also marks a seminal moment in the oft-fraught relationship between pay-TV providers and entertainment companies, which have been at loggerheads in recent years as the continued rise of streaming upended their respective businesses.\n</p></blockquote>\n<p>Disney and Charter released a <a href=\"https://corporate.charter.com/newsroom/the-walt-disney-company-and-charter-communications-announce-transformative-agreement-2023\">joint</a> <a href=\"https://thewaltdisneycompany.com/the-walt-disney-company-and-charter-communications-announce-transformative-agreement-for-distribution-of-disneys-linear-networks-and-direct-to-consumer-services/\">press release</a> that included the details:</p>\n<blockquote><p>\n  Among the key deal points:</p>\n<ul>\n<li>In the coming months, the Disney+ Basic ad-supported offering will be provided to customers who purchase the Spectrum TV Select package, as part of a wholesale arrangement;</li>\n<li>The ESPN flagship direct-to-consumer service will be made available to Spectrum TV Select subscribers upon launch and;</li>\n<li>Charter will maintain flexibility to offer a range of video packages at varying price points based upon different customer’s viewing preferences.</li>\n</ul>\n<p>  Charter also will use its significant distribution capabilities to offer Disney’s direct-to-consumer services to all of its customers – in particular its large broadband-only customer base – for purchase at retail rates. These include Disney+, Hulu and ESPN+, as well as The Disney Bundle.</p>\n<p>  Effective immediately, Spectrum TV will provide its customers widespread access to a more curated lineup of 19 networks from The Walt Disney Company. Spectrum will continue to carry the ABC Owned Television Stations, Disney Channel, FX and the Nat Geo Channel, in addition to the full ESPN network suite. Networks that will no longer be included in Spectrum TV video packages are Baby TV, Disney Junior, Disney XD, Freeform, FXM, FXX, Nat Geo Wild, and Nat Geo Mundo.</p>\n<p>  To preserve all these valuable business models, the parties also have renewed their commitment to lead the industry in mitigating the effects of unauthorized password sharing.\n</p></blockquote>\n<p>The biggest question here is the third bullet point: Charter would like to offer bundles that don’t include ESPN, but it’s notable that the press release says “maintain flexibility”, as opposed to “gain flexibility”; that lines up with something ESPN chairman Jimmy Pitaro told the <a href=\"https://www.hollywoodreporter.com/business/digital/disney-dana-walden-espn-jimmy-pitaro-spectrum-deal-interview-1235587183/\">Hollywood Reporter</a>:</p>\n<blockquote><p>\n  “The first [priority] was protecting the traditional business model, one that’s been very, very good to us and continues to be good to us,” Pitaro adds. “And we were able to do that, we secured commitments that were very strong in terms of rates and minimum penetration.”\n</p></blockquote>\n<p>I’m going to assume that this means that Spectrum will continue to be contractually compelled to have ESPN in 70%~80% of its TV packages (that’s why it’s hard to even find information about the company’s TV Basic and TV Essentials packages on <a href=\"https://www.spectrum.com/cable-tv/choose-tv-your-way\">its marketing pages</a>); that leaves the question of who won, and the answer depends on your perspective: are you asking about this specific stand-off, the overall future of TV, or the entire arc of video?</p>\n<p><em>UPDATE: <a href=\"https://x.com/andrewmarchand/status/1701605179441987921\">The minimum penetration is 85%</a>.</em></p>\n<h3>Current Standoff — Winner: Charter</h3>\n<p>Charter is the unequivocal winner of this standoff. Indeed, the agreement details above completely validate my argument last week that <a href=\"https://stratechery.com/2023/the-rise-and-fall-of-espns-leverage/\">ESPN no longer has the same leverage it enjoyed for decades</a>. Remember, Charter was willing to meet Disney’s demand for higher ESPN affiliate fees; what Charter wanted was all of Disney’s non-sports content too. That used to exist on Disney’s cable channels, but Disney — along with the rest of Hollywood — had broken the bundle by putting all of their best content on its streaming services. Now the ad-supported Disney+ will be a part of the cable bundle as well, along with the future ESPN streaming service.</p>\n<p>Disney did, of course, get its rate increase and minimum penetration guarantees, and will charge for Disney+; that charge, though, is balanced out by the elimination of the channels that Disney cannibalized from the bundle.</p>\n<p>More important is the fact that Disney has been forced to give up its attempt at double-dipping: no longer can the company get paid by Charter for channels <em>and</em> charge subscribers directly for what is generally the same general entertainment content. That was what Charter wanted, and Disney, lacking leverage and the reality of massive sports rights fees that presumed the presence of Charter’s millions of TV subscribers, gave in.</p>\n<h3>Future of TV — Winner: Disney</h3>\n<p>Here is perhaps the biggest surprise in this deal: I actually think it is <em>Disney</em> that is the bigger winner when it comes to the future of TV. Note this paragraph in the press release:</p>\n<blockquote><p>\n  Charter will also use its significant distribution capabilities to offer Disney’s direct-to-consumer services to all its customers – in particular its large broadband-only customer base – for purchase at retail rates. These include Disney+, Hulu and ESPN+, as well as The Disney Bundle.\n</p></blockquote>\n<p>I wrote extensively about the go-to-market capabilities of cable companies and why they were well-positioned to bundle streaming services last year in <a href=\"https://stratechery.com/2022/cables-last-laugh/\">Cable’s Last Laugh</a>; I will refrain from quoting half of the piece, and briefly summarize:</p>\n<ul>\n<li>First, Disney, along with every other streaming service, needs help improving their go-to-market efficiency; in this there is no better asset than the cable companies’ existing go-to-market machines.</li>\n<li>Second, Disney, along with every other streaming service, needs help lowering churn. When you are a standalone streaming service the only way to stop churn is by continually producing new must-see content, which is extremely expensive. It is much easier if you are part of a bundle and sharing the burden of generating new content with other companies.</li>\n<li>Third, Disney, along with every other streaming service, has come to realize that <a href=\"https://stratechery.com/2023/the-unified-content-business-model/\">their greatest growth opportunity is in advertising</a>. A profitable advertising business, though, depends on scale; the fact that Disney has just quadrupled its ad-supported Disney+ base is a big deal!</li>\n</ul>\n<p>It’s obvious, of course, that a stronger bundle is better for Disney’s existing cable channels, particularly ESPN; what should also be clear is that a stronger bundle is better for Disney’s streaming services as well, and now Disney is committed to building exactly that alongside of Charter, and inevitably over the next several years, every other pay-TV provider.</p>\n<p>This is why Disney is the long-term winner: the obstacle to the company doing the right thing for the long-term health of their business was not Charter, it was Disney’s own management, and Charter did the company the tremendous favor of forcing it to give up an unsustainable double-dipping strategy and take a step into a future of re-bundling.</p>\n<p>Charter, meanwhile, knows better than anyone the value of bundles: the more services it can tie into a single billing statement the stickier their offering is for end users. Yes, the company may have been willing to give up video, but it is stronger for having it.</p>\n<h3>The Arc of Video — Winner: Consumers</h3>\n<p>All that noted, both Charter and Disney emerge from the last decade weaker than they were before. Disney, <a href=\"https://stratechery.com/2023/hollywood-on-strike/\">along with the rest of Hollywood</a>, killed the golden goose that was 90% of households subscribed to cable. No matter how successful Disney+ or an over-the-top ESPN streaming service becomes it will never be as profitable as effectively charging a tax on every household in America.</p>\n<p>Charter is worse off as well: yes, the company had leverage over Disney in this negotiation, but that was a function of no longer caring whether or not it carried ESPN, not because the alternative was better. Indeed, Charter’s strategy of directing unhappy customers to Fubo was a necessary negotiating ploy that carried long-term risks: once customers are accustomed to getting their sports and news from an app it quickly becomes apparent that that app can be accessed over any broadband provider, including fiber and fixed wireless. As I noted above, Charter knows the value of bundles better than anyone, and this new bundle is much weaker than the old one.</p>\n<p>The big winners, though, are consumers, on multiple levels:</p>\n<ul>\n<li>First, consumers will soon have the option to get nearly all of their entertainment on an a la carte basis, particularly once the ESPN streaming service launches, even as they have access to a bundle that includes nearly all of their entertainment for a lower price than if they subscribed individually.</li>\n<li>Second, consumers will be able to access general entertainment on-demand, and a far greater range of sports thanks to the effectively infinite number of channels enabled by streaming.</li>\n<li>Third, consumers will be able to get their general entertainment ad-free if they are willing to pay (this is a win for the entertainment companies as well, who will gain the opportunity to segment their customer base based on their willingness to pay for an ad-free experience).</li>\n</ul>\n<p>The biggest win of all, though, comes at Charter’s expense specifically: as noted above the loosening of the TV part of the bundle will make it easier to change broadband providers. That means that Charter will have to compete based on the quality of its broadband, which has fallen behind fiber over the last several years. Charter has announced plans to rectify that, pledging to spend $5.5 billion over the next three years <a href=\"https://www.fiercetelecom.com/broadband/charter-plots-3-year-upgrade-deploy-docsis-40-2025\">to move its entire network to DOCSIS 4.0</a>; other cable carriers are plotting similar upgrades. Meanwhile, Charter has been very aggressive in pushing its mobile service, significantly undercutting the big phone carriers in price, particularly as part of a bundle.</p>\n<p>This is great news for consumers, and redolent of <a href=\"https://stratechery.com/2013/why-do-carriers-subsidize-the-iphone/\">what happened with the iPhone</a>. When the consumer point of contact changed from a carrier-controlled interface to an Apple-controlled one, the only alternative for phone carriers was to compete on their network quality; that was bad for profitability but great for consumers, both in terms of price and quality. I would expect a similar effect as the consumer point of contact for TV continues to change from a cable box to apps: infrastructure providers like Charter will have to compete by building infrastructure, and that’s a good thing.</p>\n<h3>Other Winners and Losers</h3>\n<p>Tech is another big winner of this fight, which shouldn’t be a surprise: big tech is so dominant in part because it provides so much consumer surplus in its markets; a market where consumers are winning is probably one where tech is as well. In this case video is becoming an app game, and while Charter and the other pay-TV providers have useful go-to-market channels, tech is the king of distribution and customer acquisition.</p>\n<p>To that end, what cable can do for streamers is already being done by Amazon, Apple, Roku, etc.; the latest entrant is YouTube, <a href=\"https://stratechery.com/2022/ftc-fines-epic-netflix-ads-youtube-and-the-nfl/\">which is using NFL Sunday Ticket to launch YouTube Channels</a>, a streaming marketplace designed to sell services like Disney+ (for an ongoing commission, of course). YouTube, though, can pair that offering with YouTube TV, which means it has everything that Spectrum has; in this regard the fact that YouTube <a href=\"https://awfulannouncing.com/youtube/sunday-ticket-review-week-1.html\">has already significantly increased the value of Sunday Ticket through better technology</a> should make competitors nervous.</p>\n<p>The second big winner is Fox. Fox sold off its 21st Century division to Disney to focus on news and sports; Fox News charges the second highest affiliate fees amongst cable channels, and Fox has invested heavily in sports rights that run across the Fox broadcast network (which gets large retransmission fees from cable companies), FS1, and regional networks like the Big Ten network. The cable bundle sticking together is existential for Fox, and it looks like that is going to happen — indeed, Fox’s live offerings are now going to be re-bundled with 21st Century content streamed by Disney.</p>\n<p>Fox’s fate, meanwhile, gets to why sports leagues are big winners as well. Had the bundle fallen apart than the NBA, which is in the midst of negotiating a new rights deal, would have been in big trouble; now that it has a future ESPN can more confidently bid. At the same time, now that everything is becoming an app, including traditional TV, the motivation for tech companies to bid in order to secure their marketplaces as the ultimate winners is higher as well.</p>\n<hr>\n<p>One final comment about the significance of this deal.</p>\n<p>There is a certain flavor of detached cynicism that is often the default response to news; examples abounded yesterday after this deal was announced. For example:</p>\n<p><a href=\"https://twitter.com/sherman4949/status/1701277788941017476\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?resize=640%2C653&amp;ssl=1\" alt=\"A cynical response to the Charter-Disney deal that is wrong\" width=\"640\" height=\"653\" class=\"aligncenter size-full wp-image-11545\" style=\"border: 1px solid #ddd;\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?w=1180&amp;ssl=1 1180w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?resize=294%2C300&amp;ssl=1 294w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?resize=1004%2C1024&amp;ssl=1 1004w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?resize=768%2C784&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?resize=617%2C630&amp;ssl=1 617w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>Most of the time this response works well: the status quo is a powerful thing, and if your goal is being right more often than not than it is always safer to be skeptical that things are different this time.</p>\n<p>In this case, though, I think Sherman has it wrong: cable TV as we know it ended several years ago with <a href=\"https://stratechery.com/2017/the-great-unbundling/\">The Great Unbundling</a>. The significance of the just-announced deal between Disney and Charter is that The Great Re-bundling has begun.</p>\n\n\t\t\t",
      "contentText": "\n\t\tFrom the Wall Street Journal:\n\n  Disney and Charter Communications have reached an agreement that will restore popular channels, including ESPN and ABC, to the cable operator’s nearly 15 million subscribers, ending a blackout that lasted for more than a week. The agreement comes just hours before ESPN’s coverage of the first “Monday Night Football” game of the season — a highly anticipated matchup between the New York Jets and Buffalo Bills. It also marks a seminal moment in the oft-fraught relationship between pay-TV providers and entertainment companies, which have been at loggerheads in recent years as the continued rise of streaming upended their respective businesses.\n\nDisney and Charter released a joint press release that included the details:\n\n  Among the key deal points:\n\nIn the coming months, the Disney+ Basic ad-supported offering will be provided to customers who purchase the Spectrum TV Select package, as part of a wholesale arrangement;\nThe ESPN flagship direct-to-consumer service will be made available to Spectrum TV Select subscribers upon launch and;\nCharter will maintain flexibility to offer a range of video packages at varying price points based upon different customer’s viewing preferences.\n\n  Charter also will use its significant distribution capabilities to offer Disney’s direct-to-consumer services to all of its customers – in particular its large broadband-only customer base – for purchase at retail rates. These include Disney+, Hulu and ESPN+, as well as The Disney Bundle.\n  Effective immediately, Spectrum TV will provide its customers widespread access to a more curated lineup of 19 networks from The Walt Disney Company. Spectrum will continue to carry the ABC Owned Television Stations, Disney Channel, FX and the Nat Geo Channel, in addition to the full ESPN network suite. Networks that will no longer be included in Spectrum TV video packages are Baby TV, Disney Junior, Disney XD, Freeform, FXM, FXX, Nat Geo Wild, and Nat Geo Mundo.\n  To preserve all these valuable business models, the parties also have renewed their commitment to lead the industry in mitigating the effects of unauthorized password sharing.\n\nThe biggest question here is the third bullet point: Charter would like to offer bundles that don’t include ESPN, but it’s notable that the press release says “maintain flexibility”, as opposed to “gain flexibility”; that lines up with something ESPN chairman Jimmy Pitaro told the Hollywood Reporter:\n\n  “The first [priority] was protecting the traditional business model, one that’s been very, very good to us and continues to be good to us,” Pitaro adds. “And we were able to do that, we secured commitments that were very strong in terms of rates and minimum penetration.”\n\nI’m going to assume that this means that Spectrum will continue to be contractually compelled to have ESPN in 70%~80% of its TV packages (that’s why it’s hard to even find information about the company’s TV Basic and TV Essentials packages on its marketing pages); that leaves the question of who won, and the answer depends on your perspective: are you asking about this specific stand-off, the overall future of TV, or the entire arc of video?\nUPDATE: The minimum penetration is 85%.\nCurrent Standoff — Winner: Charter\nCharter is the unequivocal winner of this standoff. Indeed, the agreement details above completely validate my argument last week that ESPN no longer has the same leverage it enjoyed for decades. Remember, Charter was willing to meet Disney’s demand for higher ESPN affiliate fees; what Charter wanted was all of Disney’s non-sports content too. That used to exist on Disney’s cable channels, but Disney — along with the rest of Hollywood — had broken the bundle by putting all of their best content on its streaming services. Now the ad-supported Disney+ will be a part of the cable bundle as well, along with the future ESPN streaming service.\nDisney did, of course, get its rate increase and minimum penetration guarantees, and will charge for Disney+; that charge, though, is balanced out by the elimination of the channels that Disney cannibalized from the bundle.\nMore important is the fact that Disney has been forced to give up its attempt at double-dipping: no longer can the company get paid by Charter for channels and charge subscribers directly for what is generally the same general entertainment content. That was what Charter wanted, and Disney, lacking leverage and the reality of massive sports rights fees that presumed the presence of Charter’s millions of TV subscribers, gave in.\nFuture of TV — Winner: Disney\nHere is perhaps the biggest surprise in this deal: I actually think it is Disney that is the bigger winner when it comes to the future of TV. Note this paragraph in the press release:\n\n  Charter will also use its significant distribution capabilities to offer Disney’s direct-to-consumer services to all its customers – in particular its large broadband-only customer base – for purchase at retail rates. These include Disney+, Hulu and ESPN+, as well as The Disney Bundle.\n\nI wrote extensively about the go-to-market capabilities of cable companies and why they were well-positioned to bundle streaming services last year in Cable’s Last Laugh; I will refrain from quoting half of the piece, and briefly summarize:\n\nFirst, Disney, along with every other streaming service, needs help improving their go-to-market efficiency; in this there is no better asset than the cable companies’ existing go-to-market machines.\nSecond, Disney, along with every other streaming service, needs help lowering churn. When you are a standalone streaming service the only way to stop churn is by continually producing new must-see content, which is extremely expensive. It is much easier if you are part of a bundle and sharing the burden of generating new content with other companies.\nThird, Disney, along with every other streaming service, has come to realize that their greatest growth opportunity is in advertising. A profitable advertising business, though, depends on scale; the fact that Disney has just quadrupled its ad-supported Disney+ base is a big deal!\n\nIt’s obvious, of course, that a stronger bundle is better for Disney’s existing cable channels, particularly ESPN; what should also be clear is that a stronger bundle is better for Disney’s streaming services as well, and now Disney is committed to building exactly that alongside of Charter, and inevitably over the next several years, every other pay-TV provider.\nThis is why Disney is the long-term winner: the obstacle to the company doing the right thing for the long-term health of their business was not Charter, it was Disney’s own management, and Charter did the company the tremendous favor of forcing it to give up an unsustainable double-dipping strategy and take a step into a future of re-bundling.\nCharter, meanwhile, knows better than anyone the value of bundles: the more services it can tie into a single billing statement the stickier their offering is for end users. Yes, the company may have been willing to give up video, but it is stronger for having it.\nThe Arc of Video — Winner: Consumers\nAll that noted, both Charter and Disney emerge from the last decade weaker than they were before. Disney, along with the rest of Hollywood, killed the golden goose that was 90% of households subscribed to cable. No matter how successful Disney+ or an over-the-top ESPN streaming service becomes it will never be as profitable as effectively charging a tax on every household in America.\nCharter is worse off as well: yes, the company had leverage over Disney in this negotiation, but that was a function of no longer caring whether or not it carried ESPN, not because the alternative was better. Indeed, Charter’s strategy of directing unhappy customers to Fubo was a necessary negotiating ploy that carried long-term risks: once customers are accustomed to getting their sports and news from an app it quickly becomes apparent that that app can be accessed over any broadband provider, including fiber and fixed wireless. As I noted above, Charter knows the value of bundles better than anyone, and this new bundle is much weaker than the old one.\nThe big winners, though, are consumers, on multiple levels:\n\nFirst, consumers will soon have the option to get nearly all of their entertainment on an a la carte basis, particularly once the ESPN streaming service launches, even as they have access to a bundle that includes nearly all of their entertainment for a lower price than if they subscribed individually.\nSecond, consumers will be able to access general entertainment on-demand, and a far greater range of sports thanks to the effectively infinite number of channels enabled by streaming.\nThird, consumers will be able to get their general entertainment ad-free if they are willing to pay (this is a win for the entertainment companies as well, who will gain the opportunity to segment their customer base based on their willingness to pay for an ad-free experience).\n\nThe biggest win of all, though, comes at Charter’s expense specifically: as noted above the loosening of the TV part of the bundle will make it easier to change broadband providers. That means that Charter will have to compete based on the quality of its broadband, which has fallen behind fiber over the last several years. Charter has announced plans to rectify that, pledging to spend $5.5 billion over the next three years to move its entire network to DOCSIS 4.0; other cable carriers are plotting similar upgrades. Meanwhile, Charter has been very aggressive in pushing its mobile service, significantly undercutting the big phone carriers in price, particularly as part of a bundle.\nThis is great news for consumers, and redolent of what happened with the iPhone. When the consumer point of contact changed from a carrier-controlled interface to an Apple-controlled one, the only alternative for phone carriers was to compete on their network quality; that was bad for profitability but great for consumers, both in terms of price and quality. I would expect a similar effect as the consumer point of contact for TV continues to change from a cable box to apps: infrastructure providers like Charter will have to compete by building infrastructure, and that’s a good thing.\nOther Winners and Losers\nTech is another big winner of this fight, which shouldn’t be a surprise: big tech is so dominant in part because it provides so much consumer surplus in its markets; a market where consumers are winning is probably one where tech is as well. In this case video is becoming an app game, and while Charter and the other pay-TV providers have useful go-to-market channels, tech is the king of distribution and customer acquisition.\nTo that end, what cable can do for streamers is already being done by Amazon, Apple, Roku, etc.; the latest entrant is YouTube, which is using NFL Sunday Ticket to launch YouTube Channels, a streaming marketplace designed to sell services like Disney+ (for an ongoing commission, of course). YouTube, though, can pair that offering with YouTube TV, which means it has everything that Spectrum has; in this regard the fact that YouTube has already significantly increased the value of Sunday Ticket through better technology should make competitors nervous.\nThe second big winner is Fox. Fox sold off its 21st Century division to Disney to focus on news and sports; Fox News charges the second highest affiliate fees amongst cable channels, and Fox has invested heavily in sports rights that run across the Fox broadcast network (which gets large retransmission fees from cable companies), FS1, and regional networks like the Big Ten network. The cable bundle sticking together is existential for Fox, and it looks like that is going to happen — indeed, Fox’s live offerings are now going to be re-bundled with 21st Century content streamed by Disney.\nFox’s fate, meanwhile, gets to why sports leagues are big winners as well. Had the bundle fallen apart than the NBA, which is in the midst of negotiating a new rights deal, would have been in big trouble; now that it has a future ESPN can more confidently bid. At the same time, now that everything is becoming an app, including traditional TV, the motivation for tech companies to bid in order to secure their marketplaces as the ultimate winners is higher as well.\n\nOne final comment about the significance of this deal.\nThere is a certain flavor of detached cynicism that is often the default response to news; examples abounded yesterday after this deal was announced. For example:\n\nMost of the time this response works well: the status quo is a powerful thing, and if your goal is being right more often than not than it is always safer to be skeptical that things are different this time.\nIn this case, though, I think Sherman has it wrong: cable TV as we know it ended several years ago with The Great Unbundling. The significance of the just-announced deal between Disney and Charter is that The Great Re-bundling has begun.\n\n\t\t\t",
      "subsections": [
        {
          "subtitle": "Current Standoff — Winner: Charter",
          "contentHtml": "<p>Charter is the unequivocal winner of this standoff. Indeed, the agreement details above completely validate my argument last week that <a href=\"https://stratechery.com/2023/the-rise-and-fall-of-espns-leverage/\">ESPN no longer has the same leverage it enjoyed for decades</a>. Remember, Charter was willing to meet Disney’s demand for higher ESPN affiliate fees; what Charter wanted was all of Disney’s non-sports content too. That used to exist on Disney’s cable channels, but Disney — along with the rest of Hollywood — had broken the bundle by putting all of their best content on its streaming services. Now the ad-supported Disney+ will be a part of the cable bundle as well, along with the future ESPN streaming service.</p><p>Disney did, of course, get its rate increase and minimum penetration guarantees, and will charge for Disney+; that charge, though, is balanced out by the elimination of the channels that Disney cannibalized from the bundle.</p><p>More important is the fact that Disney has been forced to give up its attempt at double-dipping: no longer can the company get paid by Charter for channels <em>and</em> charge subscribers directly for what is generally the same general entertainment content. That was what Charter wanted, and Disney, lacking leverage and the reality of massive sports rights fees that presumed the presence of Charter’s millions of TV subscribers, gave in.</p>",
          "contentText": "Charter is the unequivocal winner of this standoff. Indeed, the agreement details above completely validate my argument last week that ESPN no longer has the same leverage it enjoyed for decades. Remember, Charter was willing to meet Disney’s demand for higher ESPN affiliate fees; what Charter wanted was all of Disney’s non-sports content too. That used to exist on Disney’s cable channels, but Disney — along with the rest of Hollywood — had broken the bundle by putting all of their best content on its streaming services. Now the ad-supported Disney+ will be a part of the cable bundle as well, along with the future ESPN streaming service.Disney did, of course, get its rate increase and minimum penetration guarantees, and will charge for Disney+; that charge, though, is balanced out by the elimination of the channels that Disney cannibalized from the bundle.More important is the fact that Disney has been forced to give up its attempt at double-dipping: no longer can the company get paid by Charter for channels and charge subscribers directly for what is generally the same general entertainment content. That was what Charter wanted, and Disney, lacking leverage and the reality of massive sports rights fees that presumed the presence of Charter’s millions of TV subscribers, gave in."
        },
        {
          "subtitle": "Future of TV — Winner: Disney",
          "contentHtml": "<p>Here is perhaps the biggest surprise in this deal: I actually think it is <em>Disney</em> that is the bigger winner when it comes to the future of TV. Note this paragraph in the press release:</p><blockquote><p>\n  Charter will also use its significant distribution capabilities to offer Disney’s direct-to-consumer services to all its customers – in particular its large broadband-only customer base – for purchase at retail rates. These include Disney+, Hulu and ESPN+, as well as The Disney Bundle.\n</p></blockquote><p>I wrote extensively about the go-to-market capabilities of cable companies and why they were well-positioned to bundle streaming services last year in <a href=\"https://stratechery.com/2022/cables-last-laugh/\">Cable’s Last Laugh</a>; I will refrain from quoting half of the piece, and briefly summarize:</p><ul>\n<li>First, Disney, along with every other streaming service, needs help improving their go-to-market efficiency; in this there is no better asset than the cable companies’ existing go-to-market machines.</li>\n<li>Second, Disney, along with every other streaming service, needs help lowering churn. When you are a standalone streaming service the only way to stop churn is by continually producing new must-see content, which is extremely expensive. It is much easier if you are part of a bundle and sharing the burden of generating new content with other companies.</li>\n<li>Third, Disney, along with every other streaming service, has come to realize that <a href=\"https://stratechery.com/2023/the-unified-content-business-model/\">their greatest growth opportunity is in advertising</a>. A profitable advertising business, though, depends on scale; the fact that Disney has just quadrupled its ad-supported Disney+ base is a big deal!</li>\n</ul><p>It’s obvious, of course, that a stronger bundle is better for Disney’s existing cable channels, particularly ESPN; what should also be clear is that a stronger bundle is better for Disney’s streaming services as well, and now Disney is committed to building exactly that alongside of Charter, and inevitably over the next several years, every other pay-TV provider.</p><p>This is why Disney is the long-term winner: the obstacle to the company doing the right thing for the long-term health of their business was not Charter, it was Disney’s own management, and Charter did the company the tremendous favor of forcing it to give up an unsustainable double-dipping strategy and take a step into a future of re-bundling.</p><p>Charter, meanwhile, knows better than anyone the value of bundles: the more services it can tie into a single billing statement the stickier their offering is for end users. Yes, the company may have been willing to give up video, but it is stronger for having it.</p>",
          "contentText": "Here is perhaps the biggest surprise in this deal: I actually think it is Disney that is the bigger winner when it comes to the future of TV. Note this paragraph in the press release:\n  Charter will also use its significant distribution capabilities to offer Disney’s direct-to-consumer services to all its customers – in particular its large broadband-only customer base – for purchase at retail rates. These include Disney+, Hulu and ESPN+, as well as The Disney Bundle.\nI wrote extensively about the go-to-market capabilities of cable companies and why they were well-positioned to bundle streaming services last year in Cable’s Last Laugh; I will refrain from quoting half of the piece, and briefly summarize:\nFirst, Disney, along with every other streaming service, needs help improving their go-to-market efficiency; in this there is no better asset than the cable companies’ existing go-to-market machines.\nSecond, Disney, along with every other streaming service, needs help lowering churn. When you are a standalone streaming service the only way to stop churn is by continually producing new must-see content, which is extremely expensive. It is much easier if you are part of a bundle and sharing the burden of generating new content with other companies.\nThird, Disney, along with every other streaming service, has come to realize that their greatest growth opportunity is in advertising. A profitable advertising business, though, depends on scale; the fact that Disney has just quadrupled its ad-supported Disney+ base is a big deal!\nIt’s obvious, of course, that a stronger bundle is better for Disney’s existing cable channels, particularly ESPN; what should also be clear is that a stronger bundle is better for Disney’s streaming services as well, and now Disney is committed to building exactly that alongside of Charter, and inevitably over the next several years, every other pay-TV provider.This is why Disney is the long-term winner: the obstacle to the company doing the right thing for the long-term health of their business was not Charter, it was Disney’s own management, and Charter did the company the tremendous favor of forcing it to give up an unsustainable double-dipping strategy and take a step into a future of re-bundling.Charter, meanwhile, knows better than anyone the value of bundles: the more services it can tie into a single billing statement the stickier their offering is for end users. Yes, the company may have been willing to give up video, but it is stronger for having it."
        },
        {
          "subtitle": "The Arc of Video — Winner: Consumers",
          "contentHtml": "<p>All that noted, both Charter and Disney emerge from the last decade weaker than they were before. Disney, <a href=\"https://stratechery.com/2023/hollywood-on-strike/\">along with the rest of Hollywood</a>, killed the golden goose that was 90% of households subscribed to cable. No matter how successful Disney+ or an over-the-top ESPN streaming service becomes it will never be as profitable as effectively charging a tax on every household in America.</p><p>Charter is worse off as well: yes, the company had leverage over Disney in this negotiation, but that was a function of no longer caring whether or not it carried ESPN, not because the alternative was better. Indeed, Charter’s strategy of directing unhappy customers to Fubo was a necessary negotiating ploy that carried long-term risks: once customers are accustomed to getting their sports and news from an app it quickly becomes apparent that that app can be accessed over any broadband provider, including fiber and fixed wireless. As I noted above, Charter knows the value of bundles better than anyone, and this new bundle is much weaker than the old one.</p><p>The big winners, though, are consumers, on multiple levels:</p><ul>\n<li>First, consumers will soon have the option to get nearly all of their entertainment on an a la carte basis, particularly once the ESPN streaming service launches, even as they have access to a bundle that includes nearly all of their entertainment for a lower price than if they subscribed individually.</li>\n<li>Second, consumers will be able to access general entertainment on-demand, and a far greater range of sports thanks to the effectively infinite number of channels enabled by streaming.</li>\n<li>Third, consumers will be able to get their general entertainment ad-free if they are willing to pay (this is a win for the entertainment companies as well, who will gain the opportunity to segment their customer base based on their willingness to pay for an ad-free experience).</li>\n</ul><p>The biggest win of all, though, comes at Charter’s expense specifically: as noted above the loosening of the TV part of the bundle will make it easier to change broadband providers. That means that Charter will have to compete based on the quality of its broadband, which has fallen behind fiber over the last several years. Charter has announced plans to rectify that, pledging to spend $5.5 billion over the next three years <a href=\"https://www.fiercetelecom.com/broadband/charter-plots-3-year-upgrade-deploy-docsis-40-2025\">to move its entire network to DOCSIS 4.0</a>; other cable carriers are plotting similar upgrades. Meanwhile, Charter has been very aggressive in pushing its mobile service, significantly undercutting the big phone carriers in price, particularly as part of a bundle.</p><p>This is great news for consumers, and redolent of <a href=\"https://stratechery.com/2013/why-do-carriers-subsidize-the-iphone/\">what happened with the iPhone</a>. When the consumer point of contact changed from a carrier-controlled interface to an Apple-controlled one, the only alternative for phone carriers was to compete on their network quality; that was bad for profitability but great for consumers, both in terms of price and quality. I would expect a similar effect as the consumer point of contact for TV continues to change from a cable box to apps: infrastructure providers like Charter will have to compete by building infrastructure, and that’s a good thing.</p>",
          "contentText": "All that noted, both Charter and Disney emerge from the last decade weaker than they were before. Disney, along with the rest of Hollywood, killed the golden goose that was 90% of households subscribed to cable. No matter how successful Disney+ or an over-the-top ESPN streaming service becomes it will never be as profitable as effectively charging a tax on every household in America.Charter is worse off as well: yes, the company had leverage over Disney in this negotiation, but that was a function of no longer caring whether or not it carried ESPN, not because the alternative was better. Indeed, Charter’s strategy of directing unhappy customers to Fubo was a necessary negotiating ploy that carried long-term risks: once customers are accustomed to getting their sports and news from an app it quickly becomes apparent that that app can be accessed over any broadband provider, including fiber and fixed wireless. As I noted above, Charter knows the value of bundles better than anyone, and this new bundle is much weaker than the old one.The big winners, though, are consumers, on multiple levels:\nFirst, consumers will soon have the option to get nearly all of their entertainment on an a la carte basis, particularly once the ESPN streaming service launches, even as they have access to a bundle that includes nearly all of their entertainment for a lower price than if they subscribed individually.\nSecond, consumers will be able to access general entertainment on-demand, and a far greater range of sports thanks to the effectively infinite number of channels enabled by streaming.\nThird, consumers will be able to get their general entertainment ad-free if they are willing to pay (this is a win for the entertainment companies as well, who will gain the opportunity to segment their customer base based on their willingness to pay for an ad-free experience).\nThe biggest win of all, though, comes at Charter’s expense specifically: as noted above the loosening of the TV part of the bundle will make it easier to change broadband providers. That means that Charter will have to compete based on the quality of its broadband, which has fallen behind fiber over the last several years. Charter has announced plans to rectify that, pledging to spend $5.5 billion over the next three years to move its entire network to DOCSIS 4.0; other cable carriers are plotting similar upgrades. Meanwhile, Charter has been very aggressive in pushing its mobile service, significantly undercutting the big phone carriers in price, particularly as part of a bundle.This is great news for consumers, and redolent of what happened with the iPhone. When the consumer point of contact changed from a carrier-controlled interface to an Apple-controlled one, the only alternative for phone carriers was to compete on their network quality; that was bad for profitability but great for consumers, both in terms of price and quality. I would expect a similar effect as the consumer point of contact for TV continues to change from a cable box to apps: infrastructure providers like Charter will have to compete by building infrastructure, and that’s a good thing."
        },
        {
          "subtitle": "Other Winners and Losers",
          "contentHtml": "<p>Tech is another big winner of this fight, which shouldn’t be a surprise: big tech is so dominant in part because it provides so much consumer surplus in its markets; a market where consumers are winning is probably one where tech is as well. In this case video is becoming an app game, and while Charter and the other pay-TV providers have useful go-to-market channels, tech is the king of distribution and customer acquisition.</p><p>To that end, what cable can do for streamers is already being done by Amazon, Apple, Roku, etc.; the latest entrant is YouTube, <a href=\"https://stratechery.com/2022/ftc-fines-epic-netflix-ads-youtube-and-the-nfl/\">which is using NFL Sunday Ticket to launch YouTube Channels</a>, a streaming marketplace designed to sell services like Disney+ (for an ongoing commission, of course). YouTube, though, can pair that offering with YouTube TV, which means it has everything that Spectrum has; in this regard the fact that YouTube <a href=\"https://awfulannouncing.com/youtube/sunday-ticket-review-week-1.html\">has already significantly increased the value of Sunday Ticket through better technology</a> should make competitors nervous.</p><p>The second big winner is Fox. Fox sold off its 21st Century division to Disney to focus on news and sports; Fox News charges the second highest affiliate fees amongst cable channels, and Fox has invested heavily in sports rights that run across the Fox broadcast network (which gets large retransmission fees from cable companies), FS1, and regional networks like the Big Ten network. The cable bundle sticking together is existential for Fox, and it looks like that is going to happen — indeed, Fox’s live offerings are now going to be re-bundled with 21st Century content streamed by Disney.</p><p>Fox’s fate, meanwhile, gets to why sports leagues are big winners as well. Had the bundle fallen apart than the NBA, which is in the midst of negotiating a new rights deal, would have been in big trouble; now that it has a future ESPN can more confidently bid. At the same time, now that everything is becoming an app, including traditional TV, the motivation for tech companies to bid in order to secure their marketplaces as the ultimate winners is higher as well.</p><hr><p>One final comment about the significance of this deal.</p><p>There is a certain flavor of detached cynicism that is often the default response to news; examples abounded yesterday after this deal was announced. For example:</p><p><a href=\"https://twitter.com/sherman4949/status/1701277788941017476\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?resize=640%2C653&amp;ssl=1\" alt=\"A cynical response to the Charter-Disney deal that is wrong\" width=\"640\" height=\"653\" class=\"aligncenter size-full wp-image-11545\" style=\"border: 1px solid #ddd;\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?w=1180&amp;ssl=1 1180w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?resize=294%2C300&amp;ssl=1 294w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?resize=1004%2C1024&amp;ssl=1 1004w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?resize=768%2C784&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/charter-disney-1.png?resize=617%2C630&amp;ssl=1 617w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p><p>Most of the time this response works well: the status quo is a powerful thing, and if your goal is being right more often than not than it is always safer to be skeptical that things are different this time.</p><p>In this case, though, I think Sherman has it wrong: cable TV as we know it ended several years ago with <a href=\"https://stratechery.com/2017/the-great-unbundling/\">The Great Unbundling</a>. The significance of the just-announced deal between Disney and Charter is that The Great Re-bundling has begun.</p>",
          "contentText": "Tech is another big winner of this fight, which shouldn’t be a surprise: big tech is so dominant in part because it provides so much consumer surplus in its markets; a market where consumers are winning is probably one where tech is as well. In this case video is becoming an app game, and while Charter and the other pay-TV providers have useful go-to-market channels, tech is the king of distribution and customer acquisition.To that end, what cable can do for streamers is already being done by Amazon, Apple, Roku, etc.; the latest entrant is YouTube, which is using NFL Sunday Ticket to launch YouTube Channels, a streaming marketplace designed to sell services like Disney+ (for an ongoing commission, of course). YouTube, though, can pair that offering with YouTube TV, which means it has everything that Spectrum has; in this regard the fact that YouTube has already significantly increased the value of Sunday Ticket through better technology should make competitors nervous.The second big winner is Fox. Fox sold off its 21st Century division to Disney to focus on news and sports; Fox News charges the second highest affiliate fees amongst cable channels, and Fox has invested heavily in sports rights that run across the Fox broadcast network (which gets large retransmission fees from cable companies), FS1, and regional networks like the Big Ten network. The cable bundle sticking together is existential for Fox, and it looks like that is going to happen — indeed, Fox’s live offerings are now going to be re-bundled with 21st Century content streamed by Disney.Fox’s fate, meanwhile, gets to why sports leagues are big winners as well. Had the bundle fallen apart than the NBA, which is in the midst of negotiating a new rights deal, would have been in big trouble; now that it has a future ESPN can more confidently bid. At the same time, now that everything is becoming an app, including traditional TV, the motivation for tech companies to bid in order to secure their marketplaces as the ultimate winners is higher as well.One final comment about the significance of this deal.There is a certain flavor of detached cynicism that is often the default response to news; examples abounded yesterday after this deal was announced. For example:Most of the time this response works well: the status quo is a powerful thing, and if your goal is being right more often than not than it is always safer to be skeptical that things are different this time.In this case, though, I think Sherman has it wrong: cable TV as we know it ended several years ago with The Great Unbundling. The significance of the just-announced deal between Disney and Charter is that The Great Re-bundling has begun."
        }
      ]
    },
    {
      "title": "The Rise and Fall of ESPN’s Leverage",
      "publishedDate": "2023-09-05T07:42:02-07:00",
      "updatedDate": "2023-09-05T09:59:24-07:00",
      "contentHtml": "\n\t\t<p>On December 12, 1975, RCA Corporation launched its Satcom I communications satellite; the primary purpose was to provide long-distance telephone service between Alaska and the continental U.S. RCA had hopes, though, that there might be new uses for its capacity; to that end the company had listed for sale a 24-hour transponder that covered the entire United States, only to discontinue the offering after failing to find a single buyer.</p>\n<p>Three years later Bill Rasmussen, the communications manager for the Hartford Whalers, was let go from his job; he had the idea of doing the same coverage he did for the team, but independently, along with other Connecticut sports, leveraging the then-expanding cable access TV facilities in Connecticut. These facilities existed to capture broadcast signals from New York and Boston using large antennas and deliver them to people’s houses; the cables, though, had capacity to carry more channels at basically zero cost, including Rasmussen’s proposed Connecticut sports network.</p>\n<p>It was in the course of canvasing Connecticut cable providers that Rasmussen was introduced to the concept of satellite communications, and Al Parinello, a manager at RCA. At first Rasmussen pitched his Connecticut sports network idea, and Parinello was confused: satellites covered the entire country, so why was Rasmussen only talking about a single state? Parinello told James Andrew Miller in <a href=\"https://www.hachettebookgroup.com/titles/tom-shales/those-guys-have-all-the-fun/9780316125765/?lens=little-brown\">Those Guys Have All The Fun</a>:</p>\n<blockquote><p>\n  I can still remember the conversation. Bill said, “Let me get this straight. You mean to tell me, for no extra money — for no extra money! — we could take this signal and beam it anywhere in the country?” And I said, “That’s right.” And then he asked again, “Anywhere in the country?” And I said, “Anywhere.” I remember we went back and forth like this a couple times. Bill and Scott were looking at each other, and they might have been getting sexually excited, I’m not sure. But I can tell that they were very, very excited.\n</p></blockquote>\n<p>It was in the course of that conversation that Parinello mentioned the unbought 24-hour transponders, which would let Rasmussen send a signal around the entire United States for less than it could cost him to buy access on those Connecticut cable companies; he bought it the next day, and only then set out to create what would become ESPN.</p>\n<p>In other words, the very idea for ESPN sprung from:</p>\n<ol>\n<li>The fact that RCA had invested massive capital costs in the Satcom I satellite and thus:</li>\n<li>Was selling access to that satellite at a relatively low price, given that said access had zero marginal costs, which meant:</li>\n<li>Rasmussen could leverage that access to reach every home in America, or at least every cable operator, for an even lower price than it cost to reach only the state of Connecticut.</li>\n</ol>\n<p>Massive fixed costs resulting in zero distribution costs and massive scalability on a platform that is inherently indifferent to the data it is distributing might sound familiar: it’s the same economic forces undergirding the Internet, and it speaks to those forces’ power that while they may have made ESPN in the first place, they threaten to destroy it in the long run.</p>\n<h3>ESPN and the Advent of Affiliate Fees</h3>\n<p>The first ESPN broadcast was a year later, on September 7, 1979; in the intervening time Rasmussen had made a deal with the NCAA, which oversaw a host of untelevised sports, to televise the early rounds of the men’s basketball tournament along with several other less popular sports. The other important deal was with Anheuser-Busch, which signed an advertising contract for $1.3 million. The idea was to convince cable distributors around the country to pick up the free ESPN signal, and to make up the cost with advertising; 1.4 million homes had access to that first broadcast.</p>\n<p>In another foreshadowing of the Internet, ESPN soon realized that providing ongoing content monetized with nothing but advertising was good for growth but bad for actually making money; a year later the company reached 6 million homes and had a new deal with Anheuser-Busch that didn’t come close to covering its costs. Rasmussen was also out, as new management sought to rework its deal with the NCAA and, most importantly, the cable operators.</p>\n<p>Then, in 1982, CBS Cable failed, leading Wall Street to question the whole business model; this didn’t affect ESPN, which was still mostly owned by Getty Oil, with ABC as a new investor and partner, but it did affect the cable companies, who saw their stocks plummet. The last thing they needed was for ESPN to go out of business too; Roger Werner, ESPN’s then CEO, told Miller:</p>\n<blockquote><p>\n  We went to the market with this sort of survival pitch essentially as follows: If you come in voluntarily and do a new deal with us, we’ll start your rate at four cents in 1983 or ’84 and then we’ll go to six cents the next year, then eight cents. Either rip up the old contract and have some protection for whatever the term of your new affiliation agreement is going to be, or pay the prevailing rate when your old deal expires. There was the specter that if we were still around—and we intended to be around—we’d be a much more expensive service…</p>\n<p>  Essentially we were saying, guys, if you’re not interested in paying a fee and you’re really not interested in stepping up to the plate in the near term, tell us now and we’ll pull the plug. Nobody really wanted to deal with the idea that they were going to be paying for a product that had been free, but actually my recollection of this is that it was very stress-filled, it was very contentious.\n</p></blockquote>\n<p>It worked. Suddenly ESPN had <em>two</em> business models: advertising and a per-subscriber fee, whether or not they watched ESPN. Andy Brilliant, the then-general counsel told Miller:</p>\n<blockquote><p>\n  At the end of the day, they blinked and agreed to pay us a dime per household. We breathed a massive sigh of relief. It was the first time we actually received validation that our service was worth something to the cable operators. I think that really put us on the map for good.\n</p></blockquote>\n<p>It also changed how ESPN thought about programming. Then-President of ESPN Bill Grimes told Miller:</p>\n<blockquote><p>\n  This was, like, ’83; at that time we had boxing one night and skiing, tennis, and a whole bunch of other stuff on the schedule. We were talking one day about the fact that there was a lot of college basketball becoming available. I said, “You know, we could get basketball six nights a week. Our weekly ratings in prime would really go up.” But Roger [Werner] said, “That’s true, we could probably get a better rating. But they’re only numbers. We’re now in the business of subscriber fees. So what we want is as diverse programming as possible. Even if a program like skiing or auto racing gets a lower rating, there are people who will never watch a basketball game. So we should now think a little bit differently.” This was totally contrary to what I had grown up with in the business — rating, rating, rating. Get the highest ratings we can get. But Roger was right. We didn’t want all our ratings from one thing, because it’s only those hundred people who watch the skiing event that’ll yell like hell if the cable operators ever do decide to drop ESPN. His belief that sacrificing a little bit of ratings to have greater variety was going to create more rabid fans of ESPN was absolutely right.\n</p></blockquote>\n<p>Werner was right: ESPN could raise its affiliate fees, and cable operators that tried to drop them in protest were overrun with complaints, quickly adding the channel back. By 1986 ESPN was charging around 27 cents per subscriber, and then they signed a deal with the NFL, adding a 9 cent surcharge to their fees; cable operators could choose to not show the game (and avoid the surcharge), but within weeks nearly every cable operator realized their customers would not tolerate not having access to the NFL. George Bodenheimer, who would later become President of ESPN, told Miller this anecdote about the surcharge:</p>\n<blockquote><p>\n  We set a deadline and we told everybody there was a benefit to committing to us then, but those who didn’t sign by midnight of the deadline date would pay a higher price. I remember pleading with one particular cable operator who was my account who said he wasn’t going to agree to sign on. His name was Leonard Tow.\n</p></blockquote>\n<p>Tow was in the process of building what is now known as Frontier Communications; Grimes picked up the story:</p>\n<blockquote><p>\n  Leonard comes in, and you know what the first thing he says about the deal was? “We can’t afford to do this.” I said, “People not seeing the games aren’t going to like it.” Leonard said, “I know football’s popular, but we’re already paying you guys a subscriber fee. We’ll just put on some other local programming the night of the game.” I reminded him that if he changed his mind after tomorrow, he would have to pay a 20 percent incremental fee, a premium, but he just kept saying nope. On the way out I said, “Leonard, look, we’re really successful now and we’re going to be more successful in the future. It would be awful not having you a part of this, but I really believe you’re going to wind up changing your mind. Just wait until people find out you won’t have the games.” He disagreed and we said good-bye. One week later, he called and signed on. And, oh yeah, he paid the extra 20 percent.\n</p></blockquote>\n<p>ESPN paid $153 million over three years for those NFL rights; the first broadcast reached 45 million homes, earning the network an incremental $4.05 million/month, just about enough to cover the NFL rights. What was more important is that the NFL attracted new subscribers which paid ESPN’s full fees, which amounted to over $12 million a month. Moreover, ESPN also got rights from the NFL for unlimited access to highlights: that fueled studio shows like <em>NFL Primetime</em>  and <em>SportsCenter</em> that cost very little to produce, yet both attracted large audiences (for advertising), and made the NFL and other sports even more popular. The flywheel was fully engaged.</p>\n<h3>Charter vs. Disney</h3>\n<p>Over the last decade the story of ESPN specifically, Disney more broadly, and cable as a whole has been the slow but steady disintegration of that flywheel, culminating in the current standoff between Charter and Disney. From the <a href=\"https://www.wsj.com/business/media/why-spectrum-customers-cant-watch-the-u-s-open-and-college-football-cfc24af9\">Wall Street Journal</a>:</p>\n<blockquote><p>\n  Charter Communications subscribers are caught in the middle of a philosophical fight between the cable giant and Disney, parent company of ESPN, ABC and several other networks. Disney-owned networks on Thursday went dark for customers of Charter’s Spectrum cable systems, which has nearly 15 million video subscribers across the country including the New York and Los Angeles markets. As a result, sports fans who are Charter subscribers are losing access to college football and the U.S. Open. And the National Football League season is about to begin: ESPN’s “Monday Night Football” starts Sept. 11. Other channels no longer available to Charter include ABC-owned TV stations and cable networks FX, Disney Channel, Freeform and National Geographic.\n</p></blockquote>\n<p>Channels going dark in the midst of an affiliate fee dispute aren’t new: indeed, they were how ESPN managed to extract per-subscriber fees in the first place. And, for 40 years, ESPN usually won, including a standoff with YouTube TV in late 2021; I wrote at the time in <a href=\"https://stratechery.com/2021/the-creator-opportunity-the-value-of-abundance-tv-and-sports-follow-up/\">an Update</a>:</p>\n<blockquote><p>\n  It appears that Disney decisively won its stand-off with Google; YouTube TV dropped Disney channels for about two days, only to come to an agreement that Disney characterized as “fair terms that are consistent with the market”; this strongly suggests that Google saw sufficient cancellations in that two-day window that it caved on its demands to get a lower rate. This further reaffirms just how powerful the ESPN bundle is (and Disney’s bundle generally).\n</p></blockquote>\n<p>When Disney went dark on Charter last week, I initially assumed a similar outcome; then came the Charter investor call the next morning, and <a href=\"https://ir.charter.com/static-files/05f899dd-7ef3-40d8-84c1-f16a7acfe318\">this slide</a>:</p>\n<p><a href=\"https://ir.charter.com/static-files/05f899dd-7ef3-40d8-84c1-f16a7acfe318\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?resize=640%2C478&amp;ssl=1\" alt=\"Charter's investor slide about video\" width=\"640\" height=\"478\" class=\"aligncenter size-full wp-image-11504\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?resize=300%2C224&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?resize=1024%2C765&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?resize=768%2C574&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?resize=844%2C630&amp;ssl=1 844w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>The most important sentence is in the light blue box on the far right: “The video product is no longer a key driver of financial performance.” This is the culmination of a 25-year shift in business model for the cable companies: those initial investments in wires in the ground to provide small communities access to big city TV broadcasts turned out to be very well suited to providing broadband Internet access. Remember the lesson of RCA and ESPN’s founding: the digital transmission of information is inherently indifferent to the data being distributed. In the case of cable the initial use case was digital TV signals, but the exact same cable could also carry packets running the TCP/IP protocol.<sup id=\"rf1-11503\"><a href=\"#fn1-11503\" title=\"Yes, I know I just said “protocol” twice\" rel=\"footnote\">1</a></sup></p>\n<p>Of course for a long time it was very profitable to carry both, along with voice: cable companies offered “triple play” bundles that included TV, Internet, and telephony. Over time the telephony part dropped off, as people used mobile phones exclusively; cable carriers have since moved into the mobile carrier space as well, fueled by profits from TV and broadband Internet. What made the Internet part the most valuable, though, is that the cable companies didn’t need to pay for content: everything was just a packet.</p>\n<p>That, though, was also the problem: some of those packets reformed themselves as Netflix video streams, which ate into time spent watching TV. Worse, Netflix’s stock was rising and rising as it acquired ever more customers, much to the chagrin of Hollywood, which felt entitled to those multiples given they were the ones producing the most compelling content. That resulted in the fateful decision to start their own streaming services, impoverishing the TV bundle; Charter’s investor presentation included “The Impoverishment Cycle” created by MoffettNathanson:</p>\n<p><a href=\"https://ir.charter.com/static-files/05f899dd-7ef3-40d8-84c1-f16a7acfe318\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?resize=640%2C479&amp;ssl=1\" alt=\"&quot;The Impoverishment Cycle&quot; from MoffettNathanson, via Charter\" width=\"640\" height=\"479\" class=\"aligncenter size-full wp-image-11505\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?resize=300%2C224&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?resize=1024%2C766&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?resize=768%2C574&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?resize=843%2C630&amp;ssl=1 843w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>What Disney and all of the rest forgot was the lesson first imparted by Werner at the dawn of affiliate fees: retaining customers means offering content for everyone; in the case of the cable bundle, that meant having compelling programming above-and-beyond sports.</p>\n<p>The second lesson Disney forgot was why that NFL deal made sense for ESPN at the time, even though the surcharge ESPN charged cable providers was only projected to barely cover the deal: high end sports deals drove customer demand, but the real money was made on (1) everyone who didn’t care about football and (2) cheap content like SportsCenter. The latter, though, has also been impoverished by the Internet; I noted last year <a href=\"https://stratechery.com/2022/house-of-the-dragon-and-creative-programming-the-big-tens-deal-disneys-new-prices/\">when the Big Ten signed a TV deal that excluded ESPN</a>:</p>\n<blockquote><p>\n  The Big Ten’s exclusion of ESPN really highlights the degree to which social media has supplanted ESPN’s previous tentpole shows like SportsCenter; ESPN used to get discounts on rights deals because to be excluded from SportsCenter meant publicity death. That’s no longer the case.\n</p></blockquote>\n<p>The former, meanwhile, is a reminder that while ESPN has generally made money from rights deals, particularly for smaller sports that filled the schedule and inspired niche fans to badger the cable companies, the biggest properties — particularly the NFL — have always been cognizant of their worth and willing to extract their full value. Disney, in turn, can only maintain ESPN profitability by passing on those rights fees to cable distributors, who must in turn pass them on to their customers.</p>\n<p>The third lesson Disney has forgotten is the most counter-intuitive takeaway of this battle: the worst thing that has happened to the company’s negotiating position is that ESPN is already available on the Internet.</p>\n<h3>The Phases of Cable TV</h3>\n<p>The cable TV industry has gone through four distinct phases in terms of competition:</p>\n<p><strong>Phase 1: Non-Consumption</strong></p>\n<p>The first phase was the time in ESPN’s history I detailed above: burgeoning cable TV services were running cables to every home in America and trying to convince customers to sign-up. In this case their competition was non-consumption: a lot of people didn’t have cable, and the cable companies wanted them to sign up for service. ESPN was a particularly unique asset in this regard thanks to its provision of sports content that wasn’t available elsewhere — indeed, until the NFL deal, most of the content had never been available at all. This certainly led to some bruising fights between ESPN and the cable companies over affiliate fees, but it’s always easier to come to an agreement when the pie is growing.</p>\n<p><strong>Phase 2: Satellite</strong></p>\n<p>The second phase was the 90s emergence of DirecTV and Dish Network, which offered the same channels as cable TV but via a small satellite dish you could mount on your roof or porch. This was a more involved installation process, but ultimately cheaper thanks to the fact that DirecTV and Dish didn’t need to put an actual cable in the ground. This was also good news for ESPN because now there was an alternative to traditional cable TV: if a cable provider didn’t want to accept higher affiliate fees then ESPN could withhold service, trusting its viewers would punish the cable provider by moving to satellite (which means they would probably be gone forever).</p>\n<p><strong>Phase 3: IPTV</strong></p>\n<p>By the 2000s the satellite threat to cable was fading because satellite was TV only: if a customer had both Internet and TV via their cable provider than it was much harder to switch. Remember, though, that it’s all data in the end; thus the 2000s saw the rise of IPTV offerings from traditional telecom providers like AT&amp;T and Verizon. They too saw salvation for their own fading telephony business in providing broadband Internet, but providing a competitive offering to cable meant offering TV as well. And, thanks to the Internet, they could simply provide said TV using the TCP/IP protocol.</p>\n<p>The decade that followed was probably the time of maximum ESPN leverage: it was easier for customers to switch from the cable bundle to the telecom bundle than it was to install an extra satellite dish; it’s no surprise that this was the decade when ESPN’s aggressiveness in terms of both acquiring sports rights and in raising affiliate fees increased; it was also the peak of ESPN’s relative share of Disney profits.</p>\n<p><strong>Phase 4: vMVPDs</strong></p>\n<p>The virtual multichannel video programming distributor (vMVPD) era kicked off in 2015 with <a href=\"https://stratechery.com/2015/daily-update-sling-tv-interview-makes-15-million-rokus-smart-tv-play/\">the launch of Sling TV</a>. This took the IPTV trend in Phase 3 to its logical endpoint: instead of needing a box to display IPTV signals, you could simply use an app. vMVPD’s have had a big impact on the landscape in two ways: first, they significantly diminished the cord-cutting trend for years as they both captured cord-cutters and also non-consumers, and second, <a href=\"https://stratechery.com/2023/the-phoenix-suns-go-over-the-air-fans-and-franchise-valuation-attention-and-customer-acquisition/\">they decimated regional sports networks</a> that had long increased affiliate fees even more aggressively than ESPN. I wrote in <a href=\"https://stratechery.com/2023/what-the-nba-can-learn-from-formula-1/\">What the NBA Can Learn From Formula 1</a>:</p>\n<blockquote><p>\n  There just aren’t that many SuperFans of a single team, yet regional networks cost more than anything outside of ESPN — more in some markets. This worked in a world where everyone got cable by default, but remember that cable is losing far more customers than pay-TV as a whole, thanks to the rise of the aforementioned virtual pay-TV providers. Virtual pay-TV providers don’t have a customer base to defend, or infrastructure costs to leverage: they distribute via the Internet that people already pay for. To that end, they don’t have to carry everything, and regional sports networks were the most obvious thing to drop: this lets virtual pay-TV providers have a lower price than cable by virtue of excluding content that most people don’t want.\n</p></blockquote>\n<p>Still, this didn’t seem to affect ESPN, as exemplified by the fact they appear to have won their negotiation with YouTube TV in 2021. In fact, though, this dispute with Charter is showing why ESPN may be a loser as well. Go back to the issue of cable customer churn in response to ESPN’s lack of availability; here’s how it manifested in each phase:</p>\n<ul>\n<li>In Phase 1, a churned customer meant less leverage on expensive buildouts, and pressure from Wall Street.</li>\n<li>In Phase 2, a churned customer went to the effort of getting satellite and probably never came back.</li>\n<li>In Phase 3, a churned customer would not just change their TV provider, but also their broadband provider, and remember that broadband was becoming the cable companies’ biggest business.</li>\n</ul>\n<p>In Phase 4, though, a churned TV customer is still a broadband customer, because the Internet is a precondition for watching the vMVPD! Sure, a customer could be so incensed that they also change their Internet provider, but that is completely unnecessary and, given the inconvenience involved, highly unlikely.</p>\n<p>That means that ESPN, for the first time in its history, has no leverage over the cable companies. Indeed, <a href=\"https://www.moffettnathanson.com/\">MoffettNathanson</a> reported that Charter is actively helping customers move to vMVPDs:</p>\n<blockquote><p>\n  For Charter, the uncomfortable truth is that it just doesn’t matter all that much. Yes, they probably do still make some money on video. But not much, and they recognize that linear video is going to be a rapidly declining line of service under even the most optimistic scenarios, so the issue is arguably nothing more than when, not if, video goes away. Charter has already established a referral capability for customers to switch them to YouTube TV or FuboTV (predictably, they haven’t mentioned referring customers to Sling TV or DirecIV Now, and they presumably wouldn’t steer anyone to Hulu Live if the trigger was a dispute with Disney).</p>\n<p>  Notably, the first NFL Monday Night Football game (ESPN) features two Spectrum-market teams; the New York Jets and the Buffalo Bills. To handle a potential rush of customers anxious about missing the game, Charter is preparing a one-touch QR code that would not only create a new YouTube TV or Fubo subscription, but would also downgrade from a Spectrum video bundle with a single click…Disney may learn the hard way that it’s tough to win a negotiation with a counterparty that has nothing to lose.\n</p></blockquote>\n<p>This truth may be uncomfortable for Charter; it ought to be sobering for Disney, particularly since the company, along with the rest of Hollywood, was the one responsible for destroying the value of TV to companies like Charter who were built on it.</p>\n<h3>The Case for Re-Bundling</h3>\n<p>Once-and-current Disney CEO Bob Iger has been <a href=\"https://stratechery.com/2023/bob-iger-on-cnbc-the-end-of-linear-tv-espn-and-strategic-partnerships-and-apple/\">talking a lot recently</a> about ESPN’s inevitable shift to going over-the-top, including stating that he has a particular date in mind; this showdown with Charter and the revelation of ESPN’s dramatic diminishment in its negotiating position is a reminder that declining businesses often don’t have the luxury of dictating their future.</p>\n<p>So what does that future look like?</p>\n<p>First, it’s very possible — perhaps even likely — that Charter and Disney come to an agreement. As the MoffettNathanson note observes, Charter probably still is making some money on video, and it is also both a customer acquisition tool and churn mitigation factor for their broadband business, and a part of the modern triple play bundle (with mobile). Disney, meanwhile, along with all of the other Hollywood studios, still needs the substantial amount of cash that they receive from cable TV providers (this is particularly pressing for Disney given that they still have to pay for sports rights). Yes, they also earn money from vMVPDs like YouTube TV, but not every customer will seamlessly transition.</p>\n<p>To that end, the company that ought to give here is Disney: according to that Wall Street Journal article Charter is willing to accept the reported $1.50 increase in affiliate fees Disney is demanding if they receive the right to bundle the ad-supported versions of Disney+. Charter argues that it is only right that Disney re-add its most valuable entertainment content to the pay-TV bundle, and frankly, I think they have a point.</p>\n<p>More importantly for Disney, though, is that cable TV providers like Charter remain potent go-to-market entities — decades of servicing customers in their homes has meant a massive build-up in everything from stores to local sales to customer support — and that could be very helpful as Disney seeks to acquire more marginal customers. More importantly, though, I think it is in the long-term interest of the streaming services to be part of a bundle. I wrote last year in <a href=\"https://stratechery.com/2022/cables-last-laugh/\">Cable’s Last Laugh</a>:</p>\n<blockquote><p>\n  The cable companies are better suited than almost anyone else to rebundle for real. Imagine a “streaming bundle” that includes Netflix, HBO Max, Disney+, Paramount+, Peacock, etc., available for a price that is less than the sum of its parts…Owning the customer may be less important than simply having more customers, particularly if those customers are much less likely to churn. After all, that’s one of the advantages of a bundle: instead of your streaming service needing to produce compelling content every single month, you can work as a team to keep customers on board with the bundle.\n</p></blockquote>\n<p>What Charter is proposing is a bit different — they want to bundle traditional TV with streaming services — and I get why Disney is resistant: there are a lot of people paying for both traditional TV and Disney+ (and Hulu and ESPN+); giving Charter bundling rights would cannibalize some amount of revenue. Moreover, it would also mean the end of whatever grand plans Disney might have about offering its own bundle, or cutting out the cable companies’ margin once-and-for-all. At some point, though, Disney and everyone else in Hollywood has to wake up to reality; I wrote in <a href=\"https://stratechery.com/2023/hollywood-on-strike/\">Hollywood on Strike</a>:</p>\n<blockquote><p>\n  The broader issue is that the video industry finally seems to be facing what happened to the print and music industry before them: the Internet comes bearing gifts like infinite capacity and free distribution, but those gifts are a poisoned chalice for industries predicated on scarcity. When anyone could publish text, most text-based businesses went from massive profitability to terminal decline; when anyone could distribute music the music industry could only be saved by tech companies like Spotify helping them sell convenience in place of plastic discs.</p>\n<p>  For the video industry the first step to survival must be to retreat to what they are good at — producing content that isn’t available anywhere else — and getting away from what they are not, i.e. running undifferentiated streaming services with massive direct costs and even larger opportunity ones. Talent, meanwhile, has to realize that they and the studios are not divided by this new paradigm, but jointly threatened: the Internet is bad news for content producers with outsized costs, and long-term sustainability will be that much harder to achieve if the focus is on increasing them.\n</p></blockquote>\n<p>Re-bundling is better for everyone; it’s Disney’s fault that the entities best-placed to pull that off no longer need it.</p>\n<p>Second, for all of the talk about ESPN, it’s worth noting that its content is still valuable — that’s the entire reason this dispute is a big deal. Will anyone care if Charter stops carrying channels from anyone else in Hollywood? And yet, all of those studios are just as dependent on cable TV cashflow, even as many of them have “cheated” to a much greater extent than Disney: Peacock, for example, carries most of NBC’s sports programming, including football, and even put some of the most attractive Olympics programming exclusively on the streaming service. Why on earth should Charter or any other cable provider pay for NBCUniversal channels? Or, more pertinently, if ESPN isn’t available, why would any of the dwindling number of subscribers stay?</p>\n<p>The biggest long-term question, though, has to be around sports itself. Sports leagues could extract ever higher rights fees from ESPN because ESPN could extract ever higher affiliate fees from cable TV providers; if the latter is broken than the former is as well. Yes, vMVPDs like YouTube TV will still exist — and be big winners — and Disney still plans an ESPN streaming service. All of those options, though, entail dramatically increased customer choice; leagues like the NBA have shrugged off declining ratings with the certainty that they would, via cable TV subscribers, get paid regardless, but now the choice isn’t just whether to click the remote, but whether to simply click cancel and watch something else. Better to re-bundle sooner rather than later!</p>\n<hr class=\"footnotes\"><ol class=\"footnotes\" style=\"list-style-type:decimal\"><li id=\"fn1-11503\"><p>Yes, I know I just said “protocol” twice&nbsp;<a href=\"#rf1-11503\" class=\"backlink\" title=\"Return to footnote 1.\">↩</a></p></li></ol>\n\t\t\t",
      "contentText": "\n\t\tOn December 12, 1975, RCA Corporation launched its Satcom I communications satellite; the primary purpose was to provide long-distance telephone service between Alaska and the continental U.S. RCA had hopes, though, that there might be new uses for its capacity; to that end the company had listed for sale a 24-hour transponder that covered the entire United States, only to discontinue the offering after failing to find a single buyer.\nThree years later Bill Rasmussen, the communications manager for the Hartford Whalers, was let go from his job; he had the idea of doing the same coverage he did for the team, but independently, along with other Connecticut sports, leveraging the then-expanding cable access TV facilities in Connecticut. These facilities existed to capture broadcast signals from New York and Boston using large antennas and deliver them to people’s houses; the cables, though, had capacity to carry more channels at basically zero cost, including Rasmussen’s proposed Connecticut sports network.\nIt was in the course of canvasing Connecticut cable providers that Rasmussen was introduced to the concept of satellite communications, and Al Parinello, a manager at RCA. At first Rasmussen pitched his Connecticut sports network idea, and Parinello was confused: satellites covered the entire country, so why was Rasmussen only talking about a single state? Parinello told James Andrew Miller in Those Guys Have All The Fun:\n\n  I can still remember the conversation. Bill said, “Let me get this straight. You mean to tell me, for no extra money — for no extra money! — we could take this signal and beam it anywhere in the country?” And I said, “That’s right.” And then he asked again, “Anywhere in the country?” And I said, “Anywhere.” I remember we went back and forth like this a couple times. Bill and Scott were looking at each other, and they might have been getting sexually excited, I’m not sure. But I can tell that they were very, very excited.\n\nIt was in the course of that conversation that Parinello mentioned the unbought 24-hour transponders, which would let Rasmussen send a signal around the entire United States for less than it could cost him to buy access on those Connecticut cable companies; he bought it the next day, and only then set out to create what would become ESPN.\nIn other words, the very idea for ESPN sprung from:\n\nThe fact that RCA had invested massive capital costs in the Satcom I satellite and thus:\nWas selling access to that satellite at a relatively low price, given that said access had zero marginal costs, which meant:\nRasmussen could leverage that access to reach every home in America, or at least every cable operator, for an even lower price than it cost to reach only the state of Connecticut.\n\nMassive fixed costs resulting in zero distribution costs and massive scalability on a platform that is inherently indifferent to the data it is distributing might sound familiar: it’s the same economic forces undergirding the Internet, and it speaks to those forces’ power that while they may have made ESPN in the first place, they threaten to destroy it in the long run.\nESPN and the Advent of Affiliate Fees\nThe first ESPN broadcast was a year later, on September 7, 1979; in the intervening time Rasmussen had made a deal with the NCAA, which oversaw a host of untelevised sports, to televise the early rounds of the men’s basketball tournament along with several other less popular sports. The other important deal was with Anheuser-Busch, which signed an advertising contract for $1.3 million. The idea was to convince cable distributors around the country to pick up the free ESPN signal, and to make up the cost with advertising; 1.4 million homes had access to that first broadcast.\nIn another foreshadowing of the Internet, ESPN soon realized that providing ongoing content monetized with nothing but advertising was good for growth but bad for actually making money; a year later the company reached 6 million homes and had a new deal with Anheuser-Busch that didn’t come close to covering its costs. Rasmussen was also out, as new management sought to rework its deal with the NCAA and, most importantly, the cable operators.\nThen, in 1982, CBS Cable failed, leading Wall Street to question the whole business model; this didn’t affect ESPN, which was still mostly owned by Getty Oil, with ABC as a new investor and partner, but it did affect the cable companies, who saw their stocks plummet. The last thing they needed was for ESPN to go out of business too; Roger Werner, ESPN’s then CEO, told Miller:\n\n  We went to the market with this sort of survival pitch essentially as follows: If you come in voluntarily and do a new deal with us, we’ll start your rate at four cents in 1983 or ’84 and then we’ll go to six cents the next year, then eight cents. Either rip up the old contract and have some protection for whatever the term of your new affiliation agreement is going to be, or pay the prevailing rate when your old deal expires. There was the specter that if we were still around—and we intended to be around—we’d be a much more expensive service…\n  Essentially we were saying, guys, if you’re not interested in paying a fee and you’re really not interested in stepping up to the plate in the near term, tell us now and we’ll pull the plug. Nobody really wanted to deal with the idea that they were going to be paying for a product that had been free, but actually my recollection of this is that it was very stress-filled, it was very contentious.\n\nIt worked. Suddenly ESPN had two business models: advertising and a per-subscriber fee, whether or not they watched ESPN. Andy Brilliant, the then-general counsel told Miller:\n\n  At the end of the day, they blinked and agreed to pay us a dime per household. We breathed a massive sigh of relief. It was the first time we actually received validation that our service was worth something to the cable operators. I think that really put us on the map for good.\n\nIt also changed how ESPN thought about programming. Then-President of ESPN Bill Grimes told Miller:\n\n  This was, like, ’83; at that time we had boxing one night and skiing, tennis, and a whole bunch of other stuff on the schedule. We were talking one day about the fact that there was a lot of college basketball becoming available. I said, “You know, we could get basketball six nights a week. Our weekly ratings in prime would really go up.” But Roger [Werner] said, “That’s true, we could probably get a better rating. But they’re only numbers. We’re now in the business of subscriber fees. So what we want is as diverse programming as possible. Even if a program like skiing or auto racing gets a lower rating, there are people who will never watch a basketball game. So we should now think a little bit differently.” This was totally contrary to what I had grown up with in the business — rating, rating, rating. Get the highest ratings we can get. But Roger was right. We didn’t want all our ratings from one thing, because it’s only those hundred people who watch the skiing event that’ll yell like hell if the cable operators ever do decide to drop ESPN. His belief that sacrificing a little bit of ratings to have greater variety was going to create more rabid fans of ESPN was absolutely right.\n\nWerner was right: ESPN could raise its affiliate fees, and cable operators that tried to drop them in protest were overrun with complaints, quickly adding the channel back. By 1986 ESPN was charging around 27 cents per subscriber, and then they signed a deal with the NFL, adding a 9 cent surcharge to their fees; cable operators could choose to not show the game (and avoid the surcharge), but within weeks nearly every cable operator realized their customers would not tolerate not having access to the NFL. George Bodenheimer, who would later become President of ESPN, told Miller this anecdote about the surcharge:\n\n  We set a deadline and we told everybody there was a benefit to committing to us then, but those who didn’t sign by midnight of the deadline date would pay a higher price. I remember pleading with one particular cable operator who was my account who said he wasn’t going to agree to sign on. His name was Leonard Tow.\n\nTow was in the process of building what is now known as Frontier Communications; Grimes picked up the story:\n\n  Leonard comes in, and you know what the first thing he says about the deal was? “We can’t afford to do this.” I said, “People not seeing the games aren’t going to like it.” Leonard said, “I know football’s popular, but we’re already paying you guys a subscriber fee. We’ll just put on some other local programming the night of the game.” I reminded him that if he changed his mind after tomorrow, he would have to pay a 20 percent incremental fee, a premium, but he just kept saying nope. On the way out I said, “Leonard, look, we’re really successful now and we’re going to be more successful in the future. It would be awful not having you a part of this, but I really believe you’re going to wind up changing your mind. Just wait until people find out you won’t have the games.” He disagreed and we said good-bye. One week later, he called and signed on. And, oh yeah, he paid the extra 20 percent.\n\nESPN paid $153 million over three years for those NFL rights; the first broadcast reached 45 million homes, earning the network an incremental $4.05 million/month, just about enough to cover the NFL rights. What was more important is that the NFL attracted new subscribers which paid ESPN’s full fees, which amounted to over $12 million a month. Moreover, ESPN also got rights from the NFL for unlimited access to highlights: that fueled studio shows like NFL Primetime  and SportsCenter that cost very little to produce, yet both attracted large audiences (for advertising), and made the NFL and other sports even more popular. The flywheel was fully engaged.\nCharter vs. Disney\nOver the last decade the story of ESPN specifically, Disney more broadly, and cable as a whole has been the slow but steady disintegration of that flywheel, culminating in the current standoff between Charter and Disney. From the Wall Street Journal:\n\n  Charter Communications subscribers are caught in the middle of a philosophical fight between the cable giant and Disney, parent company of ESPN, ABC and several other networks. Disney-owned networks on Thursday went dark for customers of Charter’s Spectrum cable systems, which has nearly 15 million video subscribers across the country including the New York and Los Angeles markets. As a result, sports fans who are Charter subscribers are losing access to college football and the U.S. Open. And the National Football League season is about to begin: ESPN’s “Monday Night Football” starts Sept. 11. Other channels no longer available to Charter include ABC-owned TV stations and cable networks FX, Disney Channel, Freeform and National Geographic.\n\nChannels going dark in the midst of an affiliate fee dispute aren’t new: indeed, they were how ESPN managed to extract per-subscriber fees in the first place. And, for 40 years, ESPN usually won, including a standoff with YouTube TV in late 2021; I wrote at the time in an Update:\n\n  It appears that Disney decisively won its stand-off with Google; YouTube TV dropped Disney channels for about two days, only to come to an agreement that Disney characterized as “fair terms that are consistent with the market”; this strongly suggests that Google saw sufficient cancellations in that two-day window that it caved on its demands to get a lower rate. This further reaffirms just how powerful the ESPN bundle is (and Disney’s bundle generally).\n\nWhen Disney went dark on Charter last week, I initially assumed a similar outcome; then came the Charter investor call the next morning, and this slide:\n\nThe most important sentence is in the light blue box on the far right: “The video product is no longer a key driver of financial performance.” This is the culmination of a 25-year shift in business model for the cable companies: those initial investments in wires in the ground to provide small communities access to big city TV broadcasts turned out to be very well suited to providing broadband Internet access. Remember the lesson of RCA and ESPN’s founding: the digital transmission of information is inherently indifferent to the data being distributed. In the case of cable the initial use case was digital TV signals, but the exact same cable could also carry packets running the TCP/IP protocol.1\nOf course for a long time it was very profitable to carry both, along with voice: cable companies offered “triple play” bundles that included TV, Internet, and telephony. Over time the telephony part dropped off, as people used mobile phones exclusively; cable carriers have since moved into the mobile carrier space as well, fueled by profits from TV and broadband Internet. What made the Internet part the most valuable, though, is that the cable companies didn’t need to pay for content: everything was just a packet.\nThat, though, was also the problem: some of those packets reformed themselves as Netflix video streams, which ate into time spent watching TV. Worse, Netflix’s stock was rising and rising as it acquired ever more customers, much to the chagrin of Hollywood, which felt entitled to those multiples given they were the ones producing the most compelling content. That resulted in the fateful decision to start their own streaming services, impoverishing the TV bundle; Charter’s investor presentation included “The Impoverishment Cycle” created by MoffettNathanson:\n\nWhat Disney and all of the rest forgot was the lesson first imparted by Werner at the dawn of affiliate fees: retaining customers means offering content for everyone; in the case of the cable bundle, that meant having compelling programming above-and-beyond sports.\nThe second lesson Disney forgot was why that NFL deal made sense for ESPN at the time, even though the surcharge ESPN charged cable providers was only projected to barely cover the deal: high end sports deals drove customer demand, but the real money was made on (1) everyone who didn’t care about football and (2) cheap content like SportsCenter. The latter, though, has also been impoverished by the Internet; I noted last year when the Big Ten signed a TV deal that excluded ESPN:\n\n  The Big Ten’s exclusion of ESPN really highlights the degree to which social media has supplanted ESPN’s previous tentpole shows like SportsCenter; ESPN used to get discounts on rights deals because to be excluded from SportsCenter meant publicity death. That’s no longer the case.\n\nThe former, meanwhile, is a reminder that while ESPN has generally made money from rights deals, particularly for smaller sports that filled the schedule and inspired niche fans to badger the cable companies, the biggest properties — particularly the NFL — have always been cognizant of their worth and willing to extract their full value. Disney, in turn, can only maintain ESPN profitability by passing on those rights fees to cable distributors, who must in turn pass them on to their customers.\nThe third lesson Disney has forgotten is the most counter-intuitive takeaway of this battle: the worst thing that has happened to the company’s negotiating position is that ESPN is already available on the Internet.\nThe Phases of Cable TV\nThe cable TV industry has gone through four distinct phases in terms of competition:\nPhase 1: Non-Consumption\nThe first phase was the time in ESPN’s history I detailed above: burgeoning cable TV services were running cables to every home in America and trying to convince customers to sign-up. In this case their competition was non-consumption: a lot of people didn’t have cable, and the cable companies wanted them to sign up for service. ESPN was a particularly unique asset in this regard thanks to its provision of sports content that wasn’t available elsewhere — indeed, until the NFL deal, most of the content had never been available at all. This certainly led to some bruising fights between ESPN and the cable companies over affiliate fees, but it’s always easier to come to an agreement when the pie is growing.\nPhase 2: Satellite\nThe second phase was the 90s emergence of DirecTV and Dish Network, which offered the same channels as cable TV but via a small satellite dish you could mount on your roof or porch. This was a more involved installation process, but ultimately cheaper thanks to the fact that DirecTV and Dish didn’t need to put an actual cable in the ground. This was also good news for ESPN because now there was an alternative to traditional cable TV: if a cable provider didn’t want to accept higher affiliate fees then ESPN could withhold service, trusting its viewers would punish the cable provider by moving to satellite (which means they would probably be gone forever).\nPhase 3: IPTV\nBy the 2000s the satellite threat to cable was fading because satellite was TV only: if a customer had both Internet and TV via their cable provider than it was much harder to switch. Remember, though, that it’s all data in the end; thus the 2000s saw the rise of IPTV offerings from traditional telecom providers like AT&T and Verizon. They too saw salvation for their own fading telephony business in providing broadband Internet, but providing a competitive offering to cable meant offering TV as well. And, thanks to the Internet, they could simply provide said TV using the TCP/IP protocol.\nThe decade that followed was probably the time of maximum ESPN leverage: it was easier for customers to switch from the cable bundle to the telecom bundle than it was to install an extra satellite dish; it’s no surprise that this was the decade when ESPN’s aggressiveness in terms of both acquiring sports rights and in raising affiliate fees increased; it was also the peak of ESPN’s relative share of Disney profits.\nPhase 4: vMVPDs\nThe virtual multichannel video programming distributor (vMVPD) era kicked off in 2015 with the launch of Sling TV. This took the IPTV trend in Phase 3 to its logical endpoint: instead of needing a box to display IPTV signals, you could simply use an app. vMVPD’s have had a big impact on the landscape in two ways: first, they significantly diminished the cord-cutting trend for years as they both captured cord-cutters and also non-consumers, and second, they decimated regional sports networks that had long increased affiliate fees even more aggressively than ESPN. I wrote in What the NBA Can Learn From Formula 1:\n\n  There just aren’t that many SuperFans of a single team, yet regional networks cost more than anything outside of ESPN — more in some markets. This worked in a world where everyone got cable by default, but remember that cable is losing far more customers than pay-TV as a whole, thanks to the rise of the aforementioned virtual pay-TV providers. Virtual pay-TV providers don’t have a customer base to defend, or infrastructure costs to leverage: they distribute via the Internet that people already pay for. To that end, they don’t have to carry everything, and regional sports networks were the most obvious thing to drop: this lets virtual pay-TV providers have a lower price than cable by virtue of excluding content that most people don’t want.\n\nStill, this didn’t seem to affect ESPN, as exemplified by the fact they appear to have won their negotiation with YouTube TV in 2021. In fact, though, this dispute with Charter is showing why ESPN may be a loser as well. Go back to the issue of cable customer churn in response to ESPN’s lack of availability; here’s how it manifested in each phase:\n\nIn Phase 1, a churned customer meant less leverage on expensive buildouts, and pressure from Wall Street.\nIn Phase 2, a churned customer went to the effort of getting satellite and probably never came back.\nIn Phase 3, a churned customer would not just change their TV provider, but also their broadband provider, and remember that broadband was becoming the cable companies’ biggest business.\n\nIn Phase 4, though, a churned TV customer is still a broadband customer, because the Internet is a precondition for watching the vMVPD! Sure, a customer could be so incensed that they also change their Internet provider, but that is completely unnecessary and, given the inconvenience involved, highly unlikely.\nThat means that ESPN, for the first time in its history, has no leverage over the cable companies. Indeed, MoffettNathanson reported that Charter is actively helping customers move to vMVPDs:\n\n  For Charter, the uncomfortable truth is that it just doesn’t matter all that much. Yes, they probably do still make some money on video. But not much, and they recognize that linear video is going to be a rapidly declining line of service under even the most optimistic scenarios, so the issue is arguably nothing more than when, not if, video goes away. Charter has already established a referral capability for customers to switch them to YouTube TV or FuboTV (predictably, they haven’t mentioned referring customers to Sling TV or DirecIV Now, and they presumably wouldn’t steer anyone to Hulu Live if the trigger was a dispute with Disney).\n  Notably, the first NFL Monday Night Football game (ESPN) features two Spectrum-market teams; the New York Jets and the Buffalo Bills. To handle a potential rush of customers anxious about missing the game, Charter is preparing a one-touch QR code that would not only create a new YouTube TV or Fubo subscription, but would also downgrade from a Spectrum video bundle with a single click…Disney may learn the hard way that it’s tough to win a negotiation with a counterparty that has nothing to lose.\n\nThis truth may be uncomfortable for Charter; it ought to be sobering for Disney, particularly since the company, along with the rest of Hollywood, was the one responsible for destroying the value of TV to companies like Charter who were built on it.\nThe Case for Re-Bundling\nOnce-and-current Disney CEO Bob Iger has been talking a lot recently about ESPN’s inevitable shift to going over-the-top, including stating that he has a particular date in mind; this showdown with Charter and the revelation of ESPN’s dramatic diminishment in its negotiating position is a reminder that declining businesses often don’t have the luxury of dictating their future.\nSo what does that future look like?\nFirst, it’s very possible — perhaps even likely — that Charter and Disney come to an agreement. As the MoffettNathanson note observes, Charter probably still is making some money on video, and it is also both a customer acquisition tool and churn mitigation factor for their broadband business, and a part of the modern triple play bundle (with mobile). Disney, meanwhile, along with all of the other Hollywood studios, still needs the substantial amount of cash that they receive from cable TV providers (this is particularly pressing for Disney given that they still have to pay for sports rights). Yes, they also earn money from vMVPDs like YouTube TV, but not every customer will seamlessly transition.\nTo that end, the company that ought to give here is Disney: according to that Wall Street Journal article Charter is willing to accept the reported $1.50 increase in affiliate fees Disney is demanding if they receive the right to bundle the ad-supported versions of Disney+. Charter argues that it is only right that Disney re-add its most valuable entertainment content to the pay-TV bundle, and frankly, I think they have a point.\nMore importantly for Disney, though, is that cable TV providers like Charter remain potent go-to-market entities — decades of servicing customers in their homes has meant a massive build-up in everything from stores to local sales to customer support — and that could be very helpful as Disney seeks to acquire more marginal customers. More importantly, though, I think it is in the long-term interest of the streaming services to be part of a bundle. I wrote last year in Cable’s Last Laugh:\n\n  The cable companies are better suited than almost anyone else to rebundle for real. Imagine a “streaming bundle” that includes Netflix, HBO Max, Disney+, Paramount+, Peacock, etc., available for a price that is less than the sum of its parts…Owning the customer may be less important than simply having more customers, particularly if those customers are much less likely to churn. After all, that’s one of the advantages of a bundle: instead of your streaming service needing to produce compelling content every single month, you can work as a team to keep customers on board with the bundle.\n\nWhat Charter is proposing is a bit different — they want to bundle traditional TV with streaming services — and I get why Disney is resistant: there are a lot of people paying for both traditional TV and Disney+ (and Hulu and ESPN+); giving Charter bundling rights would cannibalize some amount of revenue. Moreover, it would also mean the end of whatever grand plans Disney might have about offering its own bundle, or cutting out the cable companies’ margin once-and-for-all. At some point, though, Disney and everyone else in Hollywood has to wake up to reality; I wrote in Hollywood on Strike:\n\n  The broader issue is that the video industry finally seems to be facing what happened to the print and music industry before them: the Internet comes bearing gifts like infinite capacity and free distribution, but those gifts are a poisoned chalice for industries predicated on scarcity. When anyone could publish text, most text-based businesses went from massive profitability to terminal decline; when anyone could distribute music the music industry could only be saved by tech companies like Spotify helping them sell convenience in place of plastic discs.\n  For the video industry the first step to survival must be to retreat to what they are good at — producing content that isn’t available anywhere else — and getting away from what they are not, i.e. running undifferentiated streaming services with massive direct costs and even larger opportunity ones. Talent, meanwhile, has to realize that they and the studios are not divided by this new paradigm, but jointly threatened: the Internet is bad news for content producers with outsized costs, and long-term sustainability will be that much harder to achieve if the focus is on increasing them.\n\nRe-bundling is better for everyone; it’s Disney’s fault that the entities best-placed to pull that off no longer need it.\nSecond, for all of the talk about ESPN, it’s worth noting that its content is still valuable — that’s the entire reason this dispute is a big deal. Will anyone care if Charter stops carrying channels from anyone else in Hollywood? And yet, all of those studios are just as dependent on cable TV cashflow, even as many of them have “cheated” to a much greater extent than Disney: Peacock, for example, carries most of NBC’s sports programming, including football, and even put some of the most attractive Olympics programming exclusively on the streaming service. Why on earth should Charter or any other cable provider pay for NBCUniversal channels? Or, more pertinently, if ESPN isn’t available, why would any of the dwindling number of subscribers stay?\nThe biggest long-term question, though, has to be around sports itself. Sports leagues could extract ever higher rights fees from ESPN because ESPN could extract ever higher affiliate fees from cable TV providers; if the latter is broken than the former is as well. Yes, vMVPDs like YouTube TV will still exist — and be big winners — and Disney still plans an ESPN streaming service. All of those options, though, entail dramatically increased customer choice; leagues like the NBA have shrugged off declining ratings with the certainty that they would, via cable TV subscribers, get paid regardless, but now the choice isn’t just whether to click the remote, but whether to simply click cancel and watch something else. Better to re-bundle sooner rather than later!\nYes, I know I just said “protocol” twice ↩\n\t\t\t",
      "subsections": [
        {
          "subtitle": "ESPN and the Advent of Affiliate Fees",
          "contentHtml": "<p>The first ESPN broadcast was a year later, on September 7, 1979; in the intervening time Rasmussen had made a deal with the NCAA, which oversaw a host of untelevised sports, to televise the early rounds of the men’s basketball tournament along with several other less popular sports. The other important deal was with Anheuser-Busch, which signed an advertising contract for $1.3 million. The idea was to convince cable distributors around the country to pick up the free ESPN signal, and to make up the cost with advertising; 1.4 million homes had access to that first broadcast.</p><p>In another foreshadowing of the Internet, ESPN soon realized that providing ongoing content monetized with nothing but advertising was good for growth but bad for actually making money; a year later the company reached 6 million homes and had a new deal with Anheuser-Busch that didn’t come close to covering its costs. Rasmussen was also out, as new management sought to rework its deal with the NCAA and, most importantly, the cable operators.</p><p>Then, in 1982, CBS Cable failed, leading Wall Street to question the whole business model; this didn’t affect ESPN, which was still mostly owned by Getty Oil, with ABC as a new investor and partner, but it did affect the cable companies, who saw their stocks plummet. The last thing they needed was for ESPN to go out of business too; Roger Werner, ESPN’s then CEO, told Miller:</p><blockquote><p>\n  We went to the market with this sort of survival pitch essentially as follows: If you come in voluntarily and do a new deal with us, we’ll start your rate at four cents in 1983 or ’84 and then we’ll go to six cents the next year, then eight cents. Either rip up the old contract and have some protection for whatever the term of your new affiliation agreement is going to be, or pay the prevailing rate when your old deal expires. There was the specter that if we were still around—and we intended to be around—we’d be a much more expensive service…</p>\n<p>  Essentially we were saying, guys, if you’re not interested in paying a fee and you’re really not interested in stepping up to the plate in the near term, tell us now and we’ll pull the plug. Nobody really wanted to deal with the idea that they were going to be paying for a product that had been free, but actually my recollection of this is that it was very stress-filled, it was very contentious.\n</p></blockquote><p>It worked. Suddenly ESPN had <em>two</em> business models: advertising and a per-subscriber fee, whether or not they watched ESPN. Andy Brilliant, the then-general counsel told Miller:</p><blockquote><p>\n  At the end of the day, they blinked and agreed to pay us a dime per household. We breathed a massive sigh of relief. It was the first time we actually received validation that our service was worth something to the cable operators. I think that really put us on the map for good.\n</p></blockquote><p>It also changed how ESPN thought about programming. Then-President of ESPN Bill Grimes told Miller:</p><blockquote><p>\n  This was, like, ’83; at that time we had boxing one night and skiing, tennis, and a whole bunch of other stuff on the schedule. We were talking one day about the fact that there was a lot of college basketball becoming available. I said, “You know, we could get basketball six nights a week. Our weekly ratings in prime would really go up.” But Roger [Werner] said, “That’s true, we could probably get a better rating. But they’re only numbers. We’re now in the business of subscriber fees. So what we want is as diverse programming as possible. Even if a program like skiing or auto racing gets a lower rating, there are people who will never watch a basketball game. So we should now think a little bit differently.” This was totally contrary to what I had grown up with in the business — rating, rating, rating. Get the highest ratings we can get. But Roger was right. We didn’t want all our ratings from one thing, because it’s only those hundred people who watch the skiing event that’ll yell like hell if the cable operators ever do decide to drop ESPN. His belief that sacrificing a little bit of ratings to have greater variety was going to create more rabid fans of ESPN was absolutely right.\n</p></blockquote><p>Werner was right: ESPN could raise its affiliate fees, and cable operators that tried to drop them in protest were overrun with complaints, quickly adding the channel back. By 1986 ESPN was charging around 27 cents per subscriber, and then they signed a deal with the NFL, adding a 9 cent surcharge to their fees; cable operators could choose to not show the game (and avoid the surcharge), but within weeks nearly every cable operator realized their customers would not tolerate not having access to the NFL. George Bodenheimer, who would later become President of ESPN, told Miller this anecdote about the surcharge:</p><blockquote><p>\n  We set a deadline and we told everybody there was a benefit to committing to us then, but those who didn’t sign by midnight of the deadline date would pay a higher price. I remember pleading with one particular cable operator who was my account who said he wasn’t going to agree to sign on. His name was Leonard Tow.\n</p></blockquote><p>Tow was in the process of building what is now known as Frontier Communications; Grimes picked up the story:</p><blockquote><p>\n  Leonard comes in, and you know what the first thing he says about the deal was? “We can’t afford to do this.” I said, “People not seeing the games aren’t going to like it.” Leonard said, “I know football’s popular, but we’re already paying you guys a subscriber fee. We’ll just put on some other local programming the night of the game.” I reminded him that if he changed his mind after tomorrow, he would have to pay a 20 percent incremental fee, a premium, but he just kept saying nope. On the way out I said, “Leonard, look, we’re really successful now and we’re going to be more successful in the future. It would be awful not having you a part of this, but I really believe you’re going to wind up changing your mind. Just wait until people find out you won’t have the games.” He disagreed and we said good-bye. One week later, he called and signed on. And, oh yeah, he paid the extra 20 percent.\n</p></blockquote><p>ESPN paid $153 million over three years for those NFL rights; the first broadcast reached 45 million homes, earning the network an incremental $4.05 million/month, just about enough to cover the NFL rights. What was more important is that the NFL attracted new subscribers which paid ESPN’s full fees, which amounted to over $12 million a month. Moreover, ESPN also got rights from the NFL for unlimited access to highlights: that fueled studio shows like <em>NFL Primetime</em>  and <em>SportsCenter</em> that cost very little to produce, yet both attracted large audiences (for advertising), and made the NFL and other sports even more popular. The flywheel was fully engaged.</p>",
          "contentText": "The first ESPN broadcast was a year later, on September 7, 1979; in the intervening time Rasmussen had made a deal with the NCAA, which oversaw a host of untelevised sports, to televise the early rounds of the men’s basketball tournament along with several other less popular sports. The other important deal was with Anheuser-Busch, which signed an advertising contract for $1.3 million. The idea was to convince cable distributors around the country to pick up the free ESPN signal, and to make up the cost with advertising; 1.4 million homes had access to that first broadcast.In another foreshadowing of the Internet, ESPN soon realized that providing ongoing content monetized with nothing but advertising was good for growth but bad for actually making money; a year later the company reached 6 million homes and had a new deal with Anheuser-Busch that didn’t come close to covering its costs. Rasmussen was also out, as new management sought to rework its deal with the NCAA and, most importantly, the cable operators.Then, in 1982, CBS Cable failed, leading Wall Street to question the whole business model; this didn’t affect ESPN, which was still mostly owned by Getty Oil, with ABC as a new investor and partner, but it did affect the cable companies, who saw their stocks plummet. The last thing they needed was for ESPN to go out of business too; Roger Werner, ESPN’s then CEO, told Miller:\n  We went to the market with this sort of survival pitch essentially as follows: If you come in voluntarily and do a new deal with us, we’ll start your rate at four cents in 1983 or ’84 and then we’ll go to six cents the next year, then eight cents. Either rip up the old contract and have some protection for whatever the term of your new affiliation agreement is going to be, or pay the prevailing rate when your old deal expires. There was the specter that if we were still around—and we intended to be around—we’d be a much more expensive service…\n  Essentially we were saying, guys, if you’re not interested in paying a fee and you’re really not interested in stepping up to the plate in the near term, tell us now and we’ll pull the plug. Nobody really wanted to deal with the idea that they were going to be paying for a product that had been free, but actually my recollection of this is that it was very stress-filled, it was very contentious.\nIt worked. Suddenly ESPN had two business models: advertising and a per-subscriber fee, whether or not they watched ESPN. Andy Brilliant, the then-general counsel told Miller:\n  At the end of the day, they blinked and agreed to pay us a dime per household. We breathed a massive sigh of relief. It was the first time we actually received validation that our service was worth something to the cable operators. I think that really put us on the map for good.\nIt also changed how ESPN thought about programming. Then-President of ESPN Bill Grimes told Miller:\n  This was, like, ’83; at that time we had boxing one night and skiing, tennis, and a whole bunch of other stuff on the schedule. We were talking one day about the fact that there was a lot of college basketball becoming available. I said, “You know, we could get basketball six nights a week. Our weekly ratings in prime would really go up.” But Roger [Werner] said, “That’s true, we could probably get a better rating. But they’re only numbers. We’re now in the business of subscriber fees. So what we want is as diverse programming as possible. Even if a program like skiing or auto racing gets a lower rating, there are people who will never watch a basketball game. So we should now think a little bit differently.” This was totally contrary to what I had grown up with in the business — rating, rating, rating. Get the highest ratings we can get. But Roger was right. We didn’t want all our ratings from one thing, because it’s only those hundred people who watch the skiing event that’ll yell like hell if the cable operators ever do decide to drop ESPN. His belief that sacrificing a little bit of ratings to have greater variety was going to create more rabid fans of ESPN was absolutely right.\nWerner was right: ESPN could raise its affiliate fees, and cable operators that tried to drop them in protest were overrun with complaints, quickly adding the channel back. By 1986 ESPN was charging around 27 cents per subscriber, and then they signed a deal with the NFL, adding a 9 cent surcharge to their fees; cable operators could choose to not show the game (and avoid the surcharge), but within weeks nearly every cable operator realized their customers would not tolerate not having access to the NFL. George Bodenheimer, who would later become President of ESPN, told Miller this anecdote about the surcharge:\n  We set a deadline and we told everybody there was a benefit to committing to us then, but those who didn’t sign by midnight of the deadline date would pay a higher price. I remember pleading with one particular cable operator who was my account who said he wasn’t going to agree to sign on. His name was Leonard Tow.\nTow was in the process of building what is now known as Frontier Communications; Grimes picked up the story:\n  Leonard comes in, and you know what the first thing he says about the deal was? “We can’t afford to do this.” I said, “People not seeing the games aren’t going to like it.” Leonard said, “I know football’s popular, but we’re already paying you guys a subscriber fee. We’ll just put on some other local programming the night of the game.” I reminded him that if he changed his mind after tomorrow, he would have to pay a 20 percent incremental fee, a premium, but he just kept saying nope. On the way out I said, “Leonard, look, we’re really successful now and we’re going to be more successful in the future. It would be awful not having you a part of this, but I really believe you’re going to wind up changing your mind. Just wait until people find out you won’t have the games.” He disagreed and we said good-bye. One week later, he called and signed on. And, oh yeah, he paid the extra 20 percent.\nESPN paid $153 million over three years for those NFL rights; the first broadcast reached 45 million homes, earning the network an incremental $4.05 million/month, just about enough to cover the NFL rights. What was more important is that the NFL attracted new subscribers which paid ESPN’s full fees, which amounted to over $12 million a month. Moreover, ESPN also got rights from the NFL for unlimited access to highlights: that fueled studio shows like NFL Primetime  and SportsCenter that cost very little to produce, yet both attracted large audiences (for advertising), and made the NFL and other sports even more popular. The flywheel was fully engaged."
        },
        {
          "subtitle": "Charter vs. Disney",
          "contentHtml": "<p>Over the last decade the story of ESPN specifically, Disney more broadly, and cable as a whole has been the slow but steady disintegration of that flywheel, culminating in the current standoff between Charter and Disney. From the <a href=\"https://www.wsj.com/business/media/why-spectrum-customers-cant-watch-the-u-s-open-and-college-football-cfc24af9\">Wall Street Journal</a>:</p><blockquote><p>\n  Charter Communications subscribers are caught in the middle of a philosophical fight between the cable giant and Disney, parent company of ESPN, ABC and several other networks. Disney-owned networks on Thursday went dark for customers of Charter’s Spectrum cable systems, which has nearly 15 million video subscribers across the country including the New York and Los Angeles markets. As a result, sports fans who are Charter subscribers are losing access to college football and the U.S. Open. And the National Football League season is about to begin: ESPN’s “Monday Night Football” starts Sept. 11. Other channels no longer available to Charter include ABC-owned TV stations and cable networks FX, Disney Channel, Freeform and National Geographic.\n</p></blockquote><p>Channels going dark in the midst of an affiliate fee dispute aren’t new: indeed, they were how ESPN managed to extract per-subscriber fees in the first place. And, for 40 years, ESPN usually won, including a standoff with YouTube TV in late 2021; I wrote at the time in <a href=\"https://stratechery.com/2021/the-creator-opportunity-the-value-of-abundance-tv-and-sports-follow-up/\">an Update</a>:</p><blockquote><p>\n  It appears that Disney decisively won its stand-off with Google; YouTube TV dropped Disney channels for about two days, only to come to an agreement that Disney characterized as “fair terms that are consistent with the market”; this strongly suggests that Google saw sufficient cancellations in that two-day window that it caved on its demands to get a lower rate. This further reaffirms just how powerful the ESPN bundle is (and Disney’s bundle generally).\n</p></blockquote><p>When Disney went dark on Charter last week, I initially assumed a similar outcome; then came the Charter investor call the next morning, and <a href=\"https://ir.charter.com/static-files/05f899dd-7ef3-40d8-84c1-f16a7acfe318\">this slide</a>:</p><p><a href=\"https://ir.charter.com/static-files/05f899dd-7ef3-40d8-84c1-f16a7acfe318\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?resize=640%2C478&amp;ssl=1\" alt=\"Charter's investor slide about video\" width=\"640\" height=\"478\" class=\"aligncenter size-full wp-image-11504\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?resize=300%2C224&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?resize=1024%2C765&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?resize=768%2C574&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-1.png?resize=844%2C630&amp;ssl=1 844w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p><p>The most important sentence is in the light blue box on the far right: “The video product is no longer a key driver of financial performance.” This is the culmination of a 25-year shift in business model for the cable companies: those initial investments in wires in the ground to provide small communities access to big city TV broadcasts turned out to be very well suited to providing broadband Internet access. Remember the lesson of RCA and ESPN’s founding: the digital transmission of information is inherently indifferent to the data being distributed. In the case of cable the initial use case was digital TV signals, but the exact same cable could also carry packets running the TCP/IP protocol.<sup id=\"rf1-11503\"><a href=\"#fn1-11503\" title=\"Yes, I know I just said “protocol” twice\" rel=\"footnote\">1</a></sup></p><p>Of course for a long time it was very profitable to carry both, along with voice: cable companies offered “triple play” bundles that included TV, Internet, and telephony. Over time the telephony part dropped off, as people used mobile phones exclusively; cable carriers have since moved into the mobile carrier space as well, fueled by profits from TV and broadband Internet. What made the Internet part the most valuable, though, is that the cable companies didn’t need to pay for content: everything was just a packet.</p><p>That, though, was also the problem: some of those packets reformed themselves as Netflix video streams, which ate into time spent watching TV. Worse, Netflix’s stock was rising and rising as it acquired ever more customers, much to the chagrin of Hollywood, which felt entitled to those multiples given they were the ones producing the most compelling content. That resulted in the fateful decision to start their own streaming services, impoverishing the TV bundle; Charter’s investor presentation included “The Impoverishment Cycle” created by MoffettNathanson:</p><p><a href=\"https://ir.charter.com/static-files/05f899dd-7ef3-40d8-84c1-f16a7acfe318\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?resize=640%2C479&amp;ssl=1\" alt=\"&quot;The Impoverishment Cycle&quot; from MoffettNathanson, via Charter\" width=\"640\" height=\"479\" class=\"aligncenter size-full wp-image-11505\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?resize=300%2C224&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?resize=1024%2C766&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?resize=768%2C574&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/09/disneycharter-2.png?resize=843%2C630&amp;ssl=1 843w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p><p>What Disney and all of the rest forgot was the lesson first imparted by Werner at the dawn of affiliate fees: retaining customers means offering content for everyone; in the case of the cable bundle, that meant having compelling programming above-and-beyond sports.</p><p>The second lesson Disney forgot was why that NFL deal made sense for ESPN at the time, even though the surcharge ESPN charged cable providers was only projected to barely cover the deal: high end sports deals drove customer demand, but the real money was made on (1) everyone who didn’t care about football and (2) cheap content like SportsCenter. The latter, though, has also been impoverished by the Internet; I noted last year <a href=\"https://stratechery.com/2022/house-of-the-dragon-and-creative-programming-the-big-tens-deal-disneys-new-prices/\">when the Big Ten signed a TV deal that excluded ESPN</a>:</p><blockquote><p>\n  The Big Ten’s exclusion of ESPN really highlights the degree to which social media has supplanted ESPN’s previous tentpole shows like SportsCenter; ESPN used to get discounts on rights deals because to be excluded from SportsCenter meant publicity death. That’s no longer the case.\n</p></blockquote><p>The former, meanwhile, is a reminder that while ESPN has generally made money from rights deals, particularly for smaller sports that filled the schedule and inspired niche fans to badger the cable companies, the biggest properties — particularly the NFL — have always been cognizant of their worth and willing to extract their full value. Disney, in turn, can only maintain ESPN profitability by passing on those rights fees to cable distributors, who must in turn pass them on to their customers.</p><p>The third lesson Disney has forgotten is the most counter-intuitive takeaway of this battle: the worst thing that has happened to the company’s negotiating position is that ESPN is already available on the Internet.</p>",
          "contentText": "Over the last decade the story of ESPN specifically, Disney more broadly, and cable as a whole has been the slow but steady disintegration of that flywheel, culminating in the current standoff between Charter and Disney. From the Wall Street Journal:\n  Charter Communications subscribers are caught in the middle of a philosophical fight between the cable giant and Disney, parent company of ESPN, ABC and several other networks. Disney-owned networks on Thursday went dark for customers of Charter’s Spectrum cable systems, which has nearly 15 million video subscribers across the country including the New York and Los Angeles markets. As a result, sports fans who are Charter subscribers are losing access to college football and the U.S. Open. And the National Football League season is about to begin: ESPN’s “Monday Night Football” starts Sept. 11. Other channels no longer available to Charter include ABC-owned TV stations and cable networks FX, Disney Channel, Freeform and National Geographic.\nChannels going dark in the midst of an affiliate fee dispute aren’t new: indeed, they were how ESPN managed to extract per-subscriber fees in the first place. And, for 40 years, ESPN usually won, including a standoff with YouTube TV in late 2021; I wrote at the time in an Update:\n  It appears that Disney decisively won its stand-off with Google; YouTube TV dropped Disney channels for about two days, only to come to an agreement that Disney characterized as “fair terms that are consistent with the market”; this strongly suggests that Google saw sufficient cancellations in that two-day window that it caved on its demands to get a lower rate. This further reaffirms just how powerful the ESPN bundle is (and Disney’s bundle generally).\nWhen Disney went dark on Charter last week, I initially assumed a similar outcome; then came the Charter investor call the next morning, and this slide:The most important sentence is in the light blue box on the far right: “The video product is no longer a key driver of financial performance.” This is the culmination of a 25-year shift in business model for the cable companies: those initial investments in wires in the ground to provide small communities access to big city TV broadcasts turned out to be very well suited to providing broadband Internet access. Remember the lesson of RCA and ESPN’s founding: the digital transmission of information is inherently indifferent to the data being distributed. In the case of cable the initial use case was digital TV signals, but the exact same cable could also carry packets running the TCP/IP protocol.1Of course for a long time it was very profitable to carry both, along with voice: cable companies offered “triple play” bundles that included TV, Internet, and telephony. Over time the telephony part dropped off, as people used mobile phones exclusively; cable carriers have since moved into the mobile carrier space as well, fueled by profits from TV and broadband Internet. What made the Internet part the most valuable, though, is that the cable companies didn’t need to pay for content: everything was just a packet.That, though, was also the problem: some of those packets reformed themselves as Netflix video streams, which ate into time spent watching TV. Worse, Netflix’s stock was rising and rising as it acquired ever more customers, much to the chagrin of Hollywood, which felt entitled to those multiples given they were the ones producing the most compelling content. That resulted in the fateful decision to start their own streaming services, impoverishing the TV bundle; Charter’s investor presentation included “The Impoverishment Cycle” created by MoffettNathanson:What Disney and all of the rest forgot was the lesson first imparted by Werner at the dawn of affiliate fees: retaining customers means offering content for everyone; in the case of the cable bundle, that meant having compelling programming above-and-beyond sports.The second lesson Disney forgot was why that NFL deal made sense for ESPN at the time, even though the surcharge ESPN charged cable providers was only projected to barely cover the deal: high end sports deals drove customer demand, but the real money was made on (1) everyone who didn’t care about football and (2) cheap content like SportsCenter. The latter, though, has also been impoverished by the Internet; I noted last year when the Big Ten signed a TV deal that excluded ESPN:\n  The Big Ten’s exclusion of ESPN really highlights the degree to which social media has supplanted ESPN’s previous tentpole shows like SportsCenter; ESPN used to get discounts on rights deals because to be excluded from SportsCenter meant publicity death. That’s no longer the case.\nThe former, meanwhile, is a reminder that while ESPN has generally made money from rights deals, particularly for smaller sports that filled the schedule and inspired niche fans to badger the cable companies, the biggest properties — particularly the NFL — have always been cognizant of their worth and willing to extract their full value. Disney, in turn, can only maintain ESPN profitability by passing on those rights fees to cable distributors, who must in turn pass them on to their customers.The third lesson Disney has forgotten is the most counter-intuitive takeaway of this battle: the worst thing that has happened to the company’s negotiating position is that ESPN is already available on the Internet."
        },
        {
          "subtitle": "The Phases of Cable TV",
          "contentHtml": "<p>The cable TV industry has gone through four distinct phases in terms of competition:</p><p><strong>Phase 1: Non-Consumption</strong></p><p>The first phase was the time in ESPN’s history I detailed above: burgeoning cable TV services were running cables to every home in America and trying to convince customers to sign-up. In this case their competition was non-consumption: a lot of people didn’t have cable, and the cable companies wanted them to sign up for service. ESPN was a particularly unique asset in this regard thanks to its provision of sports content that wasn’t available elsewhere — indeed, until the NFL deal, most of the content had never been available at all. This certainly led to some bruising fights between ESPN and the cable companies over affiliate fees, but it’s always easier to come to an agreement when the pie is growing.</p><p><strong>Phase 2: Satellite</strong></p><p>The second phase was the 90s emergence of DirecTV and Dish Network, which offered the same channels as cable TV but via a small satellite dish you could mount on your roof or porch. This was a more involved installation process, but ultimately cheaper thanks to the fact that DirecTV and Dish didn’t need to put an actual cable in the ground. This was also good news for ESPN because now there was an alternative to traditional cable TV: if a cable provider didn’t want to accept higher affiliate fees then ESPN could withhold service, trusting its viewers would punish the cable provider by moving to satellite (which means they would probably be gone forever).</p><p><strong>Phase 3: IPTV</strong></p><p>By the 2000s the satellite threat to cable was fading because satellite was TV only: if a customer had both Internet and TV via their cable provider than it was much harder to switch. Remember, though, that it’s all data in the end; thus the 2000s saw the rise of IPTV offerings from traditional telecom providers like AT&amp;T and Verizon. They too saw salvation for their own fading telephony business in providing broadband Internet, but providing a competitive offering to cable meant offering TV as well. And, thanks to the Internet, they could simply provide said TV using the TCP/IP protocol.</p><p>The decade that followed was probably the time of maximum ESPN leverage: it was easier for customers to switch from the cable bundle to the telecom bundle than it was to install an extra satellite dish; it’s no surprise that this was the decade when ESPN’s aggressiveness in terms of both acquiring sports rights and in raising affiliate fees increased; it was also the peak of ESPN’s relative share of Disney profits.</p><p><strong>Phase 4: vMVPDs</strong></p><p>The virtual multichannel video programming distributor (vMVPD) era kicked off in 2015 with <a href=\"https://stratechery.com/2015/daily-update-sling-tv-interview-makes-15-million-rokus-smart-tv-play/\">the launch of Sling TV</a>. This took the IPTV trend in Phase 3 to its logical endpoint: instead of needing a box to display IPTV signals, you could simply use an app. vMVPD’s have had a big impact on the landscape in two ways: first, they significantly diminished the cord-cutting trend for years as they both captured cord-cutters and also non-consumers, and second, <a href=\"https://stratechery.com/2023/the-phoenix-suns-go-over-the-air-fans-and-franchise-valuation-attention-and-customer-acquisition/\">they decimated regional sports networks</a> that had long increased affiliate fees even more aggressively than ESPN. I wrote in <a href=\"https://stratechery.com/2023/what-the-nba-can-learn-from-formula-1/\">What the NBA Can Learn From Formula 1</a>:</p><blockquote><p>\n  There just aren’t that many SuperFans of a single team, yet regional networks cost more than anything outside of ESPN — more in some markets. This worked in a world where everyone got cable by default, but remember that cable is losing far more customers than pay-TV as a whole, thanks to the rise of the aforementioned virtual pay-TV providers. Virtual pay-TV providers don’t have a customer base to defend, or infrastructure costs to leverage: they distribute via the Internet that people already pay for. To that end, they don’t have to carry everything, and regional sports networks were the most obvious thing to drop: this lets virtual pay-TV providers have a lower price than cable by virtue of excluding content that most people don’t want.\n</p></blockquote><p>Still, this didn’t seem to affect ESPN, as exemplified by the fact they appear to have won their negotiation with YouTube TV in 2021. In fact, though, this dispute with Charter is showing why ESPN may be a loser as well. Go back to the issue of cable customer churn in response to ESPN’s lack of availability; here’s how it manifested in each phase:</p><ul>\n<li>In Phase 1, a churned customer meant less leverage on expensive buildouts, and pressure from Wall Street.</li>\n<li>In Phase 2, a churned customer went to the effort of getting satellite and probably never came back.</li>\n<li>In Phase 3, a churned customer would not just change their TV provider, but also their broadband provider, and remember that broadband was becoming the cable companies’ biggest business.</li>\n</ul><p>In Phase 4, though, a churned TV customer is still a broadband customer, because the Internet is a precondition for watching the vMVPD! Sure, a customer could be so incensed that they also change their Internet provider, but that is completely unnecessary and, given the inconvenience involved, highly unlikely.</p><p>That means that ESPN, for the first time in its history, has no leverage over the cable companies. Indeed, <a href=\"https://www.moffettnathanson.com/\">MoffettNathanson</a> reported that Charter is actively helping customers move to vMVPDs:</p><blockquote><p>\n  For Charter, the uncomfortable truth is that it just doesn’t matter all that much. Yes, they probably do still make some money on video. But not much, and they recognize that linear video is going to be a rapidly declining line of service under even the most optimistic scenarios, so the issue is arguably nothing more than when, not if, video goes away. Charter has already established a referral capability for customers to switch them to YouTube TV or FuboTV (predictably, they haven’t mentioned referring customers to Sling TV or DirecIV Now, and they presumably wouldn’t steer anyone to Hulu Live if the trigger was a dispute with Disney).</p>\n<p>  Notably, the first NFL Monday Night Football game (ESPN) features two Spectrum-market teams; the New York Jets and the Buffalo Bills. To handle a potential rush of customers anxious about missing the game, Charter is preparing a one-touch QR code that would not only create a new YouTube TV or Fubo subscription, but would also downgrade from a Spectrum video bundle with a single click…Disney may learn the hard way that it’s tough to win a negotiation with a counterparty that has nothing to lose.\n</p></blockquote><p>This truth may be uncomfortable for Charter; it ought to be sobering for Disney, particularly since the company, along with the rest of Hollywood, was the one responsible for destroying the value of TV to companies like Charter who were built on it.</p>",
          "contentText": "The cable TV industry has gone through four distinct phases in terms of competition:Phase 1: Non-ConsumptionThe first phase was the time in ESPN’s history I detailed above: burgeoning cable TV services were running cables to every home in America and trying to convince customers to sign-up. In this case their competition was non-consumption: a lot of people didn’t have cable, and the cable companies wanted them to sign up for service. ESPN was a particularly unique asset in this regard thanks to its provision of sports content that wasn’t available elsewhere — indeed, until the NFL deal, most of the content had never been available at all. This certainly led to some bruising fights between ESPN and the cable companies over affiliate fees, but it’s always easier to come to an agreement when the pie is growing.Phase 2: SatelliteThe second phase was the 90s emergence of DirecTV and Dish Network, which offered the same channels as cable TV but via a small satellite dish you could mount on your roof or porch. This was a more involved installation process, but ultimately cheaper thanks to the fact that DirecTV and Dish didn’t need to put an actual cable in the ground. This was also good news for ESPN because now there was an alternative to traditional cable TV: if a cable provider didn’t want to accept higher affiliate fees then ESPN could withhold service, trusting its viewers would punish the cable provider by moving to satellite (which means they would probably be gone forever).Phase 3: IPTVBy the 2000s the satellite threat to cable was fading because satellite was TV only: if a customer had both Internet and TV via their cable provider than it was much harder to switch. Remember, though, that it’s all data in the end; thus the 2000s saw the rise of IPTV offerings from traditional telecom providers like AT&T and Verizon. They too saw salvation for their own fading telephony business in providing broadband Internet, but providing a competitive offering to cable meant offering TV as well. And, thanks to the Internet, they could simply provide said TV using the TCP/IP protocol.The decade that followed was probably the time of maximum ESPN leverage: it was easier for customers to switch from the cable bundle to the telecom bundle than it was to install an extra satellite dish; it’s no surprise that this was the decade when ESPN’s aggressiveness in terms of both acquiring sports rights and in raising affiliate fees increased; it was also the peak of ESPN’s relative share of Disney profits.Phase 4: vMVPDsThe virtual multichannel video programming distributor (vMVPD) era kicked off in 2015 with the launch of Sling TV. This took the IPTV trend in Phase 3 to its logical endpoint: instead of needing a box to display IPTV signals, you could simply use an app. vMVPD’s have had a big impact on the landscape in two ways: first, they significantly diminished the cord-cutting trend for years as they both captured cord-cutters and also non-consumers, and second, they decimated regional sports networks that had long increased affiliate fees even more aggressively than ESPN. I wrote in What the NBA Can Learn From Formula 1:\n  There just aren’t that many SuperFans of a single team, yet regional networks cost more than anything outside of ESPN — more in some markets. This worked in a world where everyone got cable by default, but remember that cable is losing far more customers than pay-TV as a whole, thanks to the rise of the aforementioned virtual pay-TV providers. Virtual pay-TV providers don’t have a customer base to defend, or infrastructure costs to leverage: they distribute via the Internet that people already pay for. To that end, they don’t have to carry everything, and regional sports networks were the most obvious thing to drop: this lets virtual pay-TV providers have a lower price than cable by virtue of excluding content that most people don’t want.\nStill, this didn’t seem to affect ESPN, as exemplified by the fact they appear to have won their negotiation with YouTube TV in 2021. In fact, though, this dispute with Charter is showing why ESPN may be a loser as well. Go back to the issue of cable customer churn in response to ESPN’s lack of availability; here’s how it manifested in each phase:\nIn Phase 1, a churned customer meant less leverage on expensive buildouts, and pressure from Wall Street.\nIn Phase 2, a churned customer went to the effort of getting satellite and probably never came back.\nIn Phase 3, a churned customer would not just change their TV provider, but also their broadband provider, and remember that broadband was becoming the cable companies’ biggest business.\nIn Phase 4, though, a churned TV customer is still a broadband customer, because the Internet is a precondition for watching the vMVPD! Sure, a customer could be so incensed that they also change their Internet provider, but that is completely unnecessary and, given the inconvenience involved, highly unlikely.That means that ESPN, for the first time in its history, has no leverage over the cable companies. Indeed, MoffettNathanson reported that Charter is actively helping customers move to vMVPDs:\n  For Charter, the uncomfortable truth is that it just doesn’t matter all that much. Yes, they probably do still make some money on video. But not much, and they recognize that linear video is going to be a rapidly declining line of service under even the most optimistic scenarios, so the issue is arguably nothing more than when, not if, video goes away. Charter has already established a referral capability for customers to switch them to YouTube TV or FuboTV (predictably, they haven’t mentioned referring customers to Sling TV or DirecIV Now, and they presumably wouldn’t steer anyone to Hulu Live if the trigger was a dispute with Disney).\n  Notably, the first NFL Monday Night Football game (ESPN) features two Spectrum-market teams; the New York Jets and the Buffalo Bills. To handle a potential rush of customers anxious about missing the game, Charter is preparing a one-touch QR code that would not only create a new YouTube TV or Fubo subscription, but would also downgrade from a Spectrum video bundle with a single click…Disney may learn the hard way that it’s tough to win a negotiation with a counterparty that has nothing to lose.\nThis truth may be uncomfortable for Charter; it ought to be sobering for Disney, particularly since the company, along with the rest of Hollywood, was the one responsible for destroying the value of TV to companies like Charter who were built on it."
        },
        {
          "subtitle": "The Case for Re-Bundling",
          "contentHtml": "<p>Once-and-current Disney CEO Bob Iger has been <a href=\"https://stratechery.com/2023/bob-iger-on-cnbc-the-end-of-linear-tv-espn-and-strategic-partnerships-and-apple/\">talking a lot recently</a> about ESPN’s inevitable shift to going over-the-top, including stating that he has a particular date in mind; this showdown with Charter and the revelation of ESPN’s dramatic diminishment in its negotiating position is a reminder that declining businesses often don’t have the luxury of dictating their future.</p><p>So what does that future look like?</p><p>First, it’s very possible — perhaps even likely — that Charter and Disney come to an agreement. As the MoffettNathanson note observes, Charter probably still is making some money on video, and it is also both a customer acquisition tool and churn mitigation factor for their broadband business, and a part of the modern triple play bundle (with mobile). Disney, meanwhile, along with all of the other Hollywood studios, still needs the substantial amount of cash that they receive from cable TV providers (this is particularly pressing for Disney given that they still have to pay for sports rights). Yes, they also earn money from vMVPDs like YouTube TV, but not every customer will seamlessly transition.</p><p>To that end, the company that ought to give here is Disney: according to that Wall Street Journal article Charter is willing to accept the reported $1.50 increase in affiliate fees Disney is demanding if they receive the right to bundle the ad-supported versions of Disney+. Charter argues that it is only right that Disney re-add its most valuable entertainment content to the pay-TV bundle, and frankly, I think they have a point.</p><p>More importantly for Disney, though, is that cable TV providers like Charter remain potent go-to-market entities — decades of servicing customers in their homes has meant a massive build-up in everything from stores to local sales to customer support — and that could be very helpful as Disney seeks to acquire more marginal customers. More importantly, though, I think it is in the long-term interest of the streaming services to be part of a bundle. I wrote last year in <a href=\"https://stratechery.com/2022/cables-last-laugh/\">Cable’s Last Laugh</a>:</p><blockquote><p>\n  The cable companies are better suited than almost anyone else to rebundle for real. Imagine a “streaming bundle” that includes Netflix, HBO Max, Disney+, Paramount+, Peacock, etc., available for a price that is less than the sum of its parts…Owning the customer may be less important than simply having more customers, particularly if those customers are much less likely to churn. After all, that’s one of the advantages of a bundle: instead of your streaming service needing to produce compelling content every single month, you can work as a team to keep customers on board with the bundle.\n</p></blockquote><p>What Charter is proposing is a bit different — they want to bundle traditional TV with streaming services — and I get why Disney is resistant: there are a lot of people paying for both traditional TV and Disney+ (and Hulu and ESPN+); giving Charter bundling rights would cannibalize some amount of revenue. Moreover, it would also mean the end of whatever grand plans Disney might have about offering its own bundle, or cutting out the cable companies’ margin once-and-for-all. At some point, though, Disney and everyone else in Hollywood has to wake up to reality; I wrote in <a href=\"https://stratechery.com/2023/hollywood-on-strike/\">Hollywood on Strike</a>:</p><blockquote><p>\n  The broader issue is that the video industry finally seems to be facing what happened to the print and music industry before them: the Internet comes bearing gifts like infinite capacity and free distribution, but those gifts are a poisoned chalice for industries predicated on scarcity. When anyone could publish text, most text-based businesses went from massive profitability to terminal decline; when anyone could distribute music the music industry could only be saved by tech companies like Spotify helping them sell convenience in place of plastic discs.</p>\n<p>  For the video industry the first step to survival must be to retreat to what they are good at — producing content that isn’t available anywhere else — and getting away from what they are not, i.e. running undifferentiated streaming services with massive direct costs and even larger opportunity ones. Talent, meanwhile, has to realize that they and the studios are not divided by this new paradigm, but jointly threatened: the Internet is bad news for content producers with outsized costs, and long-term sustainability will be that much harder to achieve if the focus is on increasing them.\n</p></blockquote><p>Re-bundling is better for everyone; it’s Disney’s fault that the entities best-placed to pull that off no longer need it.</p><p>Second, for all of the talk about ESPN, it’s worth noting that its content is still valuable — that’s the entire reason this dispute is a big deal. Will anyone care if Charter stops carrying channels from anyone else in Hollywood? And yet, all of those studios are just as dependent on cable TV cashflow, even as many of them have “cheated” to a much greater extent than Disney: Peacock, for example, carries most of NBC’s sports programming, including football, and even put some of the most attractive Olympics programming exclusively on the streaming service. Why on earth should Charter or any other cable provider pay for NBCUniversal channels? Or, more pertinently, if ESPN isn’t available, why would any of the dwindling number of subscribers stay?</p><p>The biggest long-term question, though, has to be around sports itself. Sports leagues could extract ever higher rights fees from ESPN because ESPN could extract ever higher affiliate fees from cable TV providers; if the latter is broken than the former is as well. Yes, vMVPDs like YouTube TV will still exist — and be big winners — and Disney still plans an ESPN streaming service. All of those options, though, entail dramatically increased customer choice; leagues like the NBA have shrugged off declining ratings with the certainty that they would, via cable TV subscribers, get paid regardless, but now the choice isn’t just whether to click the remote, but whether to simply click cancel and watch something else. Better to re-bundle sooner rather than later!</p><hr class=\"footnotes\"><ol class=\"footnotes\" style=\"list-style-type:decimal\"><li id=\"fn1-11503\"><p>Yes, I know I just said “protocol” twice&nbsp;<a href=\"#rf1-11503\" class=\"backlink\" title=\"Return to footnote 1.\">↩</a></p></li></ol>",
          "contentText": "Once-and-current Disney CEO Bob Iger has been talking a lot recently about ESPN’s inevitable shift to going over-the-top, including stating that he has a particular date in mind; this showdown with Charter and the revelation of ESPN’s dramatic diminishment in its negotiating position is a reminder that declining businesses often don’t have the luxury of dictating their future.So what does that future look like?First, it’s very possible — perhaps even likely — that Charter and Disney come to an agreement. As the MoffettNathanson note observes, Charter probably still is making some money on video, and it is also both a customer acquisition tool and churn mitigation factor for their broadband business, and a part of the modern triple play bundle (with mobile). Disney, meanwhile, along with all of the other Hollywood studios, still needs the substantial amount of cash that they receive from cable TV providers (this is particularly pressing for Disney given that they still have to pay for sports rights). Yes, they also earn money from vMVPDs like YouTube TV, but not every customer will seamlessly transition.To that end, the company that ought to give here is Disney: according to that Wall Street Journal article Charter is willing to accept the reported $1.50 increase in affiliate fees Disney is demanding if they receive the right to bundle the ad-supported versions of Disney+. Charter argues that it is only right that Disney re-add its most valuable entertainment content to the pay-TV bundle, and frankly, I think they have a point.More importantly for Disney, though, is that cable TV providers like Charter remain potent go-to-market entities — decades of servicing customers in their homes has meant a massive build-up in everything from stores to local sales to customer support — and that could be very helpful as Disney seeks to acquire more marginal customers. More importantly, though, I think it is in the long-term interest of the streaming services to be part of a bundle. I wrote last year in Cable’s Last Laugh:\n  The cable companies are better suited than almost anyone else to rebundle for real. Imagine a “streaming bundle” that includes Netflix, HBO Max, Disney+, Paramount+, Peacock, etc., available for a price that is less than the sum of its parts…Owning the customer may be less important than simply having more customers, particularly if those customers are much less likely to churn. After all, that’s one of the advantages of a bundle: instead of your streaming service needing to produce compelling content every single month, you can work as a team to keep customers on board with the bundle.\nWhat Charter is proposing is a bit different — they want to bundle traditional TV with streaming services — and I get why Disney is resistant: there are a lot of people paying for both traditional TV and Disney+ (and Hulu and ESPN+); giving Charter bundling rights would cannibalize some amount of revenue. Moreover, it would also mean the end of whatever grand plans Disney might have about offering its own bundle, or cutting out the cable companies’ margin once-and-for-all. At some point, though, Disney and everyone else in Hollywood has to wake up to reality; I wrote in Hollywood on Strike:\n  The broader issue is that the video industry finally seems to be facing what happened to the print and music industry before them: the Internet comes bearing gifts like infinite capacity and free distribution, but those gifts are a poisoned chalice for industries predicated on scarcity. When anyone could publish text, most text-based businesses went from massive profitability to terminal decline; when anyone could distribute music the music industry could only be saved by tech companies like Spotify helping them sell convenience in place of plastic discs.\n  For the video industry the first step to survival must be to retreat to what they are good at — producing content that isn’t available anywhere else — and getting away from what they are not, i.e. running undifferentiated streaming services with massive direct costs and even larger opportunity ones. Talent, meanwhile, has to realize that they and the studios are not divided by this new paradigm, but jointly threatened: the Internet is bad news for content producers with outsized costs, and long-term sustainability will be that much harder to achieve if the focus is on increasing them.\nRe-bundling is better for everyone; it’s Disney’s fault that the entities best-placed to pull that off no longer need it.Second, for all of the talk about ESPN, it’s worth noting that its content is still valuable — that’s the entire reason this dispute is a big deal. Will anyone care if Charter stops carrying channels from anyone else in Hollywood? And yet, all of those studios are just as dependent on cable TV cashflow, even as many of them have “cheated” to a much greater extent than Disney: Peacock, for example, carries most of NBC’s sports programming, including football, and even put some of the most attractive Olympics programming exclusively on the streaming service. Why on earth should Charter or any other cable provider pay for NBCUniversal channels? Or, more pertinently, if ESPN isn’t available, why would any of the dwindling number of subscribers stay?The biggest long-term question, though, has to be around sports itself. Sports leagues could extract ever higher rights fees from ESPN because ESPN could extract ever higher affiliate fees from cable TV providers; if the latter is broken than the former is as well. Yes, vMVPDs like YouTube TV will still exist — and be big winners — and Disney still plans an ESPN streaming service. All of those options, though, entail dramatically increased customer choice; leagues like the NBA have shrugged off declining ratings with the certainty that they would, via cable TV subscribers, get paid regardless, but now the choice isn’t just whether to click the remote, but whether to simply click cancel and watch something else. Better to re-bundle sooner rather than later!Yes, I know I just said “protocol” twice ↩"
        }
      ]
    },
    {
      "title": "Nvidia On the Mountaintop",
      "publishedDate": "2023-08-28T09:30:50-07:00",
      "updatedDate": "2023-08-28T15:39:37-07:00",
      "contentHtml": "\n\t\t<p>It was only 11 months ago that I wrote an Article entitled <a href=\"https://stratechery.com/2022/nvidia-in-the-valley/\">Nvidia In the Valley</a>; the occasion was yet another plummet in their stock price:</p>\n<blockquote><p>\n  <img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-4.png?resize=640%2C331&amp;ssl=1\" alt=\"Nvidia's current stock price drop\" width=\"640\" height=\"331\" class=\"aligncenter size-full wp-image-9775\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-4.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-4.png?resize=300%2C155&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-4.png?resize=1024%2C529&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-4.png?resize=768%2C397&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-4.png?resize=1200%2C620&amp;ssl=1 1200w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\">\n</p></blockquote>\n<p>To say that the company has turned things around is, needless to say, an understatement:</p>\n<p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-6.png?resize=640%2C310&amp;ssl=1\" alt=\"Nvidia's latest stock rise\" width=\"640\" height=\"310\" class=\"aligncenter size-full wp-image-11467\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-6.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-6.png?resize=300%2C145&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-6.png?resize=1024%2C496&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-6.png?resize=768%2C372&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-6.png?resize=1200%2C581&amp;ssl=1 1200w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p>\n<p>That big jump in May was Nvidia’s <a href=\"https://stratechery.com/2023/nvidia-earnings-jenson-huangs-defense-training-versus-inference/\">last earnings</a>, when the company shocked investors with an incredibly ambitious forecast; this last week Nvidia vastly exceeded those expectations and forecasted even bigger growth going forward. From the <a href=\"https://www.wsj.com/business/earnings/nvidia-nvda-q2-earnings-report-2024-814cf32d\">Wall Street Journal</a>:</p>\n<blockquote><p>\n  Chip maker Nvidia said revenue in its recently completed quarter more than doubled from a year ago, setting a new company record, and projected that surging interest in artificial intelligence is propelling its business faster than expected. Nvidia is at the heart of the boom in artificial intelligence that made it a $1 trillion company this year, and it is forecasting growth that outpaces even the most bullish analyst projections.</p>\n<p>  Nvidia’s stock, already the top performer in the S&amp;P 500 this year, rose 7.5% following the results, which would be about $87 billion in market value. The company said revenue more than doubled in its fiscal second quarter to about $13.5 billion, far ahead of Wall Street forecasts in a FactSet survey. Even more strikingly, it said revenue in its current quarter would be around $16 billion, besting expectations by about $3.5 billion. Net profit for the company’s second quarter was $6.19 billion, also surpassing forecasts.</p>\n<p>  The results show a wave of investment in artificial intelligence that began late last year with the arrival of OpenAI’s ChatGPT language-generation tool is gaining steam as companies and governments seek to harness its power in business and everyday life. Many companies see AI as indispensable to their future growth and are making large investments in computing infrastructure to support it.\n</p></blockquote>\n<p>Now the big question on everyone’s mind is if Nvidia is the new Cisco:</p>\n<p><a href=\"https://twitter.com/stockgeekTV/status/1694459398934905113\"><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-1.png?resize=640%2C774&amp;ssl=1\" alt=\"Is Nvidia Cisco?\" width=\"640\" height=\"774\" class=\"aligncenter size-full wp-image-11462\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-1.png?w=1184&amp;ssl=1 1184w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-1.png?resize=248%2C300&amp;ssl=1 248w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-1.png?resize=847%2C1024&amp;ssl=1 847w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-1.png?resize=768%2C929&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-1.png?resize=521%2C630&amp;ssl=1 521w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></a></p>\n<p>I don’t think so, at least in terms of the near-term: there are some fundamental differences between Nvidia and Cisco that are worth teasing out. The bigger question is the long term, and here the comparison might be more apt.</p>\n<h3>Nvidia and Cisco</h3>\n<p>The first difference between Nvidia and Cisco is in the above charts: Nvidia already went through a crash, thanks to the double whammy of Ethereum moving to proof-of-stake and the COVID cliff in terms of PC sales; both left Nvidia with huge amounts of inventory it had to write-off over the second half of last year. The bright spot for Nvidia was the steady growth of data center revenue, thanks to the increase of machine learning workloads; I included this chart in that Article last fall:</p>\n<blockquote><p>\n  <img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?resize=640%2C370&amp;ssl=1\" alt=\"Nvidia's gaming revenue drop\" width=\"640\" height=\"370\" class=\"aligncenter size-full wp-image-9774\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?resize=300%2C173&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?resize=1024%2C591&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?resize=768%2C443&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?resize=1091%2C630&amp;ssl=1 1091w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\">\n</p></blockquote>\n<p>What has happened over the last two quarters is that data center revenue is devouring the rest of the company; here is an updated version of that same chart:</p>\n<p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?resize=640%2C370&amp;ssl=1\" alt=\"Nvidia's sky-rocketing AI revenue\" width=\"640\" height=\"370\" class=\"aligncenter size-full wp-image-11463\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?resize=300%2C173&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?resize=1024%2C592&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?resize=768%2C444&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?resize=1090%2C630&amp;ssl=1 1090w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p>\n<p>Here is Nvidia’s revenue mix:</p>\n<p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?resize=640%2C357&amp;ssl=1\" alt=\"Nvidia's revenue mix\" width=\"640\" height=\"357\" class=\"aligncenter size-full wp-image-11464\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?resize=300%2C167&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?resize=1024%2C571&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?resize=768%2C428&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?resize=1129%2C630&amp;ssl=1 1129w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p>\n<p>This dramatic shift in Nvidia’s business provides some interesting contrasts to Cisco’s dot-com run-up. First, here was Cisco’s revenue, gross profit, net profit, and stock price in the ten years starting from its 1993 IPO:</p>\n<p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?resize=640%2C398&amp;ssl=1\" alt=\"Cisco's revenue, profit, and stock price in the 90s\" width=\"640\" height=\"398\" class=\"aligncenter size-full wp-image-11466\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?resize=300%2C186&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?resize=1024%2C636&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?resize=768%2C477&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?resize=1014%2C630&amp;ssl=1 1014w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p>\n<p>Here is Nvidia’s last ten years:</p>\n<p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?resize=640%2C397&amp;ssl=1\" alt=\"Nvidia's revenue, profit, and stock price\" width=\"640\" height=\"397\" class=\"aligncenter size-full wp-image-11465\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?resize=300%2C186&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?resize=1024%2C634&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?resize=768%2C476&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?resize=1017%2C630&amp;ssl=1 1017w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p>\n<p>The first thing to note is the extent to which Nvidia’s crash last year looks similar to Cisco’s dot-com crash: in both cases steady but steep revenue increases initially outpaced the stock price, which eventually overshot just a few quarters before big inventory write-downs led to big decreases in profitability (score one for crypto optimists hopeful that the current doldrums are simply their own dot-com hangover).</p>\n<p>Cisco, though, didn’t have a second act, unlike this data center explosion. What is notable is the extent to which Nvidia’s revenue increase is matching the slope of the stock price increase (obviously this is inexact given the different axis); it seems likely that the stock will overshoot revenue growth soon enough, but it hasn’t really happened yet. It’s also worth noting how much more disciplined Nvidia appears to be in terms of below-the-line costs: its net profit is moving in concert with its revenue, unlike Cisco in the 90s; I suspect this is a function of Nvidia being a much larger and more mature company.</p>\n<p>Another difference is the nature of Nvidia’s customers: over 50% of the company’s Q2 revenue came from the large cloud service providers, followed by large consumer Internet companies (i.e. Meta). This category does, of course, include the startups that once might have purchased Cisco routers and Sun servers directly, and now rent capacity (if they can get it); cloud providers, though, monetize their hardware immediately, which is good for Nvidia.</p>\n<p>Still, there is an important difference from other cloud workloads: previously a new company or line of business only ramped their cloud utilization with usage, which ought to correlate to customer acquisition, if not revenue. Model training, though, is an up-front cost, not dissimilar to the cost needed to buy those Sun servers and Cisco routers in the dot-com era; that is cloud revenue that has a much higher likelihood of disappearing if the company in question doesn’t find a market.</p>\n<p>This point is relevant to Nvidia given that training is the part of AI where the company is the most dominant, thanks to both its software ecosystem and the ability to operate huge fleet of Nvidia chips as a single GPU; inference is where Nvidia will first see challenges, and that is also the area of AI that is correlated with usage, and thus more durable from a cloud provider perspective.</p>\n<p>Those points about a software ecosystem and hardware scalability are also the biggest reason why Nvidia is different than Cisco. Nvidia has a moat in both, along with a substantial manufacturing advantage thanks to its upfront payments to TSMC over the last several years to secure its own 4nm line (and having the good fortune of asking for more scale at a time when TSMC’s other sources of high performance computing revenue are in a slump). There is certainly a massive incentive for both the cloud providers and large Internet companies to bridge Nvidia’s moats — see AWS’s investments in its own chips, for example, or Meta’s development of and support for PyTorch — but right now Nvidia has a big lead and the frenzy inspired by ChatGPT is only deepening their install base, with all of the positive ecosystem effects that entails.</p>\n<h3>GPU Demand</h3>\n<p>The biggest challenge facing Nvidia is the one that is ultimately out of their control: what does the final market look like?</p>\n<p>Go back to the dot-com era, and the era that proceeded it. The advent of computing, first in the form of mainframes and then the PC, digitized information, making it endlessly duplicable. Then came the Internet which made the marginal cost of distributing that content go to zero (with the caveat that most people had very low bandwidth). This was an obvious business opportunity that plenty of startups jumped all over, even as telecom companies took on the bandwidth problem; Cisco was the beneficiary of both.</p>\n<p>The missing element, though, was demand: consistent consumer demand for Internet applications only started to arrive with the advent of broadband connections in the 2000s (thanks in part to a buildout that bankrupted said telecom companies), and then exploded with smartphones a decade later, which made the Internet accessible anytime, anywhere. It was demand that made the router business as big as dot-com investors thought it might be, although by then Cisco had a host of competitors, including large cloud providers who built (and open-sourced) their own.</p>\n<p>There are lots of potential starting points to choose for AI: machine learning has obviously been a thing for a while, or you might point to the 2017 invention of the transformer; the release of GPT-3 in 2020 was perhaps akin to the release of the Mosaic web browser, which would make ChatGPT the Netscape IPO. One way to categorize this emergence is to characterize training as being akin to digitization in the previous era, and creation —&nbsp;i.e. inference — as akin to distribution. Once again there are obvious business opportunities that arise from combining the two, and once again startups are jumping all over them, along with the big incumbents.</p>\n<p>However you want to make the analogy, what is important to note is that the missing element is the same: demand. ChatGPT took the world by storm, and the use of AI for writing code is both proliferating widely and is extremely high leverage. Every SaaS company in tech, meanwhile, is hard at work at an AI strategy, for the benefit of their sales team if nothing else. That is no small thing, and the exploration and implementation of those strategies will use up a lot of Nvidia GPUs over the next few years. The ultimate question, though, is how much of this AI stuff is actually used, and that is ultimately out of Nvidia’s control.</p>\n<p>My best guess is that the next several years will be occupied building out the most obvious use cases, particularly in the enterprise; the analogy here is to the 2000s build-out of the web. The question, though, is what will be the analogy to mobile (and the cloud), which exploded demand and led to one of the most profitable decades tech has ever seen? The answer may be an already discarded fad: the metaverse.</p>\n<h3>A GPU Overhang and the Metaverse</h3>\n<p>In April 2022, when Dall-E 2 came out, I wrote <a href=\"https://stratechery.com/2022/dall-e-the-metaverse-and-zero-marginal-content/\">DALL-E, the Metaverse, and Zero Marginal Content</a>, and highlighted three trends:</p>\n<ul>\n<li>First, the gaming industry was increasingly about a few AAA games, small indie titles, and the huge sea of mobile; the limiting factor in further development was the astronomical cost of developing high quality assets.</li>\n<li>Second, social media succeeded by virtue of making content creation free, because users created the content of their own volition.</li>\n<li>Third, TikTok pointed to a future where every individual not only had their own feed, but also where the provenance of that content didn’t matter.</li>\n</ul>\n<p>AI is how those three trends might intersect:</p>\n<blockquote><p>\n  What is fascinating about DALL-E is that it points to a future where these three trends can be combined. DALL-E, at the end of the day, is ultimately a product of human-generated content, just like its GPT-3 cousin. The latter, of course, is about text, while DALL-E is about images. Notice, though, that progression from text to images; it follows that machine learning-generated video is next. This will likely take several years, of course; video is a much more difficult problem, and responsive 3D environments more difficult yet, but this is a path the industry has trod before:</p>\n<ul>\n<li>Game developers pushed the limits on text, then images, then video, then 3D</li>\n<li>Social media drives content creation costs to zero first on text, then images, then video</li>\n<li>Machine learning models can now create text and images for zero marginal cost</li>\n</ul>\n<p>  In the very long run this points to a metaverse vision that is much less deterministic than your typical video game, yet much richer than what is generated on social media. Imagine environments that are not drawn by artists but rather created by AI: this not only increases the possibilities, but crucially, decreases the costs.\n</p></blockquote>\n<p>I wrote in the conclusion:</p>\n<blockquote><p>\n  Machine learning generated content is just the next step beyond TikTok: instead of pulling content from anywhere on the network, GPT and DALL-E and other similar models generate new content from content, at zero marginal cost. This is how the economics of the metaverse will ultimately make sense: virtual worlds need virtual content created at virtually zero cost, fully customizable to the individual.\n</p></blockquote>\n<p>Zero marginal cost is, I should note, aspirational at this point: inference is expensive, both in terms of power and also in terms of the need to pay off all of that money that is showing up on Nvidia’s earnings. It’s possible to imagine a scenario a few years down the line, though, where Nvidia has deployed countless ever more powerful GPUs, and inspired massive competition such that the world’s supply of GPU power far exceeds demand, driving the marginal costs down to the cost of energy (which hopefully will have become cheaper as well); suddenly the idea of making virtual environments on demand won’t seem so far-fetched, opening up entirely new end-user experiences that explode demand in the way that mobile once did.</p>\n<h3>The GPU Age</h3>\n<p>The challenge for Nvidia is that this future isn’t particularly investable; indeed, the idea assumes a capacity overhang at some point, which is not great for the stock price! That, though, is how technology advances, and even if a cliff eventually comes, there is a lot of money to be made in the meantime.</p>\n<p>That noted, the biggest short-term question I have is around Nvidia CEO Jensen Huang’s insistence that the current wave of demand is in fact the dawn of what he calls accelerated computing; from the <a href=\"https://seekingalpha.com/article/4630703-nvidia-corp-nvda-q2-2024-earnings-call-transcript\">Nvidia earnings call</a>:</p>\n<blockquote><p>\n  I’m reluctant to guess about the future and so I’ll answer the question from the first principle of computer science perspective. It is recognized for some time now that…using general purpose computing at scale is no longer the best way to go forward. It’s too energy costly, it’s too expensive, and the performance of the applications are too slow. And finally, the world has a new way of doing it. It’s called accelerated computing and what kicked it into turbocharge is generative AI. But accelerated computing could be used for all kinds of different applications that’s already in the data center. And by using it, you offload the CPUs. You save a ton of money in order of magnitude, in cost and order of magnitude and energy and the throughput is higher and that’s what the industry is really responding to.</p>\n<p>  Going forward, the best way to invest in the data center is to divert the capital investment from general purpose computing and focus it on generative AI and accelerated computing. Generative AI provides a new way of generating productivity, a new way of generating new services to offer to your customers, and accelerated computing helps you save money and save power. And the number of applications is, well, tons. Lots of developers, lots of applications, lots of libraries. It’s ready to be deployed.</p>\n<p>  And so I think the data centers around the world recognize this, that this is the best way to deploy resources, deploy capital going forward for data centers. This is true for the world’s clouds and you’re seeing a whole crop of new GPU-specialized cloud service providers. One of the famous ones is CoreWeave and they’re doing incredibly well. But you’re seeing the regional GPU specialist service providers all over the world now. And it’s because they all recognize the same thing, that the best way to invest their capital going forward is to put it into accelerated computing and generative AI.\n</p></blockquote>\n<p>My interpretation of Huang’s outlook is that all of these GPUs will be used for a lot of the same activities that are currently run on CPUs; that is certainly a bullish view for Nvidia, because it means the capacity overhang that may come from pursuing generative AI will be back-filled by current cloud computing workloads. And, to be fair, Huang has a point about the power and space limitations of current architectures.</p>\n<p>That noted, I’m skeptical: humans — and companies — are lazy, and not only are CPU-based applications easier to develop, they are also mostly already built. I have a hard time seeing what companies are going to go through the time and effort to port things that already run on CPUs to GPUs; at the end of the day, the applications that run in a cloud are determined by customers who provide the demand for cloud resources, not cloud providers looking to optimize FLOP/rack.</p>\n<p>If GPUs are going to be as big of a market as Nvidia’s investors hope it will be, it will be because applications that are only possible with GPUs generate the demand to make it so. I’m confident that time will come; what I, nor Huang, nor anyone else can be sure of is when that time will arrive.</p>\n\n\t\t\t",
      "contentText": "\n\t\tIt was only 11 months ago that I wrote an Article entitled Nvidia In the Valley; the occasion was yet another plummet in their stock price:\n\n  \n\nTo say that the company has turned things around is, needless to say, an understatement:\n\nThat big jump in May was Nvidia’s last earnings, when the company shocked investors with an incredibly ambitious forecast; this last week Nvidia vastly exceeded those expectations and forecasted even bigger growth going forward. From the Wall Street Journal:\n\n  Chip maker Nvidia said revenue in its recently completed quarter more than doubled from a year ago, setting a new company record, and projected that surging interest in artificial intelligence is propelling its business faster than expected. Nvidia is at the heart of the boom in artificial intelligence that made it a $1 trillion company this year, and it is forecasting growth that outpaces even the most bullish analyst projections.\n  Nvidia’s stock, already the top performer in the S&P 500 this year, rose 7.5% following the results, which would be about $87 billion in market value. The company said revenue more than doubled in its fiscal second quarter to about $13.5 billion, far ahead of Wall Street forecasts in a FactSet survey. Even more strikingly, it said revenue in its current quarter would be around $16 billion, besting expectations by about $3.5 billion. Net profit for the company’s second quarter was $6.19 billion, also surpassing forecasts.\n  The results show a wave of investment in artificial intelligence that began late last year with the arrival of OpenAI’s ChatGPT language-generation tool is gaining steam as companies and governments seek to harness its power in business and everyday life. Many companies see AI as indispensable to their future growth and are making large investments in computing infrastructure to support it.\n\nNow the big question on everyone’s mind is if Nvidia is the new Cisco:\n\nI don’t think so, at least in terms of the near-term: there are some fundamental differences between Nvidia and Cisco that are worth teasing out. The bigger question is the long term, and here the comparison might be more apt.\nNvidia and Cisco\nThe first difference between Nvidia and Cisco is in the above charts: Nvidia already went through a crash, thanks to the double whammy of Ethereum moving to proof-of-stake and the COVID cliff in terms of PC sales; both left Nvidia with huge amounts of inventory it had to write-off over the second half of last year. The bright spot for Nvidia was the steady growth of data center revenue, thanks to the increase of machine learning workloads; I included this chart in that Article last fall:\n\n  \n\nWhat has happened over the last two quarters is that data center revenue is devouring the rest of the company; here is an updated version of that same chart:\n\nHere is Nvidia’s revenue mix:\n\nThis dramatic shift in Nvidia’s business provides some interesting contrasts to Cisco’s dot-com run-up. First, here was Cisco’s revenue, gross profit, net profit, and stock price in the ten years starting from its 1993 IPO:\n\nHere is Nvidia’s last ten years:\n\nThe first thing to note is the extent to which Nvidia’s crash last year looks similar to Cisco’s dot-com crash: in both cases steady but steep revenue increases initially outpaced the stock price, which eventually overshot just a few quarters before big inventory write-downs led to big decreases in profitability (score one for crypto optimists hopeful that the current doldrums are simply their own dot-com hangover).\nCisco, though, didn’t have a second act, unlike this data center explosion. What is notable is the extent to which Nvidia’s revenue increase is matching the slope of the stock price increase (obviously this is inexact given the different axis); it seems likely that the stock will overshoot revenue growth soon enough, but it hasn’t really happened yet. It’s also worth noting how much more disciplined Nvidia appears to be in terms of below-the-line costs: its net profit is moving in concert with its revenue, unlike Cisco in the 90s; I suspect this is a function of Nvidia being a much larger and more mature company.\nAnother difference is the nature of Nvidia’s customers: over 50% of the company’s Q2 revenue came from the large cloud service providers, followed by large consumer Internet companies (i.e. Meta). This category does, of course, include the startups that once might have purchased Cisco routers and Sun servers directly, and now rent capacity (if they can get it); cloud providers, though, monetize their hardware immediately, which is good for Nvidia.\nStill, there is an important difference from other cloud workloads: previously a new company or line of business only ramped their cloud utilization with usage, which ought to correlate to customer acquisition, if not revenue. Model training, though, is an up-front cost, not dissimilar to the cost needed to buy those Sun servers and Cisco routers in the dot-com era; that is cloud revenue that has a much higher likelihood of disappearing if the company in question doesn’t find a market.\nThis point is relevant to Nvidia given that training is the part of AI where the company is the most dominant, thanks to both its software ecosystem and the ability to operate huge fleet of Nvidia chips as a single GPU; inference is where Nvidia will first see challenges, and that is also the area of AI that is correlated with usage, and thus more durable from a cloud provider perspective.\nThose points about a software ecosystem and hardware scalability are also the biggest reason why Nvidia is different than Cisco. Nvidia has a moat in both, along with a substantial manufacturing advantage thanks to its upfront payments to TSMC over the last several years to secure its own 4nm line (and having the good fortune of asking for more scale at a time when TSMC’s other sources of high performance computing revenue are in a slump). There is certainly a massive incentive for both the cloud providers and large Internet companies to bridge Nvidia’s moats — see AWS’s investments in its own chips, for example, or Meta’s development of and support for PyTorch — but right now Nvidia has a big lead and the frenzy inspired by ChatGPT is only deepening their install base, with all of the positive ecosystem effects that entails.\nGPU Demand\nThe biggest challenge facing Nvidia is the one that is ultimately out of their control: what does the final market look like?\nGo back to the dot-com era, and the era that proceeded it. The advent of computing, first in the form of mainframes and then the PC, digitized information, making it endlessly duplicable. Then came the Internet which made the marginal cost of distributing that content go to zero (with the caveat that most people had very low bandwidth). This was an obvious business opportunity that plenty of startups jumped all over, even as telecom companies took on the bandwidth problem; Cisco was the beneficiary of both.\nThe missing element, though, was demand: consistent consumer demand for Internet applications only started to arrive with the advent of broadband connections in the 2000s (thanks in part to a buildout that bankrupted said telecom companies), and then exploded with smartphones a decade later, which made the Internet accessible anytime, anywhere. It was demand that made the router business as big as dot-com investors thought it might be, although by then Cisco had a host of competitors, including large cloud providers who built (and open-sourced) their own.\nThere are lots of potential starting points to choose for AI: machine learning has obviously been a thing for a while, or you might point to the 2017 invention of the transformer; the release of GPT-3 in 2020 was perhaps akin to the release of the Mosaic web browser, which would make ChatGPT the Netscape IPO. One way to categorize this emergence is to characterize training as being akin to digitization in the previous era, and creation — i.e. inference — as akin to distribution. Once again there are obvious business opportunities that arise from combining the two, and once again startups are jumping all over them, along with the big incumbents.\nHowever you want to make the analogy, what is important to note is that the missing element is the same: demand. ChatGPT took the world by storm, and the use of AI for writing code is both proliferating widely and is extremely high leverage. Every SaaS company in tech, meanwhile, is hard at work at an AI strategy, for the benefit of their sales team if nothing else. That is no small thing, and the exploration and implementation of those strategies will use up a lot of Nvidia GPUs over the next few years. The ultimate question, though, is how much of this AI stuff is actually used, and that is ultimately out of Nvidia’s control.\nMy best guess is that the next several years will be occupied building out the most obvious use cases, particularly in the enterprise; the analogy here is to the 2000s build-out of the web. The question, though, is what will be the analogy to mobile (and the cloud), which exploded demand and led to one of the most profitable decades tech has ever seen? The answer may be an already discarded fad: the metaverse.\nA GPU Overhang and the Metaverse\nIn April 2022, when Dall-E 2 came out, I wrote DALL-E, the Metaverse, and Zero Marginal Content, and highlighted three trends:\n\nFirst, the gaming industry was increasingly about a few AAA games, small indie titles, and the huge sea of mobile; the limiting factor in further development was the astronomical cost of developing high quality assets.\nSecond, social media succeeded by virtue of making content creation free, because users created the content of their own volition.\nThird, TikTok pointed to a future where every individual not only had their own feed, but also where the provenance of that content didn’t matter.\n\nAI is how those three trends might intersect:\n\n  What is fascinating about DALL-E is that it points to a future where these three trends can be combined. DALL-E, at the end of the day, is ultimately a product of human-generated content, just like its GPT-3 cousin. The latter, of course, is about text, while DALL-E is about images. Notice, though, that progression from text to images; it follows that machine learning-generated video is next. This will likely take several years, of course; video is a much more difficult problem, and responsive 3D environments more difficult yet, but this is a path the industry has trod before:\n\nGame developers pushed the limits on text, then images, then video, then 3D\nSocial media drives content creation costs to zero first on text, then images, then video\nMachine learning models can now create text and images for zero marginal cost\n\n  In the very long run this points to a metaverse vision that is much less deterministic than your typical video game, yet much richer than what is generated on social media. Imagine environments that are not drawn by artists but rather created by AI: this not only increases the possibilities, but crucially, decreases the costs.\n\nI wrote in the conclusion:\n\n  Machine learning generated content is just the next step beyond TikTok: instead of pulling content from anywhere on the network, GPT and DALL-E and other similar models generate new content from content, at zero marginal cost. This is how the economics of the metaverse will ultimately make sense: virtual worlds need virtual content created at virtually zero cost, fully customizable to the individual.\n\nZero marginal cost is, I should note, aspirational at this point: inference is expensive, both in terms of power and also in terms of the need to pay off all of that money that is showing up on Nvidia’s earnings. It’s possible to imagine a scenario a few years down the line, though, where Nvidia has deployed countless ever more powerful GPUs, and inspired massive competition such that the world’s supply of GPU power far exceeds demand, driving the marginal costs down to the cost of energy (which hopefully will have become cheaper as well); suddenly the idea of making virtual environments on demand won’t seem so far-fetched, opening up entirely new end-user experiences that explode demand in the way that mobile once did.\nThe GPU Age\nThe challenge for Nvidia is that this future isn’t particularly investable; indeed, the idea assumes a capacity overhang at some point, which is not great for the stock price! That, though, is how technology advances, and even if a cliff eventually comes, there is a lot of money to be made in the meantime.\nThat noted, the biggest short-term question I have is around Nvidia CEO Jensen Huang’s insistence that the current wave of demand is in fact the dawn of what he calls accelerated computing; from the Nvidia earnings call:\n\n  I’m reluctant to guess about the future and so I’ll answer the question from the first principle of computer science perspective. It is recognized for some time now that…using general purpose computing at scale is no longer the best way to go forward. It’s too energy costly, it’s too expensive, and the performance of the applications are too slow. And finally, the world has a new way of doing it. It’s called accelerated computing and what kicked it into turbocharge is generative AI. But accelerated computing could be used for all kinds of different applications that’s already in the data center. And by using it, you offload the CPUs. You save a ton of money in order of magnitude, in cost and order of magnitude and energy and the throughput is higher and that’s what the industry is really responding to.\n  Going forward, the best way to invest in the data center is to divert the capital investment from general purpose computing and focus it on generative AI and accelerated computing. Generative AI provides a new way of generating productivity, a new way of generating new services to offer to your customers, and accelerated computing helps you save money and save power. And the number of applications is, well, tons. Lots of developers, lots of applications, lots of libraries. It’s ready to be deployed.\n  And so I think the data centers around the world recognize this, that this is the best way to deploy resources, deploy capital going forward for data centers. This is true for the world’s clouds and you’re seeing a whole crop of new GPU-specialized cloud service providers. One of the famous ones is CoreWeave and they’re doing incredibly well. But you’re seeing the regional GPU specialist service providers all over the world now. And it’s because they all recognize the same thing, that the best way to invest their capital going forward is to put it into accelerated computing and generative AI.\n\nMy interpretation of Huang’s outlook is that all of these GPUs will be used for a lot of the same activities that are currently run on CPUs; that is certainly a bullish view for Nvidia, because it means the capacity overhang that may come from pursuing generative AI will be back-filled by current cloud computing workloads. And, to be fair, Huang has a point about the power and space limitations of current architectures.\nThat noted, I’m skeptical: humans — and companies — are lazy, and not only are CPU-based applications easier to develop, they are also mostly already built. I have a hard time seeing what companies are going to go through the time and effort to port things that already run on CPUs to GPUs; at the end of the day, the applications that run in a cloud are determined by customers who provide the demand for cloud resources, not cloud providers looking to optimize FLOP/rack.\nIf GPUs are going to be as big of a market as Nvidia’s investors hope it will be, it will be because applications that are only possible with GPUs generate the demand to make it so. I’m confident that time will come; what I, nor Huang, nor anyone else can be sure of is when that time will arrive.\n\n\t\t\t",
      "subsections": [
        {
          "subtitle": "Nvidia and Cisco",
          "contentHtml": "<p>The first difference between Nvidia and Cisco is in the above charts: Nvidia already went through a crash, thanks to the double whammy of Ethereum moving to proof-of-stake and the COVID cliff in terms of PC sales; both left Nvidia with huge amounts of inventory it had to write-off over the second half of last year. The bright spot for Nvidia was the steady growth of data center revenue, thanks to the increase of machine learning workloads; I included this chart in that Article last fall:</p><blockquote><p>\n  <img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?resize=640%2C370&amp;ssl=1\" alt=\"Nvidia's gaming revenue drop\" width=\"640\" height=\"370\" class=\"aligncenter size-full wp-image-9774\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?resize=300%2C173&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?resize=1024%2C591&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?resize=768%2C443&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2022/09/nvidia-3.png?resize=1091%2C630&amp;ssl=1 1091w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\">\n</p></blockquote><p>What has happened over the last two quarters is that data center revenue is devouring the rest of the company; here is an updated version of that same chart:</p><p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?resize=640%2C370&amp;ssl=1\" alt=\"Nvidia's sky-rocketing AI revenue\" width=\"640\" height=\"370\" class=\"aligncenter size-full wp-image-11463\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?resize=300%2C173&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?resize=1024%2C592&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?resize=768%2C444&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-2.png?resize=1090%2C630&amp;ssl=1 1090w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p><p>Here is Nvidia’s revenue mix:</p><p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?resize=640%2C357&amp;ssl=1\" alt=\"Nvidia's revenue mix\" width=\"640\" height=\"357\" class=\"aligncenter size-full wp-image-11464\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?resize=300%2C167&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?resize=1024%2C571&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?resize=768%2C428&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-3.png?resize=1129%2C630&amp;ssl=1 1129w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p><p>This dramatic shift in Nvidia’s business provides some interesting contrasts to Cisco’s dot-com run-up. First, here was Cisco’s revenue, gross profit, net profit, and stock price in the ten years starting from its 1993 IPO:</p><p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?resize=640%2C398&amp;ssl=1\" alt=\"Cisco's revenue, profit, and stock price in the 90s\" width=\"640\" height=\"398\" class=\"aligncenter size-full wp-image-11466\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?resize=300%2C186&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?resize=1024%2C636&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?resize=768%2C477&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-5.png?resize=1014%2C630&amp;ssl=1 1014w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p><p>Here is Nvidia’s last ten years:</p><p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?resize=640%2C397&amp;ssl=1\" alt=\"Nvidia's revenue, profit, and stock price\" width=\"640\" height=\"397\" class=\"aligncenter size-full wp-image-11465\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?resize=300%2C186&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?resize=1024%2C634&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?resize=768%2C476&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/08/nvidia-4.png?resize=1017%2C630&amp;ssl=1 1017w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p><p>The first thing to note is the extent to which Nvidia’s crash last year looks similar to Cisco’s dot-com crash: in both cases steady but steep revenue increases initially outpaced the stock price, which eventually overshot just a few quarters before big inventory write-downs led to big decreases in profitability (score one for crypto optimists hopeful that the current doldrums are simply their own dot-com hangover).</p><p>Cisco, though, didn’t have a second act, unlike this data center explosion. What is notable is the extent to which Nvidia’s revenue increase is matching the slope of the stock price increase (obviously this is inexact given the different axis); it seems likely that the stock will overshoot revenue growth soon enough, but it hasn’t really happened yet. It’s also worth noting how much more disciplined Nvidia appears to be in terms of below-the-line costs: its net profit is moving in concert with its revenue, unlike Cisco in the 90s; I suspect this is a function of Nvidia being a much larger and more mature company.</p><p>Another difference is the nature of Nvidia’s customers: over 50% of the company’s Q2 revenue came from the large cloud service providers, followed by large consumer Internet companies (i.e. Meta). This category does, of course, include the startups that once might have purchased Cisco routers and Sun servers directly, and now rent capacity (if they can get it); cloud providers, though, monetize their hardware immediately, which is good for Nvidia.</p><p>Still, there is an important difference from other cloud workloads: previously a new company or line of business only ramped their cloud utilization with usage, which ought to correlate to customer acquisition, if not revenue. Model training, though, is an up-front cost, not dissimilar to the cost needed to buy those Sun servers and Cisco routers in the dot-com era; that is cloud revenue that has a much higher likelihood of disappearing if the company in question doesn’t find a market.</p><p>This point is relevant to Nvidia given that training is the part of AI where the company is the most dominant, thanks to both its software ecosystem and the ability to operate huge fleet of Nvidia chips as a single GPU; inference is where Nvidia will first see challenges, and that is also the area of AI that is correlated with usage, and thus more durable from a cloud provider perspective.</p><p>Those points about a software ecosystem and hardware scalability are also the biggest reason why Nvidia is different than Cisco. Nvidia has a moat in both, along with a substantial manufacturing advantage thanks to its upfront payments to TSMC over the last several years to secure its own 4nm line (and having the good fortune of asking for more scale at a time when TSMC’s other sources of high performance computing revenue are in a slump). There is certainly a massive incentive for both the cloud providers and large Internet companies to bridge Nvidia’s moats — see AWS’s investments in its own chips, for example, or Meta’s development of and support for PyTorch — but right now Nvidia has a big lead and the frenzy inspired by ChatGPT is only deepening their install base, with all of the positive ecosystem effects that entails.</p>",
          "contentText": "The first difference between Nvidia and Cisco is in the above charts: Nvidia already went through a crash, thanks to the double whammy of Ethereum moving to proof-of-stake and the COVID cliff in terms of PC sales; both left Nvidia with huge amounts of inventory it had to write-off over the second half of last year. The bright spot for Nvidia was the steady growth of data center revenue, thanks to the increase of machine learning workloads; I included this chart in that Article last fall:\n  \nWhat has happened over the last two quarters is that data center revenue is devouring the rest of the company; here is an updated version of that same chart:Here is Nvidia’s revenue mix:This dramatic shift in Nvidia’s business provides some interesting contrasts to Cisco’s dot-com run-up. First, here was Cisco’s revenue, gross profit, net profit, and stock price in the ten years starting from its 1993 IPO:Here is Nvidia’s last ten years:The first thing to note is the extent to which Nvidia’s crash last year looks similar to Cisco’s dot-com crash: in both cases steady but steep revenue increases initially outpaced the stock price, which eventually overshot just a few quarters before big inventory write-downs led to big decreases in profitability (score one for crypto optimists hopeful that the current doldrums are simply their own dot-com hangover).Cisco, though, didn’t have a second act, unlike this data center explosion. What is notable is the extent to which Nvidia’s revenue increase is matching the slope of the stock price increase (obviously this is inexact given the different axis); it seems likely that the stock will overshoot revenue growth soon enough, but it hasn’t really happened yet. It’s also worth noting how much more disciplined Nvidia appears to be in terms of below-the-line costs: its net profit is moving in concert with its revenue, unlike Cisco in the 90s; I suspect this is a function of Nvidia being a much larger and more mature company.Another difference is the nature of Nvidia’s customers: over 50% of the company’s Q2 revenue came from the large cloud service providers, followed by large consumer Internet companies (i.e. Meta). This category does, of course, include the startups that once might have purchased Cisco routers and Sun servers directly, and now rent capacity (if they can get it); cloud providers, though, monetize their hardware immediately, which is good for Nvidia.Still, there is an important difference from other cloud workloads: previously a new company or line of business only ramped their cloud utilization with usage, which ought to correlate to customer acquisition, if not revenue. Model training, though, is an up-front cost, not dissimilar to the cost needed to buy those Sun servers and Cisco routers in the dot-com era; that is cloud revenue that has a much higher likelihood of disappearing if the company in question doesn’t find a market.This point is relevant to Nvidia given that training is the part of AI where the company is the most dominant, thanks to both its software ecosystem and the ability to operate huge fleet of Nvidia chips as a single GPU; inference is where Nvidia will first see challenges, and that is also the area of AI that is correlated with usage, and thus more durable from a cloud provider perspective.Those points about a software ecosystem and hardware scalability are also the biggest reason why Nvidia is different than Cisco. Nvidia has a moat in both, along with a substantial manufacturing advantage thanks to its upfront payments to TSMC over the last several years to secure its own 4nm line (and having the good fortune of asking for more scale at a time when TSMC’s other sources of high performance computing revenue are in a slump). There is certainly a massive incentive for both the cloud providers and large Internet companies to bridge Nvidia’s moats — see AWS’s investments in its own chips, for example, or Meta’s development of and support for PyTorch — but right now Nvidia has a big lead and the frenzy inspired by ChatGPT is only deepening their install base, with all of the positive ecosystem effects that entails."
        },
        {
          "subtitle": "GPU Demand",
          "contentHtml": "<p>The biggest challenge facing Nvidia is the one that is ultimately out of their control: what does the final market look like?</p><p>Go back to the dot-com era, and the era that proceeded it. The advent of computing, first in the form of mainframes and then the PC, digitized information, making it endlessly duplicable. Then came the Internet which made the marginal cost of distributing that content go to zero (with the caveat that most people had very low bandwidth). This was an obvious business opportunity that plenty of startups jumped all over, even as telecom companies took on the bandwidth problem; Cisco was the beneficiary of both.</p><p>The missing element, though, was demand: consistent consumer demand for Internet applications only started to arrive with the advent of broadband connections in the 2000s (thanks in part to a buildout that bankrupted said telecom companies), and then exploded with smartphones a decade later, which made the Internet accessible anytime, anywhere. It was demand that made the router business as big as dot-com investors thought it might be, although by then Cisco had a host of competitors, including large cloud providers who built (and open-sourced) their own.</p><p>There are lots of potential starting points to choose for AI: machine learning has obviously been a thing for a while, or you might point to the 2017 invention of the transformer; the release of GPT-3 in 2020 was perhaps akin to the release of the Mosaic web browser, which would make ChatGPT the Netscape IPO. One way to categorize this emergence is to characterize training as being akin to digitization in the previous era, and creation —&nbsp;i.e. inference — as akin to distribution. Once again there are obvious business opportunities that arise from combining the two, and once again startups are jumping all over them, along with the big incumbents.</p><p>However you want to make the analogy, what is important to note is that the missing element is the same: demand. ChatGPT took the world by storm, and the use of AI for writing code is both proliferating widely and is extremely high leverage. Every SaaS company in tech, meanwhile, is hard at work at an AI strategy, for the benefit of their sales team if nothing else. That is no small thing, and the exploration and implementation of those strategies will use up a lot of Nvidia GPUs over the next few years. The ultimate question, though, is how much of this AI stuff is actually used, and that is ultimately out of Nvidia’s control.</p><p>My best guess is that the next several years will be occupied building out the most obvious use cases, particularly in the enterprise; the analogy here is to the 2000s build-out of the web. The question, though, is what will be the analogy to mobile (and the cloud), which exploded demand and led to one of the most profitable decades tech has ever seen? The answer may be an already discarded fad: the metaverse.</p>",
          "contentText": "The biggest challenge facing Nvidia is the one that is ultimately out of their control: what does the final market look like?Go back to the dot-com era, and the era that proceeded it. The advent of computing, first in the form of mainframes and then the PC, digitized information, making it endlessly duplicable. Then came the Internet which made the marginal cost of distributing that content go to zero (with the caveat that most people had very low bandwidth). This was an obvious business opportunity that plenty of startups jumped all over, even as telecom companies took on the bandwidth problem; Cisco was the beneficiary of both.The missing element, though, was demand: consistent consumer demand for Internet applications only started to arrive with the advent of broadband connections in the 2000s (thanks in part to a buildout that bankrupted said telecom companies), and then exploded with smartphones a decade later, which made the Internet accessible anytime, anywhere. It was demand that made the router business as big as dot-com investors thought it might be, although by then Cisco had a host of competitors, including large cloud providers who built (and open-sourced) their own.There are lots of potential starting points to choose for AI: machine learning has obviously been a thing for a while, or you might point to the 2017 invention of the transformer; the release of GPT-3 in 2020 was perhaps akin to the release of the Mosaic web browser, which would make ChatGPT the Netscape IPO. One way to categorize this emergence is to characterize training as being akin to digitization in the previous era, and creation — i.e. inference — as akin to distribution. Once again there are obvious business opportunities that arise from combining the two, and once again startups are jumping all over them, along with the big incumbents.However you want to make the analogy, what is important to note is that the missing element is the same: demand. ChatGPT took the world by storm, and the use of AI for writing code is both proliferating widely and is extremely high leverage. Every SaaS company in tech, meanwhile, is hard at work at an AI strategy, for the benefit of their sales team if nothing else. That is no small thing, and the exploration and implementation of those strategies will use up a lot of Nvidia GPUs over the next few years. The ultimate question, though, is how much of this AI stuff is actually used, and that is ultimately out of Nvidia’s control.My best guess is that the next several years will be occupied building out the most obvious use cases, particularly in the enterprise; the analogy here is to the 2000s build-out of the web. The question, though, is what will be the analogy to mobile (and the cloud), which exploded demand and led to one of the most profitable decades tech has ever seen? The answer may be an already discarded fad: the metaverse."
        },
        {
          "subtitle": "A GPU Overhang and the Metaverse",
          "contentHtml": "<p>In April 2022, when Dall-E 2 came out, I wrote <a href=\"https://stratechery.com/2022/dall-e-the-metaverse-and-zero-marginal-content/\">DALL-E, the Metaverse, and Zero Marginal Content</a>, and highlighted three trends:</p><ul>\n<li>First, the gaming industry was increasingly about a few AAA games, small indie titles, and the huge sea of mobile; the limiting factor in further development was the astronomical cost of developing high quality assets.</li>\n<li>Second, social media succeeded by virtue of making content creation free, because users created the content of their own volition.</li>\n<li>Third, TikTok pointed to a future where every individual not only had their own feed, but also where the provenance of that content didn’t matter.</li>\n</ul><p>AI is how those three trends might intersect:</p><blockquote><p>\n  What is fascinating about DALL-E is that it points to a future where these three trends can be combined. DALL-E, at the end of the day, is ultimately a product of human-generated content, just like its GPT-3 cousin. The latter, of course, is about text, while DALL-E is about images. Notice, though, that progression from text to images; it follows that machine learning-generated video is next. This will likely take several years, of course; video is a much more difficult problem, and responsive 3D environments more difficult yet, but this is a path the industry has trod before:</p>\n<ul>\n<li>Game developers pushed the limits on text, then images, then video, then 3D</li>\n<li>Social media drives content creation costs to zero first on text, then images, then video</li>\n<li>Machine learning models can now create text and images for zero marginal cost</li>\n</ul>\n<p>  In the very long run this points to a metaverse vision that is much less deterministic than your typical video game, yet much richer than what is generated on social media. Imagine environments that are not drawn by artists but rather created by AI: this not only increases the possibilities, but crucially, decreases the costs.\n</p></blockquote><p>I wrote in the conclusion:</p><blockquote><p>\n  Machine learning generated content is just the next step beyond TikTok: instead of pulling content from anywhere on the network, GPT and DALL-E and other similar models generate new content from content, at zero marginal cost. This is how the economics of the metaverse will ultimately make sense: virtual worlds need virtual content created at virtually zero cost, fully customizable to the individual.\n</p></blockquote><p>Zero marginal cost is, I should note, aspirational at this point: inference is expensive, both in terms of power and also in terms of the need to pay off all of that money that is showing up on Nvidia’s earnings. It’s possible to imagine a scenario a few years down the line, though, where Nvidia has deployed countless ever more powerful GPUs, and inspired massive competition such that the world’s supply of GPU power far exceeds demand, driving the marginal costs down to the cost of energy (which hopefully will have become cheaper as well); suddenly the idea of making virtual environments on demand won’t seem so far-fetched, opening up entirely new end-user experiences that explode demand in the way that mobile once did.</p>",
          "contentText": "In April 2022, when Dall-E 2 came out, I wrote DALL-E, the Metaverse, and Zero Marginal Content, and highlighted three trends:\nFirst, the gaming industry was increasingly about a few AAA games, small indie titles, and the huge sea of mobile; the limiting factor in further development was the astronomical cost of developing high quality assets.\nSecond, social media succeeded by virtue of making content creation free, because users created the content of their own volition.\nThird, TikTok pointed to a future where every individual not only had their own feed, but also where the provenance of that content didn’t matter.\nAI is how those three trends might intersect:\n  What is fascinating about DALL-E is that it points to a future where these three trends can be combined. DALL-E, at the end of the day, is ultimately a product of human-generated content, just like its GPT-3 cousin. The latter, of course, is about text, while DALL-E is about images. Notice, though, that progression from text to images; it follows that machine learning-generated video is next. This will likely take several years, of course; video is a much more difficult problem, and responsive 3D environments more difficult yet, but this is a path the industry has trod before:\n\nGame developers pushed the limits on text, then images, then video, then 3D\nSocial media drives content creation costs to zero first on text, then images, then video\nMachine learning models can now create text and images for zero marginal cost\n\n  In the very long run this points to a metaverse vision that is much less deterministic than your typical video game, yet much richer than what is generated on social media. Imagine environments that are not drawn by artists but rather created by AI: this not only increases the possibilities, but crucially, decreases the costs.\nI wrote in the conclusion:\n  Machine learning generated content is just the next step beyond TikTok: instead of pulling content from anywhere on the network, GPT and DALL-E and other similar models generate new content from content, at zero marginal cost. This is how the economics of the metaverse will ultimately make sense: virtual worlds need virtual content created at virtually zero cost, fully customizable to the individual.\nZero marginal cost is, I should note, aspirational at this point: inference is expensive, both in terms of power and also in terms of the need to pay off all of that money that is showing up on Nvidia’s earnings. It’s possible to imagine a scenario a few years down the line, though, where Nvidia has deployed countless ever more powerful GPUs, and inspired massive competition such that the world’s supply of GPU power far exceeds demand, driving the marginal costs down to the cost of energy (which hopefully will have become cheaper as well); suddenly the idea of making virtual environments on demand won’t seem so far-fetched, opening up entirely new end-user experiences that explode demand in the way that mobile once did."
        },
        {
          "subtitle": "The GPU Age",
          "contentHtml": "<p>The challenge for Nvidia is that this future isn’t particularly investable; indeed, the idea assumes a capacity overhang at some point, which is not great for the stock price! That, though, is how technology advances, and even if a cliff eventually comes, there is a lot of money to be made in the meantime.</p><p>That noted, the biggest short-term question I have is around Nvidia CEO Jensen Huang’s insistence that the current wave of demand is in fact the dawn of what he calls accelerated computing; from the <a href=\"https://seekingalpha.com/article/4630703-nvidia-corp-nvda-q2-2024-earnings-call-transcript\">Nvidia earnings call</a>:</p><blockquote><p>\n  I’m reluctant to guess about the future and so I’ll answer the question from the first principle of computer science perspective. It is recognized for some time now that…using general purpose computing at scale is no longer the best way to go forward. It’s too energy costly, it’s too expensive, and the performance of the applications are too slow. And finally, the world has a new way of doing it. It’s called accelerated computing and what kicked it into turbocharge is generative AI. But accelerated computing could be used for all kinds of different applications that’s already in the data center. And by using it, you offload the CPUs. You save a ton of money in order of magnitude, in cost and order of magnitude and energy and the throughput is higher and that’s what the industry is really responding to.</p>\n<p>  Going forward, the best way to invest in the data center is to divert the capital investment from general purpose computing and focus it on generative AI and accelerated computing. Generative AI provides a new way of generating productivity, a new way of generating new services to offer to your customers, and accelerated computing helps you save money and save power. And the number of applications is, well, tons. Lots of developers, lots of applications, lots of libraries. It’s ready to be deployed.</p>\n<p>  And so I think the data centers around the world recognize this, that this is the best way to deploy resources, deploy capital going forward for data centers. This is true for the world’s clouds and you’re seeing a whole crop of new GPU-specialized cloud service providers. One of the famous ones is CoreWeave and they’re doing incredibly well. But you’re seeing the regional GPU specialist service providers all over the world now. And it’s because they all recognize the same thing, that the best way to invest their capital going forward is to put it into accelerated computing and generative AI.\n</p></blockquote><p>My interpretation of Huang’s outlook is that all of these GPUs will be used for a lot of the same activities that are currently run on CPUs; that is certainly a bullish view for Nvidia, because it means the capacity overhang that may come from pursuing generative AI will be back-filled by current cloud computing workloads. And, to be fair, Huang has a point about the power and space limitations of current architectures.</p><p>That noted, I’m skeptical: humans — and companies — are lazy, and not only are CPU-based applications easier to develop, they are also mostly already built. I have a hard time seeing what companies are going to go through the time and effort to port things that already run on CPUs to GPUs; at the end of the day, the applications that run in a cloud are determined by customers who provide the demand for cloud resources, not cloud providers looking to optimize FLOP/rack.</p><p>If GPUs are going to be as big of a market as Nvidia’s investors hope it will be, it will be because applications that are only possible with GPUs generate the demand to make it so. I’m confident that time will come; what I, nor Huang, nor anyone else can be sure of is when that time will arrive.</p>",
          "contentText": "The challenge for Nvidia is that this future isn’t particularly investable; indeed, the idea assumes a capacity overhang at some point, which is not great for the stock price! That, though, is how technology advances, and even if a cliff eventually comes, there is a lot of money to be made in the meantime.That noted, the biggest short-term question I have is around Nvidia CEO Jensen Huang’s insistence that the current wave of demand is in fact the dawn of what he calls accelerated computing; from the Nvidia earnings call:\n  I’m reluctant to guess about the future and so I’ll answer the question from the first principle of computer science perspective. It is recognized for some time now that…using general purpose computing at scale is no longer the best way to go forward. It’s too energy costly, it’s too expensive, and the performance of the applications are too slow. And finally, the world has a new way of doing it. It’s called accelerated computing and what kicked it into turbocharge is generative AI. But accelerated computing could be used for all kinds of different applications that’s already in the data center. And by using it, you offload the CPUs. You save a ton of money in order of magnitude, in cost and order of magnitude and energy and the throughput is higher and that’s what the industry is really responding to.\n  Going forward, the best way to invest in the data center is to divert the capital investment from general purpose computing and focus it on generative AI and accelerated computing. Generative AI provides a new way of generating productivity, a new way of generating new services to offer to your customers, and accelerated computing helps you save money and save power. And the number of applications is, well, tons. Lots of developers, lots of applications, lots of libraries. It’s ready to be deployed.\n  And so I think the data centers around the world recognize this, that this is the best way to deploy resources, deploy capital going forward for data centers. This is true for the world’s clouds and you’re seeing a whole crop of new GPU-specialized cloud service providers. One of the famous ones is CoreWeave and they’re doing incredibly well. But you’re seeing the regional GPU specialist service providers all over the world now. And it’s because they all recognize the same thing, that the best way to invest their capital going forward is to put it into accelerated computing and generative AI.\nMy interpretation of Huang’s outlook is that all of these GPUs will be used for a lot of the same activities that are currently run on CPUs; that is certainly a bullish view for Nvidia, because it means the capacity overhang that may come from pursuing generative AI will be back-filled by current cloud computing workloads. And, to be fair, Huang has a point about the power and space limitations of current architectures.That noted, I’m skeptical: humans — and companies — are lazy, and not only are CPU-based applications easier to develop, they are also mostly already built. I have a hard time seeing what companies are going to go through the time and effort to port things that already run on CPUs to GPUs; at the end of the day, the applications that run in a cloud are determined by customers who provide the demand for cloud resources, not cloud providers looking to optimize FLOP/rack.If GPUs are going to be as big of a market as Nvidia’s investors hope it will be, it will be because applications that are only possible with GPUs generate the demand to make it so. I’m confident that time will come; what I, nor Huang, nor anyone else can be sure of is when that time will arrive."
        }
      ]
    },
    {
      "title": "Disney’s Taylor Swift Era",
      "publishedDate": "2023-08-15T07:10:33-07:00",
      "updatedDate": "2023-08-15T08:44:18-07:00",
      "contentHtml": "\n\t\t<p>Bill Simmons had a quick aside about Taylor Swift on <a href=\"https://www.theringer.com/the-bill-simmons-podcast/2023/8/6/23822477/usa-world-cup-collapse-destination-nba-g-league-odyssey-the-o-c-20th-anniversary-nba-expansion-draft\">the most recent episode</a> of his eponymous podcast.</p>\n<blockquote><p>\n  I have never seen anything like the phenomenon around this concert tour, and I have been alive for all the concert tours since the mid-70s…from a cultural standpoint, from a multi-generation standpoint: fathers and daughters, daughters and moms, multiple generations. You have people like my daughter who is 18, who has not even known life without a Taylor Swift song, and then you have people in their 30s who grew up with her, and then you have the moms who are used to listening with the daughters, and then the show itself: I had friends who went to the first show, and I think she played 45 songs — it was 3+ hours. This is like Michael Jordan shit, whatever is happening with her…</p>\n<p>  She’s sold out 6 straight shows here in Los Angeles; I’ve been here for 21 years I can’t remember anything as important as these Taylor Swift tickets, just being in the building for that. People coming from all parts of California to go, it’s really something. This is the summer of Taylor.\n</p></blockquote>\n<p>There is much to say about the summer of Taylor that is pertinent to technology and the Internet: start with the desire for communal in-person experiences driven not only by the pandemic, but also the general fracturing of culture inherent in a media landscape where personalized content delivered directly to your personal device is the norm. Then there is the way in which social media acts as a FOMO<sup id=\"rf1-11409\"><a href=\"#fn1-11409\" title=\"Fear Of Missing Out\" rel=\"footnote\">1</a></sup> generator: being able to access every moment of every show on social media doesn’t decrease the value of attending the show, but rather increases the desire to obtain a scarce number of tickets to see the spectacle in person. And, it must be said, there is the excellence at play: not only does Swift have an incredible catalog of popular songs, the show itself spares no expense — or exertion on Swift’s part — to give the fans exactly what they were hoping for.</p>\n<p>What made the final LA show unique though (which I had the good fortune of attending with my daughter), was the announcement of <em>1989 (Taylor’s Version)</em>. It wasn’t <a href=\"https://www.threads.net/@nathanchubbard/post/Cvv80rhuZ8W/?igshid=MTc4MmM1YmI2Ng%3D%3D\">exactly a secret</a> that the announcement was coming on 8/9, and obviously Swift’s ongoing project to re-release her old albums is <a href=\"https://stratechery.com/2021/non-fungible-taylor-swift/\">well-documented at this point</a>. What is surprising is just how much people care: <em>Speak Now (Taylor’s Version)</em>, which was announced earlier in the tour, hit number one on the Billboard charts, giving Swift <a href=\"https://www.nytimes.com/2023/07/17/arts/music/taylor-swift-speak-now-billboard-chart-record.html\">more number one albums than any woman in history</a>; it seems inevitable that <em>1989 (Taylor’s Version)</em> will be lucky number 13 for her career.</p>\n<h3>Taylor’s Versions</h3>\n<p>What is striking about the popularity of these re-releases is that it is the latest manifestation of Swift’s insistence that the opportunities for musicians are greater than ever. I had never really listened to Swift’s music when she wrote an editorial in the Wall Street Journal in 2014 entitled <a href=\"https://www.wsj.com/articles/for-taylor-swift-the-future-of-music-is-a-love-story-1404763219\">For Taylor Swift, the Future of Music is a Love Story</a>:</p>\n<blockquote><p>\n  Where will the music industry be in 20 years, 30 years, 50 years?</p>\n<p>  Before I tell you my thoughts on the matter, you should know that you’re reading the opinion of an enthusiastic optimist: one of the few living souls in the music industry who still believes that the music industry is not dying…it’s just coming alive.</p>\n<p>  There are many (many) people who predict the downfall of music sales and the irrelevancy of the album as an economic entity. I am not one of them. In my opinion, the value of an album is, and will continue to be, based on the amount of heart and soul an artist has bled into a body of work, and the financial value that artists (and their labels) place on their music when it goes out into the marketplace. Piracy, file sharing and streaming have shrunk the numbers of paid album sales drastically, and every artist has handled this blow differently.</p>\n<p>  In recent years, you’ve probably read the articles about major recording artists who have decided to practically give their music away, for this promotion or that exclusive deal. My hope for the future, not just in the music industry, but in every young girl I meet…is that they all realize their worth and ask for it.</p>\n<p>  Music is art, and art is important and rare. Important, rare things are valuable. Valuable things should be paid for. It’s my opinion that music should not be free, and my prediction is that individual artists and their labels will someday decide what an album’s price point is. I hope they don’t underestimate themselves or undervalue their art.\n</p></blockquote>\n<p>I had two reactions to this editorial. The first echoed Nilay Patel’s take that <a href=\"https://www.vox.com/2014/7/7/5878603/taylor-swift-doesnt-understand-supply-and-demand\">Taylor Swift doesn’t understand supply and demand</a>:</p>\n<blockquote><p>\n  This might make sense if you’re Taylor Swift and your enormous army of fans will pre-order anything you tell them to, but the most important lesson of the Internet music revolution is that the vast majority of consumers actually reward <em>convenience</em>. That’s why the iPod was a huge hit even though digitally-compressed music sounded terrible at the time, and it’s why teenagers today get most of their music on YouTube, even though YouTube sounds worse still. It’s also why the album is dead: you can’t sell a handful of singles and some okayish filler songs to people for $10 or $15 or $25 anymore, because convenient Internet music distribution has utterly destroyed the need to bundle everything together. You can just Google the singles and listen to them for free…</p>\n<p>  The single hardest economic problem posed by the internet is the end of scarcity. Even just 10 years ago, most people experienced a one-to-one relationship between creative works and the physical objects they were delivered on: your music came on CDs, your movies came on DVDs, and your news came on printed magazines and newspapers. Since there’s a scarce number of these objects in the world, it’s easy to buy and sell them, because their prices will follow the laws of supply and demand: limited-edition vinyl records will be more expensive than regular CDs, because there are simply fewer of them. If you wanted a CD full of songs in 1995, you went to a store and paid for them, because there was essentially no other way to get those songs. Even if you wanted to steal the music, you had to pay some price: you needed to have a friend with the right CD, and you needed time and blank CDs to make a copy.</p>\n<p>  But on the internet, there’s no scarcity:  there’s an endless amount of everything available to everyone. The laws of supply and demand don’t work terribly well when there’s infinite supply. Swift is right that “important, rare things are valuable,” but she’s failed to understand that the idea of rarity simply doesn’t exist in the digital marketplace.\n</p></blockquote>\n<p>All fair points! Swift, though, persisted: later that year she pulled her music from Spotify, which meant fans had to actually buy the original <em>1989</em> when it came out in October (it was my first Swift purchase); what struck me about this move, in conjunction with the editorial, was that this was, from a certain perspective, a gift she was giving her fans. From <a href=\"https://stratechery.com/2014/daily-update-uber-v-lyft-nyc-defending-surge-pricing-taylor-swift-music/\">an Update in November 2014</a>:</p>\n<blockquote><p>\n  Swift has long since proved herself a master at building a connection with fans, engaging on social media, customizing her concerts, and spilling her secrets through her songs. And, for&nbsp;<em>1989</em>&nbsp;she&nbsp;<a href=\"http://www.hollywoodtake.com/taylor-swifts-1989-secret-sessions-are-brilliant-why-shake-it-style-and-out-woods-are-59108\">took things to a new level</a>. What I think Swift has realized, though, is that reaching out to your fans is not enough: it has to be reciprocal: what selling an album for actual cash money does is give people a way to commit. They are quite literally giving Swift something valuable in exchange for her work…</p>\n<p>  The problem with Spotify is that at a very fundamental level it treats music as a commodity. You can’t choose where your $10/month goes based on the emotional impact of a song; the hot new hit by the artist you’ll never hear from again is treated exactly the same as the artist that was with you when you needed him or her the most. It cheapens the connection, not by withholding money per se, but by denying the commitment inherent in an explicit purchase…</p>\n<p>  And so, Swift has put up something of a door: access to her costs fans at least $12.99. Here’s the thing about doors though: while they keep people out, they also keep them in. Simply by virtue of having paid money directly to Swift for Swift’s music that album is already more meaningful to its 1 million buyers than the exact same music would have been were it listened to via Spotify’s all-you-can-eat subscription (or free with ads!), no matter how much money Ek and team pay out.\n</p></blockquote>\n<p>A few months later, though, and <em>1989</em> was on Spotify; every album since then has been also, and <em>1989 (Taylor’s Version)</em> will be as well. Something funny will happen, though, when <em>1989 (Taylor’s Version)</em> is released: streams of 1989 will plummet, while <em>1989 (Taylor’s Version)</em> will shoot to the top of the charts; the realities of music are such that not even Swift can hold out on streaming,<sup id=\"rf2-11409\"><a href=\"#fn2-11409\" title=\"It is worth noting that Swift’s fans still bought <a href=&quot;https://www.nytimes.com/2022/10/31/arts/music/taylor-swift-midnights-billboard-chart.html&quot;>over 1 million physical copies of her most recent album</a>.\" rel=\"footnote\">2</a></sup> but she has still given her fans the capability of reciprocating their relationship by making the conscious decision to only listen to (Taylor’s Version)s.</p>\n<p>Still, the value of a streaming choice doesn’t go that far; Patel’s economic argument was right, after all. The real money for Swift comes from the concerts, with the <a href=\"https://www.wsj.com/articles/taylor-swift-eras-tour-money-511fdfcf\">Eras Tour set to be the first to gross $1 billion</a>; physical scarcity is still the best way for a creator to capture value.</p>\n<h3>Disney’s Earnings</h3>\n<p>Last week Disney <a href=\"https://www.wsj.com/articles/disney-raising-prices-hulu-espn-earnings-call-64ae262\">reported earnings</a>, including a 23% decline in profit in its traditional TV business; that was more than made up for by an 11% increase in profit in its parks, experiences, and products segment, which accounted for 68% of Disney’s profit. Disney’s theme parks and cruises have always been an essential part of the Disney model; from a <a href=\"https://stratechery.com/2017/disney-shifts-to-streaming-disney-versus-netflix-espn/\">2017 Update</a>:</p>\n<blockquote><p>\n  The answer reminded me of this famous chart Walt Disney created to show how the Disney business worked:</p>\n<p>  <img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2017/08/disney-2-thumb-580x507-4071.jpeg?resize=580%2C507&amp;ssl=1\" alt=\"Walt Disney's Disney Map\" width=\"580\" height=\"507\" class=\"aligncenter size-full wp-image-2700\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2017/08/disney-2-thumb-580x507-4071.jpeg?w=580&amp;ssl=1 580w, https://i0.wp.com/stratechery.com/wp-content/uploads/2017/08/disney-2-thumb-580x507-4071.jpeg?resize=300%2C262&amp;ssl=1 300w\" sizes=\"(max-width: 580px) 100vw, 580px\" data-recalc-dims=\"1\"></p>\n<p>  At the center, of course, are the Disney Studios, and rightly so. Not only does differentiated content drive movie theater revenue, it creates the universes and characters that earn TV licensing revenue, music recording revenue, and merchandise sales.</p>\n<p>  What has always made Disney unique, though, is Disneyland: there the differentiated content comes to life, and, given the lack of an arrow, I suspect not even Walt Disney himself appreciated the extent to which theme parks and the connection with the customer they engendered drive the rest of the business. “Disney” is just as much of a brand as is Mickey Mouse or Buzz Lightyear, with stores, a cable channel, and a reason to watch a movie even if you know nothing about it.\n</p></blockquote>\n<p>It was the theme park angle that made me excited about Disney+; I wrote <a href=\"https://stratechery.com/2019/disney-and-the-future-of-tv/\">in 2019</a>:</p>\n<blockquote><p>\n  This is the only appropriate context in which to think about Disney+. While obviously Disney+ will compete with Netflix for consumer attention, the goals of the two services are very different: for Netflix, streaming is its entire business, the sole driver of revenue and profit. Disney, meanwhile, obviously plans for Disney+ to be profitable — the company projects that the service will achieve profitability in 2024, and that includes transfer payments to Disney’s studios — but the larger project is Disney itself.</p>\n<p>  By controlling distribution of its content and going direct-to-consumer, Disney can deepen its already strong connections with customers in a way that benefits all parts of the business: movies can beget original content on Disney+ which begets new attractions at theme parks which begets merchandising opportunities which begets new movies, all building on each other like a cinematic universe in real life. Indeed, it is a testament to just how lucrative the traditional TV model is that it took so long for Disney to shift to this approach: it is a far better fit for their business in the long run than simply spreading content around to the highest bidder.\n</p></blockquote>\n<p>I think, in retrospect, that this was an example of my falling in love with elegance and spending insufficient time in spreadsheets: Walt Disney’s chart may have been a satisfying business model, but the reality of Disney’s TV business is that it was scalable in a way that that Disney chart never could be. The beauty of the cable bundle is that nearly every household in America paid for it every single month, regardless of whether or not Disney had a hit TV show or a must-watch sporting event; thanks to its suite of channels, anchored by ESPN, Disney received a big chunk of that money, and it grew like clockwork. In that world Walt Disney’s model was a nice side business to the real money-maker — that’s a pretty good reason for Disney to have held on to that model as long as they did.</p>\n<p>In fact, you can very much make the case that Disney and all of its peers <a href=\"https://stratechery.com/2023/hollywood-on-strike/\">ought to have held on longer</a>: yes, streaming — i.e. Netflix — leveraged the Internet for distribution of video, but that didn’t mean that Disney and Time Warner and Paramount and all of the rest had to. Those Netflix multiples, though, which far exceeded anyone else’s in Hollywood, were too tempting, and soon enough everyone was putting their best content on streaming services, leaving the cable bundle to wither.</p>\n<h3>Disney’s Taylor Swift Era</h3>\n<p>The end result is the inversion you see in Disney’s recent results. Disney is, from this point forward, not much different than Taylor Swift: sure, there is money to be made (hopefully) in areas like streaming, but the real durable value and outsized profits will come from real life experiences. This is, to be sure, a good business, but it has its limits: it is remarkable that Swift performed six shows in seven nights in Los Angeles, but it was still only six shows; concerts don’t scale like CD sales used to. Disney, similarly, only has so many theme parks, that only accommodate so many people, and operating those theme parks takes significant ongoing resources.</p>\n<p>It’s interesting, then, to observe how differently Swift and Disney are perceived at this moment in time: I opened with Simmons analogizing Swift to Jordan, and I think it’s a fair comparison; the reality of the fractured world wrought by the Internet is that any star who can emerge from the noise becomes bigger than anything we have seen before, from hunger for a unifying experience if nothing else, and admission to that experience becomes valuable through unprecedented demand combined with physically limited supply.</p>\n<p>That limitation, though, implies a lack of scale, which means that Swift is as big as she will ever be; that’s ok, because it’s bigger than anyone has ever has been. Disney, meanwhile, may have its own physical experiences, made valuable by their scarcity, but it will never be as valuable as owning distribution. The best thing Iger can do now is <a href=\"https://stratechery.com/2023/bob-iger-on-cnbc-the-end-of-linear-tv-espn-and-strategic-partnerships-and-apple/\">move the company on</a> from the heights it once reached; maybe someday Disney and its investors will <a href=\"https://www.youtube.com/watch?v=p1cEvNn88jM\">forget that those outsized profits ever existed</a>.</p>\n<hr class=\"footnotes\"><ol class=\"footnotes\" style=\"list-style-type:decimal\"><li id=\"fn1-11409\"><p>Fear Of Missing Out&nbsp;<a href=\"#rf1-11409\" class=\"backlink\" title=\"Return to footnote 1.\">↩</a></p></li><li id=\"fn2-11409\"><p>It is worth noting that Swift’s fans still bought <a href=\"https://www.nytimes.com/2022/10/31/arts/music/taylor-swift-midnights-billboard-chart.html\">over 1 million physical copies of her most recent album</a>.&nbsp;<a href=\"#rf2-11409\" class=\"backlink\" title=\"Return to footnote 2.\">↩</a></p></li></ol>\n\t\t\t",
      "contentText": "\n\t\tBill Simmons had a quick aside about Taylor Swift on the most recent episode of his eponymous podcast.\n\n  I have never seen anything like the phenomenon around this concert tour, and I have been alive for all the concert tours since the mid-70s…from a cultural standpoint, from a multi-generation standpoint: fathers and daughters, daughters and moms, multiple generations. You have people like my daughter who is 18, who has not even known life without a Taylor Swift song, and then you have people in their 30s who grew up with her, and then you have the moms who are used to listening with the daughters, and then the show itself: I had friends who went to the first show, and I think she played 45 songs — it was 3+ hours. This is like Michael Jordan shit, whatever is happening with her…\n  She’s sold out 6 straight shows here in Los Angeles; I’ve been here for 21 years I can’t remember anything as important as these Taylor Swift tickets, just being in the building for that. People coming from all parts of California to go, it’s really something. This is the summer of Taylor.\n\nThere is much to say about the summer of Taylor that is pertinent to technology and the Internet: start with the desire for communal in-person experiences driven not only by the pandemic, but also the general fracturing of culture inherent in a media landscape where personalized content delivered directly to your personal device is the norm. Then there is the way in which social media acts as a FOMO1 generator: being able to access every moment of every show on social media doesn’t decrease the value of attending the show, but rather increases the desire to obtain a scarce number of tickets to see the spectacle in person. And, it must be said, there is the excellence at play: not only does Swift have an incredible catalog of popular songs, the show itself spares no expense — or exertion on Swift’s part — to give the fans exactly what they were hoping for.\nWhat made the final LA show unique though (which I had the good fortune of attending with my daughter), was the announcement of 1989 (Taylor’s Version). It wasn’t exactly a secret that the announcement was coming on 8/9, and obviously Swift’s ongoing project to re-release her old albums is well-documented at this point. What is surprising is just how much people care: Speak Now (Taylor’s Version), which was announced earlier in the tour, hit number one on the Billboard charts, giving Swift more number one albums than any woman in history; it seems inevitable that 1989 (Taylor’s Version) will be lucky number 13 for her career.\nTaylor’s Versions\nWhat is striking about the popularity of these re-releases is that it is the latest manifestation of Swift’s insistence that the opportunities for musicians are greater than ever. I had never really listened to Swift’s music when she wrote an editorial in the Wall Street Journal in 2014 entitled For Taylor Swift, the Future of Music is a Love Story:\n\n  Where will the music industry be in 20 years, 30 years, 50 years?\n  Before I tell you my thoughts on the matter, you should know that you’re reading the opinion of an enthusiastic optimist: one of the few living souls in the music industry who still believes that the music industry is not dying…it’s just coming alive.\n  There are many (many) people who predict the downfall of music sales and the irrelevancy of the album as an economic entity. I am not one of them. In my opinion, the value of an album is, and will continue to be, based on the amount of heart and soul an artist has bled into a body of work, and the financial value that artists (and their labels) place on their music when it goes out into the marketplace. Piracy, file sharing and streaming have shrunk the numbers of paid album sales drastically, and every artist has handled this blow differently.\n  In recent years, you’ve probably read the articles about major recording artists who have decided to practically give their music away, for this promotion or that exclusive deal. My hope for the future, not just in the music industry, but in every young girl I meet…is that they all realize their worth and ask for it.\n  Music is art, and art is important and rare. Important, rare things are valuable. Valuable things should be paid for. It’s my opinion that music should not be free, and my prediction is that individual artists and their labels will someday decide what an album’s price point is. I hope they don’t underestimate themselves or undervalue their art.\n\nI had two reactions to this editorial. The first echoed Nilay Patel’s take that Taylor Swift doesn’t understand supply and demand:\n\n  This might make sense if you’re Taylor Swift and your enormous army of fans will pre-order anything you tell them to, but the most important lesson of the Internet music revolution is that the vast majority of consumers actually reward convenience. That’s why the iPod was a huge hit even though digitally-compressed music sounded terrible at the time, and it’s why teenagers today get most of their music on YouTube, even though YouTube sounds worse still. It’s also why the album is dead: you can’t sell a handful of singles and some okayish filler songs to people for $10 or $15 or $25 anymore, because convenient Internet music distribution has utterly destroyed the need to bundle everything together. You can just Google the singles and listen to them for free…\n  The single hardest economic problem posed by the internet is the end of scarcity. Even just 10 years ago, most people experienced a one-to-one relationship between creative works and the physical objects they were delivered on: your music came on CDs, your movies came on DVDs, and your news came on printed magazines and newspapers. Since there’s a scarce number of these objects in the world, it’s easy to buy and sell them, because their prices will follow the laws of supply and demand: limited-edition vinyl records will be more expensive than regular CDs, because there are simply fewer of them. If you wanted a CD full of songs in 1995, you went to a store and paid for them, because there was essentially no other way to get those songs. Even if you wanted to steal the music, you had to pay some price: you needed to have a friend with the right CD, and you needed time and blank CDs to make a copy.\n  But on the internet, there’s no scarcity:  there’s an endless amount of everything available to everyone. The laws of supply and demand don’t work terribly well when there’s infinite supply. Swift is right that “important, rare things are valuable,” but she’s failed to understand that the idea of rarity simply doesn’t exist in the digital marketplace.\n\nAll fair points! Swift, though, persisted: later that year she pulled her music from Spotify, which meant fans had to actually buy the original 1989 when it came out in October (it was my first Swift purchase); what struck me about this move, in conjunction with the editorial, was that this was, from a certain perspective, a gift she was giving her fans. From an Update in November 2014:\n\n  Swift has long since proved herself a master at building a connection with fans, engaging on social media, customizing her concerts, and spilling her secrets through her songs. And, for 1989 she took things to a new level. What I think Swift has realized, though, is that reaching out to your fans is not enough: it has to be reciprocal: what selling an album for actual cash money does is give people a way to commit. They are quite literally giving Swift something valuable in exchange for her work…\n  The problem with Spotify is that at a very fundamental level it treats music as a commodity. You can’t choose where your $10/month goes based on the emotional impact of a song; the hot new hit by the artist you’ll never hear from again is treated exactly the same as the artist that was with you when you needed him or her the most. It cheapens the connection, not by withholding money per se, but by denying the commitment inherent in an explicit purchase…\n  And so, Swift has put up something of a door: access to her costs fans at least $12.99. Here’s the thing about doors though: while they keep people out, they also keep them in. Simply by virtue of having paid money directly to Swift for Swift’s music that album is already more meaningful to its 1 million buyers than the exact same music would have been were it listened to via Spotify’s all-you-can-eat subscription (or free with ads!), no matter how much money Ek and team pay out.\n\nA few months later, though, and 1989 was on Spotify; every album since then has been also, and 1989 (Taylor’s Version) will be as well. Something funny will happen, though, when 1989 (Taylor’s Version) is released: streams of 1989 will plummet, while 1989 (Taylor’s Version) will shoot to the top of the charts; the realities of music are such that not even Swift can hold out on streaming,2 but she has still given her fans the capability of reciprocating their relationship by making the conscious decision to only listen to (Taylor’s Version)s.\nStill, the value of a streaming choice doesn’t go that far; Patel’s economic argument was right, after all. The real money for Swift comes from the concerts, with the Eras Tour set to be the first to gross $1 billion; physical scarcity is still the best way for a creator to capture value.\nDisney’s Earnings\nLast week Disney reported earnings, including a 23% decline in profit in its traditional TV business; that was more than made up for by an 11% increase in profit in its parks, experiences, and products segment, which accounted for 68% of Disney’s profit. Disney’s theme parks and cruises have always been an essential part of the Disney model; from a 2017 Update:\n\n  The answer reminded me of this famous chart Walt Disney created to show how the Disney business worked:\n  \n  At the center, of course, are the Disney Studios, and rightly so. Not only does differentiated content drive movie theater revenue, it creates the universes and characters that earn TV licensing revenue, music recording revenue, and merchandise sales.\n  What has always made Disney unique, though, is Disneyland: there the differentiated content comes to life, and, given the lack of an arrow, I suspect not even Walt Disney himself appreciated the extent to which theme parks and the connection with the customer they engendered drive the rest of the business. “Disney” is just as much of a brand as is Mickey Mouse or Buzz Lightyear, with stores, a cable channel, and a reason to watch a movie even if you know nothing about it.\n\nIt was the theme park angle that made me excited about Disney+; I wrote in 2019:\n\n  This is the only appropriate context in which to think about Disney+. While obviously Disney+ will compete with Netflix for consumer attention, the goals of the two services are very different: for Netflix, streaming is its entire business, the sole driver of revenue and profit. Disney, meanwhile, obviously plans for Disney+ to be profitable — the company projects that the service will achieve profitability in 2024, and that includes transfer payments to Disney’s studios — but the larger project is Disney itself.\n  By controlling distribution of its content and going direct-to-consumer, Disney can deepen its already strong connections with customers in a way that benefits all parts of the business: movies can beget original content on Disney+ which begets new attractions at theme parks which begets merchandising opportunities which begets new movies, all building on each other like a cinematic universe in real life. Indeed, it is a testament to just how lucrative the traditional TV model is that it took so long for Disney to shift to this approach: it is a far better fit for their business in the long run than simply spreading content around to the highest bidder.\n\nI think, in retrospect, that this was an example of my falling in love with elegance and spending insufficient time in spreadsheets: Walt Disney’s chart may have been a satisfying business model, but the reality of Disney’s TV business is that it was scalable in a way that that Disney chart never could be. The beauty of the cable bundle is that nearly every household in America paid for it every single month, regardless of whether or not Disney had a hit TV show or a must-watch sporting event; thanks to its suite of channels, anchored by ESPN, Disney received a big chunk of that money, and it grew like clockwork. In that world Walt Disney’s model was a nice side business to the real money-maker — that’s a pretty good reason for Disney to have held on to that model as long as they did.\nIn fact, you can very much make the case that Disney and all of its peers ought to have held on longer: yes, streaming — i.e. Netflix — leveraged the Internet for distribution of video, but that didn’t mean that Disney and Time Warner and Paramount and all of the rest had to. Those Netflix multiples, though, which far exceeded anyone else’s in Hollywood, were too tempting, and soon enough everyone was putting their best content on streaming services, leaving the cable bundle to wither.\nDisney’s Taylor Swift Era\nThe end result is the inversion you see in Disney’s recent results. Disney is, from this point forward, not much different than Taylor Swift: sure, there is money to be made (hopefully) in areas like streaming, but the real durable value and outsized profits will come from real life experiences. This is, to be sure, a good business, but it has its limits: it is remarkable that Swift performed six shows in seven nights in Los Angeles, but it was still only six shows; concerts don’t scale like CD sales used to. Disney, similarly, only has so many theme parks, that only accommodate so many people, and operating those theme parks takes significant ongoing resources.\nIt’s interesting, then, to observe how differently Swift and Disney are perceived at this moment in time: I opened with Simmons analogizing Swift to Jordan, and I think it’s a fair comparison; the reality of the fractured world wrought by the Internet is that any star who can emerge from the noise becomes bigger than anything we have seen before, from hunger for a unifying experience if nothing else, and admission to that experience becomes valuable through unprecedented demand combined with physically limited supply.\nThat limitation, though, implies a lack of scale, which means that Swift is as big as she will ever be; that’s ok, because it’s bigger than anyone has ever has been. Disney, meanwhile, may have its own physical experiences, made valuable by their scarcity, but it will never be as valuable as owning distribution. The best thing Iger can do now is move the company on from the heights it once reached; maybe someday Disney and its investors will forget that those outsized profits ever existed.\nFear Of Missing Out ↩It is worth noting that Swift’s fans still bought over 1 million physical copies of her most recent album. ↩\n\t\t\t",
      "subsections": [
        {
          "subtitle": "Taylor’s Versions",
          "contentHtml": "<p>What is striking about the popularity of these re-releases is that it is the latest manifestation of Swift’s insistence that the opportunities for musicians are greater than ever. I had never really listened to Swift’s music when she wrote an editorial in the Wall Street Journal in 2014 entitled <a href=\"https://www.wsj.com/articles/for-taylor-swift-the-future-of-music-is-a-love-story-1404763219\">For Taylor Swift, the Future of Music is a Love Story</a>:</p><blockquote><p>\n  Where will the music industry be in 20 years, 30 years, 50 years?</p>\n<p>  Before I tell you my thoughts on the matter, you should know that you’re reading the opinion of an enthusiastic optimist: one of the few living souls in the music industry who still believes that the music industry is not dying…it’s just coming alive.</p>\n<p>  There are many (many) people who predict the downfall of music sales and the irrelevancy of the album as an economic entity. I am not one of them. In my opinion, the value of an album is, and will continue to be, based on the amount of heart and soul an artist has bled into a body of work, and the financial value that artists (and their labels) place on their music when it goes out into the marketplace. Piracy, file sharing and streaming have shrunk the numbers of paid album sales drastically, and every artist has handled this blow differently.</p>\n<p>  In recent years, you’ve probably read the articles about major recording artists who have decided to practically give their music away, for this promotion or that exclusive deal. My hope for the future, not just in the music industry, but in every young girl I meet…is that they all realize their worth and ask for it.</p>\n<p>  Music is art, and art is important and rare. Important, rare things are valuable. Valuable things should be paid for. It’s my opinion that music should not be free, and my prediction is that individual artists and their labels will someday decide what an album’s price point is. I hope they don’t underestimate themselves or undervalue their art.\n</p></blockquote><p>I had two reactions to this editorial. The first echoed Nilay Patel’s take that <a href=\"https://www.vox.com/2014/7/7/5878603/taylor-swift-doesnt-understand-supply-and-demand\">Taylor Swift doesn’t understand supply and demand</a>:</p><blockquote><p>\n  This might make sense if you’re Taylor Swift and your enormous army of fans will pre-order anything you tell them to, but the most important lesson of the Internet music revolution is that the vast majority of consumers actually reward <em>convenience</em>. That’s why the iPod was a huge hit even though digitally-compressed music sounded terrible at the time, and it’s why teenagers today get most of their music on YouTube, even though YouTube sounds worse still. It’s also why the album is dead: you can’t sell a handful of singles and some okayish filler songs to people for $10 or $15 or $25 anymore, because convenient Internet music distribution has utterly destroyed the need to bundle everything together. You can just Google the singles and listen to them for free…</p>\n<p>  The single hardest economic problem posed by the internet is the end of scarcity. Even just 10 years ago, most people experienced a one-to-one relationship between creative works and the physical objects they were delivered on: your music came on CDs, your movies came on DVDs, and your news came on printed magazines and newspapers. Since there’s a scarce number of these objects in the world, it’s easy to buy and sell them, because their prices will follow the laws of supply and demand: limited-edition vinyl records will be more expensive than regular CDs, because there are simply fewer of them. If you wanted a CD full of songs in 1995, you went to a store and paid for them, because there was essentially no other way to get those songs. Even if you wanted to steal the music, you had to pay some price: you needed to have a friend with the right CD, and you needed time and blank CDs to make a copy.</p>\n<p>  But on the internet, there’s no scarcity:  there’s an endless amount of everything available to everyone. The laws of supply and demand don’t work terribly well when there’s infinite supply. Swift is right that “important, rare things are valuable,” but she’s failed to understand that the idea of rarity simply doesn’t exist in the digital marketplace.\n</p></blockquote><p>All fair points! Swift, though, persisted: later that year she pulled her music from Spotify, which meant fans had to actually buy the original <em>1989</em> when it came out in October (it was my first Swift purchase); what struck me about this move, in conjunction with the editorial, was that this was, from a certain perspective, a gift she was giving her fans. From <a href=\"https://stratechery.com/2014/daily-update-uber-v-lyft-nyc-defending-surge-pricing-taylor-swift-music/\">an Update in November 2014</a>:</p><blockquote><p>\n  Swift has long since proved herself a master at building a connection with fans, engaging on social media, customizing her concerts, and spilling her secrets through her songs. And, for&nbsp;<em>1989</em>&nbsp;she&nbsp;<a href=\"http://www.hollywoodtake.com/taylor-swifts-1989-secret-sessions-are-brilliant-why-shake-it-style-and-out-woods-are-59108\">took things to a new level</a>. What I think Swift has realized, though, is that reaching out to your fans is not enough: it has to be reciprocal: what selling an album for actual cash money does is give people a way to commit. They are quite literally giving Swift something valuable in exchange for her work…</p>\n<p>  The problem with Spotify is that at a very fundamental level it treats music as a commodity. You can’t choose where your $10/month goes based on the emotional impact of a song; the hot new hit by the artist you’ll never hear from again is treated exactly the same as the artist that was with you when you needed him or her the most. It cheapens the connection, not by withholding money per se, but by denying the commitment inherent in an explicit purchase…</p>\n<p>  And so, Swift has put up something of a door: access to her costs fans at least $12.99. Here’s the thing about doors though: while they keep people out, they also keep them in. Simply by virtue of having paid money directly to Swift for Swift’s music that album is already more meaningful to its 1 million buyers than the exact same music would have been were it listened to via Spotify’s all-you-can-eat subscription (or free with ads!), no matter how much money Ek and team pay out.\n</p></blockquote><p>A few months later, though, and <em>1989</em> was on Spotify; every album since then has been also, and <em>1989 (Taylor’s Version)</em> will be as well. Something funny will happen, though, when <em>1989 (Taylor’s Version)</em> is released: streams of 1989 will plummet, while <em>1989 (Taylor’s Version)</em> will shoot to the top of the charts; the realities of music are such that not even Swift can hold out on streaming,<sup id=\"rf2-11409\"><a href=\"#fn2-11409\" title=\"It is worth noting that Swift’s fans still bought <a href=&quot;https://www.nytimes.com/2022/10/31/arts/music/taylor-swift-midnights-billboard-chart.html&quot;>over 1 million physical copies of her most recent album</a>.\" rel=\"footnote\">2</a></sup> but she has still given her fans the capability of reciprocating their relationship by making the conscious decision to only listen to (Taylor’s Version)s.</p><p>Still, the value of a streaming choice doesn’t go that far; Patel’s economic argument was right, after all. The real money for Swift comes from the concerts, with the <a href=\"https://www.wsj.com/articles/taylor-swift-eras-tour-money-511fdfcf\">Eras Tour set to be the first to gross $1 billion</a>; physical scarcity is still the best way for a creator to capture value.</p>",
          "contentText": "What is striking about the popularity of these re-releases is that it is the latest manifestation of Swift’s insistence that the opportunities for musicians are greater than ever. I had never really listened to Swift’s music when she wrote an editorial in the Wall Street Journal in 2014 entitled For Taylor Swift, the Future of Music is a Love Story:\n  Where will the music industry be in 20 years, 30 years, 50 years?\n  Before I tell you my thoughts on the matter, you should know that you’re reading the opinion of an enthusiastic optimist: one of the few living souls in the music industry who still believes that the music industry is not dying…it’s just coming alive.\n  There are many (many) people who predict the downfall of music sales and the irrelevancy of the album as an economic entity. I am not one of them. In my opinion, the value of an album is, and will continue to be, based on the amount of heart and soul an artist has bled into a body of work, and the financial value that artists (and their labels) place on their music when it goes out into the marketplace. Piracy, file sharing and streaming have shrunk the numbers of paid album sales drastically, and every artist has handled this blow differently.\n  In recent years, you’ve probably read the articles about major recording artists who have decided to practically give their music away, for this promotion or that exclusive deal. My hope for the future, not just in the music industry, but in every young girl I meet…is that they all realize their worth and ask for it.\n  Music is art, and art is important and rare. Important, rare things are valuable. Valuable things should be paid for. It’s my opinion that music should not be free, and my prediction is that individual artists and their labels will someday decide what an album’s price point is. I hope they don’t underestimate themselves or undervalue their art.\nI had two reactions to this editorial. The first echoed Nilay Patel’s take that Taylor Swift doesn’t understand supply and demand:\n  This might make sense if you’re Taylor Swift and your enormous army of fans will pre-order anything you tell them to, but the most important lesson of the Internet music revolution is that the vast majority of consumers actually reward convenience. That’s why the iPod was a huge hit even though digitally-compressed music sounded terrible at the time, and it’s why teenagers today get most of their music on YouTube, even though YouTube sounds worse still. It’s also why the album is dead: you can’t sell a handful of singles and some okayish filler songs to people for $10 or $15 or $25 anymore, because convenient Internet music distribution has utterly destroyed the need to bundle everything together. You can just Google the singles and listen to them for free…\n  The single hardest economic problem posed by the internet is the end of scarcity. Even just 10 years ago, most people experienced a one-to-one relationship between creative works and the physical objects they were delivered on: your music came on CDs, your movies came on DVDs, and your news came on printed magazines and newspapers. Since there’s a scarce number of these objects in the world, it’s easy to buy and sell them, because their prices will follow the laws of supply and demand: limited-edition vinyl records will be more expensive than regular CDs, because there are simply fewer of them. If you wanted a CD full of songs in 1995, you went to a store and paid for them, because there was essentially no other way to get those songs. Even if you wanted to steal the music, you had to pay some price: you needed to have a friend with the right CD, and you needed time and blank CDs to make a copy.\n  But on the internet, there’s no scarcity:  there’s an endless amount of everything available to everyone. The laws of supply and demand don’t work terribly well when there’s infinite supply. Swift is right that “important, rare things are valuable,” but she’s failed to understand that the idea of rarity simply doesn’t exist in the digital marketplace.\nAll fair points! Swift, though, persisted: later that year she pulled her music from Spotify, which meant fans had to actually buy the original 1989 when it came out in October (it was my first Swift purchase); what struck me about this move, in conjunction with the editorial, was that this was, from a certain perspective, a gift she was giving her fans. From an Update in November 2014:\n  Swift has long since proved herself a master at building a connection with fans, engaging on social media, customizing her concerts, and spilling her secrets through her songs. And, for 1989 she took things to a new level. What I think Swift has realized, though, is that reaching out to your fans is not enough: it has to be reciprocal: what selling an album for actual cash money does is give people a way to commit. They are quite literally giving Swift something valuable in exchange for her work…\n  The problem with Spotify is that at a very fundamental level it treats music as a commodity. You can’t choose where your $10/month goes based on the emotional impact of a song; the hot new hit by the artist you’ll never hear from again is treated exactly the same as the artist that was with you when you needed him or her the most. It cheapens the connection, not by withholding money per se, but by denying the commitment inherent in an explicit purchase…\n  And so, Swift has put up something of a door: access to her costs fans at least $12.99. Here’s the thing about doors though: while they keep people out, they also keep them in. Simply by virtue of having paid money directly to Swift for Swift’s music that album is already more meaningful to its 1 million buyers than the exact same music would have been were it listened to via Spotify’s all-you-can-eat subscription (or free with ads!), no matter how much money Ek and team pay out.\nA few months later, though, and 1989 was on Spotify; every album since then has been also, and 1989 (Taylor’s Version) will be as well. Something funny will happen, though, when 1989 (Taylor’s Version) is released: streams of 1989 will plummet, while 1989 (Taylor’s Version) will shoot to the top of the charts; the realities of music are such that not even Swift can hold out on streaming,2 but she has still given her fans the capability of reciprocating their relationship by making the conscious decision to only listen to (Taylor’s Version)s.Still, the value of a streaming choice doesn’t go that far; Patel’s economic argument was right, after all. The real money for Swift comes from the concerts, with the Eras Tour set to be the first to gross $1 billion; physical scarcity is still the best way for a creator to capture value."
        },
        {
          "subtitle": "Disney’s Earnings",
          "contentHtml": "<p>Last week Disney <a href=\"https://www.wsj.com/articles/disney-raising-prices-hulu-espn-earnings-call-64ae262\">reported earnings</a>, including a 23% decline in profit in its traditional TV business; that was more than made up for by an 11% increase in profit in its parks, experiences, and products segment, which accounted for 68% of Disney’s profit. Disney’s theme parks and cruises have always been an essential part of the Disney model; from a <a href=\"https://stratechery.com/2017/disney-shifts-to-streaming-disney-versus-netflix-espn/\">2017 Update</a>:</p><blockquote><p>\n  The answer reminded me of this famous chart Walt Disney created to show how the Disney business worked:</p>\n<p>  <img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2017/08/disney-2-thumb-580x507-4071.jpeg?resize=580%2C507&amp;ssl=1\" alt=\"Walt Disney's Disney Map\" width=\"580\" height=\"507\" class=\"aligncenter size-full wp-image-2700\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2017/08/disney-2-thumb-580x507-4071.jpeg?w=580&amp;ssl=1 580w, https://i0.wp.com/stratechery.com/wp-content/uploads/2017/08/disney-2-thumb-580x507-4071.jpeg?resize=300%2C262&amp;ssl=1 300w\" sizes=\"(max-width: 580px) 100vw, 580px\" data-recalc-dims=\"1\"></p>\n<p>  At the center, of course, are the Disney Studios, and rightly so. Not only does differentiated content drive movie theater revenue, it creates the universes and characters that earn TV licensing revenue, music recording revenue, and merchandise sales.</p>\n<p>  What has always made Disney unique, though, is Disneyland: there the differentiated content comes to life, and, given the lack of an arrow, I suspect not even Walt Disney himself appreciated the extent to which theme parks and the connection with the customer they engendered drive the rest of the business. “Disney” is just as much of a brand as is Mickey Mouse or Buzz Lightyear, with stores, a cable channel, and a reason to watch a movie even if you know nothing about it.\n</p></blockquote><p>It was the theme park angle that made me excited about Disney+; I wrote <a href=\"https://stratechery.com/2019/disney-and-the-future-of-tv/\">in 2019</a>:</p><blockquote><p>\n  This is the only appropriate context in which to think about Disney+. While obviously Disney+ will compete with Netflix for consumer attention, the goals of the two services are very different: for Netflix, streaming is its entire business, the sole driver of revenue and profit. Disney, meanwhile, obviously plans for Disney+ to be profitable — the company projects that the service will achieve profitability in 2024, and that includes transfer payments to Disney’s studios — but the larger project is Disney itself.</p>\n<p>  By controlling distribution of its content and going direct-to-consumer, Disney can deepen its already strong connections with customers in a way that benefits all parts of the business: movies can beget original content on Disney+ which begets new attractions at theme parks which begets merchandising opportunities which begets new movies, all building on each other like a cinematic universe in real life. Indeed, it is a testament to just how lucrative the traditional TV model is that it took so long for Disney to shift to this approach: it is a far better fit for their business in the long run than simply spreading content around to the highest bidder.\n</p></blockquote><p>I think, in retrospect, that this was an example of my falling in love with elegance and spending insufficient time in spreadsheets: Walt Disney’s chart may have been a satisfying business model, but the reality of Disney’s TV business is that it was scalable in a way that that Disney chart never could be. The beauty of the cable bundle is that nearly every household in America paid for it every single month, regardless of whether or not Disney had a hit TV show or a must-watch sporting event; thanks to its suite of channels, anchored by ESPN, Disney received a big chunk of that money, and it grew like clockwork. In that world Walt Disney’s model was a nice side business to the real money-maker — that’s a pretty good reason for Disney to have held on to that model as long as they did.</p><p>In fact, you can very much make the case that Disney and all of its peers <a href=\"https://stratechery.com/2023/hollywood-on-strike/\">ought to have held on longer</a>: yes, streaming — i.e. Netflix — leveraged the Internet for distribution of video, but that didn’t mean that Disney and Time Warner and Paramount and all of the rest had to. Those Netflix multiples, though, which far exceeded anyone else’s in Hollywood, were too tempting, and soon enough everyone was putting their best content on streaming services, leaving the cable bundle to wither.</p>",
          "contentText": "Last week Disney reported earnings, including a 23% decline in profit in its traditional TV business; that was more than made up for by an 11% increase in profit in its parks, experiences, and products segment, which accounted for 68% of Disney’s profit. Disney’s theme parks and cruises have always been an essential part of the Disney model; from a 2017 Update:\n  The answer reminded me of this famous chart Walt Disney created to show how the Disney business worked:\n  \n  At the center, of course, are the Disney Studios, and rightly so. Not only does differentiated content drive movie theater revenue, it creates the universes and characters that earn TV licensing revenue, music recording revenue, and merchandise sales.\n  What has always made Disney unique, though, is Disneyland: there the differentiated content comes to life, and, given the lack of an arrow, I suspect not even Walt Disney himself appreciated the extent to which theme parks and the connection with the customer they engendered drive the rest of the business. “Disney” is just as much of a brand as is Mickey Mouse or Buzz Lightyear, with stores, a cable channel, and a reason to watch a movie even if you know nothing about it.\nIt was the theme park angle that made me excited about Disney+; I wrote in 2019:\n  This is the only appropriate context in which to think about Disney+. While obviously Disney+ will compete with Netflix for consumer attention, the goals of the two services are very different: for Netflix, streaming is its entire business, the sole driver of revenue and profit. Disney, meanwhile, obviously plans for Disney+ to be profitable — the company projects that the service will achieve profitability in 2024, and that includes transfer payments to Disney’s studios — but the larger project is Disney itself.\n  By controlling distribution of its content and going direct-to-consumer, Disney can deepen its already strong connections with customers in a way that benefits all parts of the business: movies can beget original content on Disney+ which begets new attractions at theme parks which begets merchandising opportunities which begets new movies, all building on each other like a cinematic universe in real life. Indeed, it is a testament to just how lucrative the traditional TV model is that it took so long for Disney to shift to this approach: it is a far better fit for their business in the long run than simply spreading content around to the highest bidder.\nI think, in retrospect, that this was an example of my falling in love with elegance and spending insufficient time in spreadsheets: Walt Disney’s chart may have been a satisfying business model, but the reality of Disney’s TV business is that it was scalable in a way that that Disney chart never could be. The beauty of the cable bundle is that nearly every household in America paid for it every single month, regardless of whether or not Disney had a hit TV show or a must-watch sporting event; thanks to its suite of channels, anchored by ESPN, Disney received a big chunk of that money, and it grew like clockwork. In that world Walt Disney’s model was a nice side business to the real money-maker — that’s a pretty good reason for Disney to have held on to that model as long as they did.In fact, you can very much make the case that Disney and all of its peers ought to have held on longer: yes, streaming — i.e. Netflix — leveraged the Internet for distribution of video, but that didn’t mean that Disney and Time Warner and Paramount and all of the rest had to. Those Netflix multiples, though, which far exceeded anyone else’s in Hollywood, were too tempting, and soon enough everyone was putting their best content on streaming services, leaving the cable bundle to wither."
        },
        {
          "subtitle": "Disney’s Taylor Swift Era",
          "contentHtml": "<p>The end result is the inversion you see in Disney’s recent results. Disney is, from this point forward, not much different than Taylor Swift: sure, there is money to be made (hopefully) in areas like streaming, but the real durable value and outsized profits will come from real life experiences. This is, to be sure, a good business, but it has its limits: it is remarkable that Swift performed six shows in seven nights in Los Angeles, but it was still only six shows; concerts don’t scale like CD sales used to. Disney, similarly, only has so many theme parks, that only accommodate so many people, and operating those theme parks takes significant ongoing resources.</p><p>It’s interesting, then, to observe how differently Swift and Disney are perceived at this moment in time: I opened with Simmons analogizing Swift to Jordan, and I think it’s a fair comparison; the reality of the fractured world wrought by the Internet is that any star who can emerge from the noise becomes bigger than anything we have seen before, from hunger for a unifying experience if nothing else, and admission to that experience becomes valuable through unprecedented demand combined with physically limited supply.</p><p>That limitation, though, implies a lack of scale, which means that Swift is as big as she will ever be; that’s ok, because it’s bigger than anyone has ever has been. Disney, meanwhile, may have its own physical experiences, made valuable by their scarcity, but it will never be as valuable as owning distribution. The best thing Iger can do now is <a href=\"https://stratechery.com/2023/bob-iger-on-cnbc-the-end-of-linear-tv-espn-and-strategic-partnerships-and-apple/\">move the company on</a> from the heights it once reached; maybe someday Disney and its investors will <a href=\"https://www.youtube.com/watch?v=p1cEvNn88jM\">forget that those outsized profits ever existed</a>.</p><hr class=\"footnotes\"><ol class=\"footnotes\" style=\"list-style-type:decimal\"><li id=\"fn1-11409\"><p>Fear Of Missing Out&nbsp;<a href=\"#rf1-11409\" class=\"backlink\" title=\"Return to footnote 1.\">↩</a></p></li><li id=\"fn2-11409\"><p>It is worth noting that Swift’s fans still bought <a href=\"https://www.nytimes.com/2022/10/31/arts/music/taylor-swift-midnights-billboard-chart.html\">over 1 million physical copies of her most recent album</a>.&nbsp;<a href=\"#rf2-11409\" class=\"backlink\" title=\"Return to footnote 2.\">↩</a></p></li></ol>",
          "contentText": "The end result is the inversion you see in Disney’s recent results. Disney is, from this point forward, not much different than Taylor Swift: sure, there is money to be made (hopefully) in areas like streaming, but the real durable value and outsized profits will come from real life experiences. This is, to be sure, a good business, but it has its limits: it is remarkable that Swift performed six shows in seven nights in Los Angeles, but it was still only six shows; concerts don’t scale like CD sales used to. Disney, similarly, only has so many theme parks, that only accommodate so many people, and operating those theme parks takes significant ongoing resources.It’s interesting, then, to observe how differently Swift and Disney are perceived at this moment in time: I opened with Simmons analogizing Swift to Jordan, and I think it’s a fair comparison; the reality of the fractured world wrought by the Internet is that any star who can emerge from the noise becomes bigger than anything we have seen before, from hunger for a unifying experience if nothing else, and admission to that experience becomes valuable through unprecedented demand combined with physically limited supply.That limitation, though, implies a lack of scale, which means that Swift is as big as she will ever be; that’s ok, because it’s bigger than anyone has ever has been. Disney, meanwhile, may have its own physical experiences, made valuable by their scarcity, but it will never be as valuable as owning distribution. The best thing Iger can do now is move the company on from the heights it once reached; maybe someday Disney and its investors will forget that those outsized profits ever existed.Fear Of Missing Out ↩It is worth noting that Swift’s fans still bought over 1 million physical copies of her most recent album. ↩"
        }
      ]
    },
    {
      "title": "Hollywood on Strike",
      "publishedDate": "2023-07-17T02:00:07-07:00",
      "updatedDate": "2023-07-24T20:47:54-07:00",
      "contentHtml": "\n\t\t<p>From the <a href=\"https://timesmachine.nytimes.com/timesmachine/1955/08/02/83365852.html?pageNumber=49\">New York Times</a>:</p>\n<blockquote><p>\n  The Screen Actors Guild today announced that it will strike at 12:01 A.M. Friday against the multi million-dollar television entertainment production business. Union and employer sources held out very little hope that the work stoppage might be averted. Pessimism was heightened by the fact that representatives of both sides could see no grounds, on the basis of unofficial discussions held in recent days, for the resumption of formal contract negotiations, which were broken off July 13.</p>\n<p>  John L. Dales, national executive secretary of the guild, said the principal point at issue was the refusal of the producers “to agree to make any residual payment whatsoever to actors for the second run of a video film.” Under terms of the original contract negotiated three years ago, the performers receive additional pay on a percentage basis of salary minimums, starting with the third showing of a film and continuing through the sixth. That contract expired Wednesday.\n</p></blockquote>\n<p>Oh, I’m sorry — that’s an article from 63 years ago.</p>\n<p><img loading=\"lazy\" decoding=\"async\" src=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/07/hollywood-1.png?resize=640%2C242&amp;ssl=1\" alt=\"The 1960 edition of the New York Times about the Hollywood strike\" width=\"640\" height=\"242\" class=\"aligncenter size-full wp-image-11315\" srcset=\"https://i0.wp.com/stratechery.com/wp-content/uploads/2023/07/hollywood-1.png?w=1280&amp;ssl=1 1280w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/07/hollywood-1.png?resize=300%2C113&amp;ssl=1 300w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/07/hollywood-1.png?resize=1024%2C387&amp;ssl=1 1024w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/07/hollywood-1.png?resize=768%2C290&amp;ssl=1 768w, https://i0.wp.com/stratechery.com/wp-content/uploads/2023/07/hollywood-1.png?resize=1200%2C454&amp;ssl=1 1200w\" sizes=\"(max-width: 640px) 100vw, 640px\" data-recalc-dims=\"1\"></p>\n<p>Here’s the one in the New York Times from <a href=\"https://www.nytimes.com/2023/07/13/business/media/sag-aftra-writers-strike.html\">last week</a>:</p>\n<blockquote><p>\n  The Hollywood actors’ union approved a strike on Thursday for the first time in 43 years, bringing the $134 billion American movie and television business to a halt over anger about pay and fears of a tech-dominated future.The leaders of SAG-AFTRA, the union representing 160,000 television and movie actors, announced the strike after negotiations with studios over a new contract collapsed, with streaming services and artificial intelligence at the center of the standoff. On Friday, the actors will join screenwriters, who walked off the job in May, on picket lines in New York, Los Angeles and the dozens of other American cities where scripted shows and movies are made.</p>\n<p>  Actors and screenwriters had not been on strike at the same time since 1960, when Marilyn Monroe was still starring in films and Ronald Reagan was the head of the actors’ union. Dual strikes pit more than 170,000 workers against old-line studios like Disney, Universal, Sony and Paramount, as well tech juggernauts like Netflix, Amazon and Apple. Many of the actors’ demands mirror those of the writers, who belong to the Writers Guild of America. Both unions say they are trying to ensure living wages for workaday members, in particular those making movies or television shows for streaming services.\n</p></blockquote>\n<p>The reason to start with 1960 is that that was the last time actors and writers were on strike at the same time; the primary driver of that unrest was the rise of television. As for the last actors strike, in 1980? That was about the rise of home video. This leads to the first takeaway: the most important driver of unrest between studios and talent has always been technological paradigm shifts, and this time is no different.</p>\n<p>In this case it is the rise of streaming that strikes me as more consequential than AI, but to first dispatch with the latter, it seems to me that writers are relatively more threatened by AI; it’s much more plausible today to imagine using an LLM to generate a B-movie script or filler television than it is to imagine AI replicating actors (particularly since actors licensing their likeness may in fact turn out to be <a href=\"https://stratechery.com/2023/ai-nil-and-zero-trust-authenticity/\">very lucrative</a>).</p>\n<p>What is worth noting about AI is that those concerns are in-line with traditional Hollywood talent concerns when it comes to new technology: both unions have in strikes past been focused on preserving union jobs in the face of technological replacements. That is what led to the rise of residuals, which were at the core of the 1960 strike: if studios were showing movies on TV, then that meant they were occupying scarce time with content that actors weren’t getting paid for, which is to say that the actors in the movie that was being shown were competing with themselves; thus the union demand that they be paid for it.</p>\n<p>This by extension is why I think the AI questions in this debate will probably be easier to solve: there is already a paradigm in place in Hollywood to make sure that the talent gets a cut of every airing of a piece of entertainment, and again, while you can envision an LLM writing a script, I wouldn’t be surprised if Hollywood executives primarily see the issue as something to give on while getting concessions on the more consequential issue. That, as I noted above, is streaming, and the reason why this negotiation is probably going to be very difficult is that it is exceptionally hard to divide up a pie that is shriveling before one’s eyes.</p>\n<h3>Scarcity and Residuals</h3>\n<p>There was a very interesting answer in <a href=\"https://slate.com/culture/2023/07/sag-aftra-actors-writers-wga-strike-hollywood-1960-ronald-reagan.html\">this Slate interview</a> with Wayne Federman, who wrote <a href=\"https://www.theatlantic.com/entertainment/archive/2011/11/what-reagan-did-for-hollywood/248391/\">a piece in The Atlantic a decade ago</a> about the 1960 strike:</p>\n<blockquote><p>\n  For this strike, before we even negotiated, we already had strike authorization from the membership. In 1960, they didn’t. The 1960 strike was really about one issue, and this strike is about multiple issues. This is about how residuals, specifically for streaming entertainment, are being calculated. Those numbers are … not really released. It’s not like a Nielsen rating. Sometimes you’ll hear something like, Oh, 1.2 million minutes of Squid Game — what does that mean? Does that mean that many people watched one minute of it, or does that mean people watched it a number of times, or … ? I don’t know why it’s all proprietary for these streamers, but that’s just where we’re at. We want a little more transparency in that, [to consider] that if we’re on a hit show, is that paid differently than a [nonhit] show? And then there’s this A.I. situation.</p>\n<p>  <strong>You said that the studios were sort of giving up these residuals through clenched teeth. Do you think that their position on that has changed?</strong></p>\n<p>  That’s the amazing outcome of what Ronald Reagan — and other negotiators at the time — was able to do: In a way, they were changing the paradigm of how Hollywood money is divided up. They were striking for an idea: that we deserve this for A, B, C, and D reasons. You get residuals now. Not everyone; editors don’t get residuals, but directors do. I get residuals for streaming services, but they’re just not the same. They’re not as good as cable, and they’re not as good as network. When you look at the check, you’re like, OK, this doesn’t seem like a lot. But, again, you don’t know how many people are watching it.</p>\n<p>  And also, I think when we first started looking at streaming services, we were like, We want these services to thrive so that there’ll be more work for actors. So I think that’s why we were not militant about residuals for these new platforms. No one is saying, Oh, we paid you to be on this Netflix show, and we never have to pay you a residual. The problem is that it’s not as hearty as it used to be for these other mediums. But the idea of residuals … is not going away, unless [the companies] decide to try to break the unions and just use nonunion actors and not pay residuals.\n</p></blockquote>\n<p>One of the ways Netflix broke into Hollywood was by eschewing back-end deals for top talent and paying more upfront; this removed the potential for huge upside if a show was a massive hit, but it guaranteed that talent got paid, even if a show wasn’t a success. Over time Netflix and other streamers have started to pay back-end bonuses in addition to residuals, but as Federman notes, the lack of transparency into how exactly those residuals are calculated is a big sticking point.</p>\n<p>The most interesting paragraph to me, though, is the last one: at the risk of taking Federman’s word for it, the sentiment that unions saw streaming services as a net positive rings true, and aligns with the previous item. The entire idea of residuals arose from the idea that talent shouldn’t have to compete with itself when it came to re-running a movie or show; the key thing to note, though, is that this concern made sense in a world where there was scarce distribution. To go back to the 1960s, there were only three networks: that meant there were only 504 hours in a week to air content on television; airing a two-hour movie reduced the available space for talent to 502 hours.</p>\n<p>Streaming, though, is purely additive. The Internet makes distribution effectively free, which means there are an infinite number of hours available for talent to monetize. This does, it’s worth noting, render talent’s original argument for residuals moot; if anything Netflix had it right when it temporarily shifted the model to simply paying up front. In fact, Federman unwittingly makes this point when he describes the mindset of studio heads in 1960:</p>\n<blockquote><p>\n  Let’s say you get hired to act in a film. Basically, the person hiring you is taking the risk. They’re paying you your salary, and in return, they own that product. So, what SAG was saying was, You can play that film anywhere in the world, you can play it in Italy, you can have it dubbed — but when you put it on television, that’s a new revenue stream. Also, the argument was that that is taking work away from other actors. Because if you have this movie on, that time slot is no longer available for working actors.</p>\n<p>  On the other side, the head of 20th Century Fox [Spyros Skouras], his argument was very simple: Why should I pay you twice for the same job? I’ve already paid you for this job. I own this at this point. And that was basically the position of all of these studio owners. At the beginning of the strike, they were like, We’re not even going to talk about residuals. It’s a nonstarter. And Reagan said, We’re “trying to negotiate for the right to negotiate.” That’s how far apart they were. It was so foreign to these guys that they would have to share their revenues with actors after they’d already paid the actors. Ultimately, one studio, Universal Pictures—believe it or not, the head of Universal, a guy named Lew Wasserman, used to be Ronald Reagan’s agent—was the first domino that dropped. I think Lew Wasserman thought it was inevitable anyway: If it wasn’t going to happen in 1960, it might happen in ’65. And then one after another [gave in], until, I think, the 20th Century guy was the last guy, who was like, All right, I’ll give it, I’ll pay you again for something I’ve already paid you for, through clenched teeth.\n</p></blockquote>\n<p>Wasserman was right: studios were going to have to share the scarce resource, which was time on TV, with talent. Again, though, scarcity in terms of distribution is now gone; the only scarce resource on the Internet is consumer time and attention, and commanding that is far more difficult and risky. Look no further than the deteriorating financial condition of most of Hollywood: not only are the studios competing with Netflix and Amazon and Apple, but also with things like YouTube and social media. Indeed, you could very easily make the case that a far more legible labor action would be for the studios to lock out the talent in an attempt to remove residuals completely, given how much more risk any content producer is taking on today.</p>\n<p>This angle is, obviously, a non-starter, but it does point at why these negotiations are likely to be so fraught: actors and writers are angling to get a larger share of revenue that they arguably no longer deserve.</p>\n<h3>The Cost of Streaming</h3>\n<p>There is an even larger problem, though, which is that studios have — in my estimation — yet to come to grips with the true cost of streaming. To go back to the old model, studios were in the business of making movies or TV shows and then selling them to distributors. Ideally they would sell the same piece of content multiple times, better leveraging the cost of making the content in the first place. Indeed, this was a sticking point in the 1960 strike; from <a href=\"https://timesmachine.nytimes.com/timesmachine/1955/08/02/83365852.html?pageNumber=49\">that 1960 New York Times article</a>:</p>\n<blockquote><p>\n  The guild asked for 100 percent payment of minimum salaries for the second showing in a new contract. The producers insisted that the first two runs be covered by the original salary. The producers contend that it is virtually impossible to get sufficient money out of the first showing of a movie produced solely for television to pay off the initial production investment. It is reported that many bank loans are predicated on earnings from the second run.\n</p></blockquote>\n<p>Content costs a lot to produce up front, but the marginal cost of showing it again is effectively zero; that means the more times you can show a piece of content the more you can spend up front. The number of times you could show it, though, was, as noted above, governed by available distribution; if distribution was scarce then there was an opportunity cost of showing old content, because you couldn’t show something new (which again, was why talent wanted a share of multiple airings).</p>\n<p>What is critical to note is that this leverage was best realized by selling to as many distributors as possible. The classic example is the traditional movie window: first you sell a movie to first-run theaters, then to budget theaters, then to hotels and airlines, then to pay-per-view, then to videocassettes/DVDs, then to cable, and finally to broadcast TV. That’s seven distinct opportunities to sell a piece of content. Going straight to streaming, though, collapses seven windows to one, reducing the ability to make money off of a particular piece of content.</p>\n<p>Studios are enduring this cost, though, in the service of building up their own streaming services, but that has its own costs: running a streaming service entails being in the direct-to-consumer business, which is a costly one: not only do you have to build up and maintain the technical infrastructure of the service, and incur costs in customer support, but you also have to worry about things like churn that simply aren’t a consideration when you’re selling content. All of this is very expensive!</p>\n<p>The real pain, though, is opportunity cost: while studios are missing out on multi-window revenue, paying for their streaming service, and trying to simultaneously acquire customers and stopping them from churning, they are also forgoing revenue from established services like Netflix that would not only happily pay them for their content, but could actually justify a much higher price given their significantly larger user base across which that cost could be leveraged.</p>\n<p>All of these costs, it should be noted, occur in the aggregate, which is a real problem in these negotiations: talent is concerned about their compensation on a per-show basis, but studios are bleeding money on an entity-level in their foolhardy pursuit of customer-facing streaming services. Most of the discussion about this mismatch is focused on how to properly compensate the talent; note this item from <a href=\"https://puck.news/the-nanny-steps-all-over-bob-iger/?_cio_id=f6c6060fc9af019aeb1c\">Puck</a>:</p>\n<blockquote><p>\n  The union and AMPTP have by and large agreed on the residuals improvements the DGA obtained in its recent deal, but the union also wants 2 percent of subscriber revenue to be shared with the cast of a successful show, with success measured by Parrot Analytics, an analysis firm that looks at viewership, social media engagement, and other factors, to determine “demand.” That proxy metric was proposed because the companies refuse to share their internal measurements, of course. But the studios declined to engage on that issue, and the management-side source asked how the producer of a show could be expected to share revenue earned not by the producer but by the platform (i.e., subscribers pay platforms; subscribers don’t pay producers).\n</p></blockquote>\n<p>I get the talent’s perspective, but I’m pretty sure the talent doesn’t want to pay for the cost of customer service or customer acquisition or churn mitigation! Then again, neither should the studios: it doesn’t make any sense to me why the studios decided they wanted to bear these costs, and that’s not the talent’s problem.</p>\n<h3>The Shrinking Pie</h3>\n<p>There remains, though, the shrinking pie I noted in the introduction: the removal of distribution costs that enabled the rise of streaming was not a benefit that was limited to Hollywood, nor was the shift to attention as the only scarce resource. Every person on earth has only 168 hours in a week, during which time they are presumably sleeping and working. Those few remaining hours can now be filled by YouTube, or gaming, or podcasts, or reading this Article; every single minute spent doing something other than consuming Hollywood content is a minute lost forever.</p>\n<p>This is consideration enough without a labor battle: thanks to COVID a lot of people fell out of the habit of going to the movie theater, and it appears around 25% of the audience permanently found something better to do with their time; that same reality applies to TV. Just as newspapers once thought the Internet was a boon because it increased their addressable market, only to find out that it also drastically increased competition for readers’ attention, Hollywood has to face the reality that the ability to make far more shows extends not only to studios but also to literally anyone. That reality is going to come to the fore if this strike drags on: if people don’t have new movies or shows to watch they will find far more options to fill their time than existed in 1960; the risk to Hollywood is that some of those alternatives become a permanent feature of people’s media diets, in line with what seems to have happened during COVID.</p>\n<p>The broader issue is that the video industry finally seems to be facing what happened to the print and music industry before them: the Internet comes bearing gifts like infinite capacity and free distribution, but those gifts are a poisoned chalice for industries predicated on scarcity. When anyone could publish text, most text-based businesses went from massive profitability to terminal decline; when anyone could distribute music the music industry could only be saved by tech companies like Spotify helping them sell convenience in place of plastic discs.</p>\n<p>For the video industry the first step to survival must be to retreat to what they are good at — producing content that isn’t available anywhere else — and getting away from what they are not, i.e. running undifferentiated streaming services with massive direct costs and even larger opportunity ones. Talent, meanwhile, has to realize that they and the studios are not divided by this new paradigm, but jointly threatened: the Internet is bad news for content producers with outsized costs, and long-term sustainability will be that much harder to achieve if the focus is on increasing them.</p>\n<p><em>I wrote a follow-up to this Article in <a href=\"https://stratechery.com/2023/streaming-residuals-and-spotify-llama-2-open-sourced-llamas-license/\">this Daily Update</a>.</em></p>\n\n\t\t\t",
      "contentText": "\n\t\tFrom the New York Times:\n\n  The Screen Actors Guild today announced that it will strike at 12:01 A.M. Friday against the multi million-dollar television entertainment production business. Union and employer sources held out very little hope that the work stoppage might be averted. Pessimism was heightened by the fact that representatives of both sides could see no grounds, on the basis of unofficial discussions held in recent days, for the resumption of formal contract negotiations, which were broken off July 13.\n  John L. Dales, national executive secretary of the guild, said the principal point at issue was the refusal of the producers “to agree to make any residual payment whatsoever to actors for the second run of a video film.” Under terms of the original contract negotiated three years ago, the performers receive additional pay on a percentage basis of salary minimums, starting with the third showing of a film and continuing through the sixth. That contract expired Wednesday.\n\nOh, I’m sorry — that’s an article from 63 years ago.\n\nHere’s the one in the New York Times from last week:\n\n  The Hollywood actors’ union approved a strike on Thursday for the first time in 43 years, bringing the $134 billion American movie and television business to a halt over anger about pay and fears of a tech-dominated future.The leaders of SAG-AFTRA, the union representing 160,000 television and movie actors, announced the strike after negotiations with studios over a new contract collapsed, with streaming services and artificial intelligence at the center of the standoff. On Friday, the actors will join screenwriters, who walked off the job in May, on picket lines in New York, Los Angeles and the dozens of other American cities where scripted shows and movies are made.\n  Actors and screenwriters had not been on strike at the same time since 1960, when Marilyn Monroe was still starring in films and Ronald Reagan was the head of the actors’ union. Dual strikes pit more than 170,000 workers against old-line studios like Disney, Universal, Sony and Paramount, as well tech juggernauts like Netflix, Amazon and Apple. Many of the actors’ demands mirror those of the writers, who belong to the Writers Guild of America. Both unions say they are trying to ensure living wages for workaday members, in particular those making movies or television shows for streaming services.\n\nThe reason to start with 1960 is that that was the last time actors and writers were on strike at the same time; the primary driver of that unrest was the rise of television. As for the last actors strike, in 1980? That was about the rise of home video. This leads to the first takeaway: the most important driver of unrest between studios and talent has always been technological paradigm shifts, and this time is no different.\nIn this case it is the rise of streaming that strikes me as more consequential than AI, but to first dispatch with the latter, it seems to me that writers are relatively more threatened by AI; it’s much more plausible today to imagine using an LLM to generate a B-movie script or filler television than it is to imagine AI replicating actors (particularly since actors licensing their likeness may in fact turn out to be very lucrative).\nWhat is worth noting about AI is that those concerns are in-line with traditional Hollywood talent concerns when it comes to new technology: both unions have in strikes past been focused on preserving union jobs in the face of technological replacements. That is what led to the rise of residuals, which were at the core of the 1960 strike: if studios were showing movies on TV, then that meant they were occupying scarce time with content that actors weren’t getting paid for, which is to say that the actors in the movie that was being shown were competing with themselves; thus the union demand that they be paid for it.\nThis by extension is why I think the AI questions in this debate will probably be easier to solve: there is already a paradigm in place in Hollywood to make sure that the talent gets a cut of every airing of a piece of entertainment, and again, while you can envision an LLM writing a script, I wouldn’t be surprised if Hollywood executives primarily see the issue as something to give on while getting concessions on the more consequential issue. That, as I noted above, is streaming, and the reason why this negotiation is probably going to be very difficult is that it is exceptionally hard to divide up a pie that is shriveling before one’s eyes.\nScarcity and Residuals\nThere was a very interesting answer in this Slate interview with Wayne Federman, who wrote a piece in The Atlantic a decade ago about the 1960 strike:\n\n  For this strike, before we even negotiated, we already had strike authorization from the membership. In 1960, they didn’t. The 1960 strike was really about one issue, and this strike is about multiple issues. This is about how residuals, specifically for streaming entertainment, are being calculated. Those numbers are … not really released. It’s not like a Nielsen rating. Sometimes you’ll hear something like, Oh, 1.2 million minutes of Squid Game — what does that mean? Does that mean that many people watched one minute of it, or does that mean people watched it a number of times, or … ? I don’t know why it’s all proprietary for these streamers, but that’s just where we’re at. We want a little more transparency in that, [to consider] that if we’re on a hit show, is that paid differently than a [nonhit] show? And then there’s this A.I. situation.\n  You said that the studios were sort of giving up these residuals through clenched teeth. Do you think that their position on that has changed?\n  That’s the amazing outcome of what Ronald Reagan — and other negotiators at the time — was able to do: In a way, they were changing the paradigm of how Hollywood money is divided up. They were striking for an idea: that we deserve this for A, B, C, and D reasons. You get residuals now. Not everyone; editors don’t get residuals, but directors do. I get residuals for streaming services, but they’re just not the same. They’re not as good as cable, and they’re not as good as network. When you look at the check, you’re like, OK, this doesn’t seem like a lot. But, again, you don’t know how many people are watching it.\n  And also, I think when we first started looking at streaming services, we were like, We want these services to thrive so that there’ll be more work for actors. So I think that’s why we were not militant about residuals for these new platforms. No one is saying, Oh, we paid you to be on this Netflix show, and we never have to pay you a residual. The problem is that it’s not as hearty as it used to be for these other mediums. But the idea of residuals … is not going away, unless [the companies] decide to try to break the unions and just use nonunion actors and not pay residuals.\n\nOne of the ways Netflix broke into Hollywood was by eschewing back-end deals for top talent and paying more upfront; this removed the potential for huge upside if a show was a massive hit, but it guaranteed that talent got paid, even if a show wasn’t a success. Over time Netflix and other streamers have started to pay back-end bonuses in addition to residuals, but as Federman notes, the lack of transparency into how exactly those residuals are calculated is a big sticking point.\nThe most interesting paragraph to me, though, is the last one: at the risk of taking Federman’s word for it, the sentiment that unions saw streaming services as a net positive rings true, and aligns with the previous item. The entire idea of residuals arose from the idea that talent shouldn’t have to compete with itself when it came to re-running a movie or show; the key thing to note, though, is that this concern made sense in a world where there was scarce distribution. To go back to the 1960s, there were only three networks: that meant there were only 504 hours in a week to air content on television; airing a two-hour movie reduced the available space for talent to 502 hours.\nStreaming, though, is purely additive. The Internet makes distribution effectively free, which means there are an infinite number of hours available for talent to monetize. This does, it’s worth noting, render talent’s original argument for residuals moot; if anything Netflix had it right when it temporarily shifted the model to simply paying up front. In fact, Federman unwittingly makes this point when he describes the mindset of studio heads in 1960:\n\n  Let’s say you get hired to act in a film. Basically, the person hiring you is taking the risk. They’re paying you your salary, and in return, they own that product. So, what SAG was saying was, You can play that film anywhere in the world, you can play it in Italy, you can have it dubbed — but when you put it on television, that’s a new revenue stream. Also, the argument was that that is taking work away from other actors. Because if you have this movie on, that time slot is no longer available for working actors.\n  On the other side, the head of 20th Century Fox [Spyros Skouras], his argument was very simple: Why should I pay you twice for the same job? I’ve already paid you for this job. I own this at this point. And that was basically the position of all of these studio owners. At the beginning of the strike, they were like, We’re not even going to talk about residuals. It’s a nonstarter. And Reagan said, We’re “trying to negotiate for the right to negotiate.” That’s how far apart they were. It was so foreign to these guys that they would have to share their revenues with actors after they’d already paid the actors. Ultimately, one studio, Universal Pictures—believe it or not, the head of Universal, a guy named Lew Wasserman, used to be Ronald Reagan’s agent—was the first domino that dropped. I think Lew Wasserman thought it was inevitable anyway: If it wasn’t going to happen in 1960, it might happen in ’65. And then one after another [gave in], until, I think, the 20th Century guy was the last guy, who was like, All right, I’ll give it, I’ll pay you again for something I’ve already paid you for, through clenched teeth.\n\nWasserman was right: studios were going to have to share the scarce resource, which was time on TV, with talent. Again, though, scarcity in terms of distribution is now gone; the only scarce resource on the Internet is consumer time and attention, and commanding that is far more difficult and risky. Look no further than the deteriorating financial condition of most of Hollywood: not only are the studios competing with Netflix and Amazon and Apple, but also with things like YouTube and social media. Indeed, you could very easily make the case that a far more legible labor action would be for the studios to lock out the talent in an attempt to remove residuals completely, given how much more risk any content producer is taking on today.\nThis angle is, obviously, a non-starter, but it does point at why these negotiations are likely to be so fraught: actors and writers are angling to get a larger share of revenue that they arguably no longer deserve.\nThe Cost of Streaming\nThere is an even larger problem, though, which is that studios have — in my estimation — yet to come to grips with the true cost of streaming. To go back to the old model, studios were in the business of making movies or TV shows and then selling them to distributors. Ideally they would sell the same piece of content multiple times, better leveraging the cost of making the content in the first place. Indeed, this was a sticking point in the 1960 strike; from that 1960 New York Times article:\n\n  The guild asked for 100 percent payment of minimum salaries for the second showing in a new contract. The producers insisted that the first two runs be covered by the original salary. The producers contend that it is virtually impossible to get sufficient money out of the first showing of a movie produced solely for television to pay off the initial production investment. It is reported that many bank loans are predicated on earnings from the second run.\n\nContent costs a lot to produce up front, but the marginal cost of showing it again is effectively zero; that means the more times you can show a piece of content the more you can spend up front. The number of times you could show it, though, was, as noted above, governed by available distribution; if distribution was scarce then there was an opportunity cost of showing old content, because you couldn’t show something new (which again, was why talent wanted a share of multiple airings).\nWhat is critical to note is that this leverage was best realized by selling to as many distributors as possible. The classic example is the traditional movie window: first you sell a movie to first-run theaters, then to budget theaters, then to hotels and airlines, then to pay-per-view, then to videocassettes/DVDs, then to cable, and finally to broadcast TV. That’s seven distinct opportunities to sell a piece of content. Going straight to streaming, though, collapses seven windows to one, reducing the ability to make money off of a particular piece of content.\nStudios are enduring this cost, though, in the service of building up their own streaming services, but that has its own costs: running a streaming service entails being in the direct-to-consumer business, which is a costly one: not only do you have to build up and maintain the technical infrastructure of the service, and incur costs in customer support, but you also have to worry about things like churn that simply aren’t a consideration when you’re selling content. All of this is very expensive!\nThe real pain, though, is opportunity cost: while studios are missing out on multi-window revenue, paying for their streaming service, and trying to simultaneously acquire customers and stopping them from churning, they are also forgoing revenue from established services like Netflix that would not only happily pay them for their content, but could actually justify a much higher price given their significantly larger user base across which that cost could be leveraged.\nAll of these costs, it should be noted, occur in the aggregate, which is a real problem in these negotiations: talent is concerned about their compensation on a per-show basis, but studios are bleeding money on an entity-level in their foolhardy pursuit of customer-facing streaming services. Most of the discussion about this mismatch is focused on how to properly compensate the talent; note this item from Puck:\n\n  The union and AMPTP have by and large agreed on the residuals improvements the DGA obtained in its recent deal, but the union also wants 2 percent of subscriber revenue to be shared with the cast of a successful show, with success measured by Parrot Analytics, an analysis firm that looks at viewership, social media engagement, and other factors, to determine “demand.” That proxy metric was proposed because the companies refuse to share their internal measurements, of course. But the studios declined to engage on that issue, and the management-side source asked how the producer of a show could be expected to share revenue earned not by the producer but by the platform (i.e., subscribers pay platforms; subscribers don’t pay producers).\n\nI get the talent’s perspective, but I’m pretty sure the talent doesn’t want to pay for the cost of customer service or customer acquisition or churn mitigation! Then again, neither should the studios: it doesn’t make any sense to me why the studios decided they wanted to bear these costs, and that’s not the talent’s problem.\nThe Shrinking Pie\nThere remains, though, the shrinking pie I noted in the introduction: the removal of distribution costs that enabled the rise of streaming was not a benefit that was limited to Hollywood, nor was the shift to attention as the only scarce resource. Every person on earth has only 168 hours in a week, during which time they are presumably sleeping and working. Those few remaining hours can now be filled by YouTube, or gaming, or podcasts, or reading this Article; every single minute spent doing something other than consuming Hollywood content is a minute lost forever.\nThis is consideration enough without a labor battle: thanks to COVID a lot of people fell out of the habit of going to the movie theater, and it appears around 25% of the audience permanently found something better to do with their time; that same reality applies to TV. Just as newspapers once thought the Internet was a boon because it increased their addressable market, only to find out that it also drastically increased competition for readers’ attention, Hollywood has to face the reality that the ability to make far more shows extends not only to studios but also to literally anyone. That reality is going to come to the fore if this strike drags on: if people don’t have new movies or shows to watch they will find far more options to fill their time than existed in 1960; the risk to Hollywood is that some of those alternatives become a permanent feature of people’s media diets, in line with what seems to have happened during COVID.\nThe broader issue is that the video industry finally seems to be facing what happened to the print and music industry before them: the Internet comes bearing gifts like infinite capacity and free distribution, but those gifts are a poisoned chalice for industries predicated on scarcity. When anyone could publish text, most text-based businesses went from massive profitability to terminal decline; when anyone could distribute music the music industry could only be saved by tech companies like Spotify helping them sell convenience in place of plastic discs.\nFor the video industry the first step to survival must be to retreat to what they are good at — producing content that isn’t available anywhere else — and getting away from what they are not, i.e. running undifferentiated streaming services with massive direct costs and even larger opportunity ones. Talent, meanwhile, has to realize that they and the studios are not divided by this new paradigm, but jointly threatened: the Internet is bad news for content producers with outsized costs, and long-term sustainability will be that much harder to achieve if the focus is on increasing them.\nI wrote a follow-up to this Article in this Daily Update.\n\n\t\t\t",
      "subsections": [
        {
          "subtitle": "Scarcity and Residuals",
          "contentHtml": "<p>There was a very interesting answer in <a href=\"https://slate.com/culture/2023/07/sag-aftra-actors-writers-wga-strike-hollywood-1960-ronald-reagan.html\">this Slate interview</a> with Wayne Federman, who wrote <a href=\"https://www.theatlantic.com/entertainment/archive/2011/11/what-reagan-did-for-hollywood/248391/\">a piece in The Atlantic a decade ago</a> about the 1960 strike:</p><blockquote><p>\n  For this strike, before we even negotiated, we already had strike authorization from the membership. In 1960, they didn’t. The 1960 strike was really about one issue, and this strike is about multiple issues. This is about how residuals, specifically for streaming entertainment, are being calculated. Those numbers are … not really released. It’s not like a Nielsen rating. Sometimes you’ll hear something like, Oh, 1.2 million minutes of Squid Game — what does that mean? Does that mean that many people watched one minute of it, or does that mean people watched it a number of times, or … ? I don’t know why it’s all proprietary for these streamers, but that’s just where we’re at. We want a little more transparency in that, [to consider] that if we’re on a hit show, is that paid differently than a [nonhit] show? And then there’s this A.I. situation.</p>\n<p>  <strong>You said that the studios were sort of giving up these residuals through clenched teeth. Do you think that their position on that has changed?</strong></p>\n<p>  That’s the amazing outcome of what Ronald Reagan — and other negotiators at the time — was able to do: In a way, they were changing the paradigm of how Hollywood money is divided up. They were striking for an idea: that we deserve this for A, B, C, and D reasons. You get residuals now. Not everyone; editors don’t get residuals, but directors do. I get residuals for streaming services, but they’re just not the same. They’re not as good as cable, and they’re not as good as network. When you look at the check, you’re like, OK, this doesn’t seem like a lot. But, again, you don’t know how many people are watching it.</p>\n<p>  And also, I think when we first started looking at streaming services, we were like, We want these services to thrive so that there’ll be more work for actors. So I think that’s why we were not militant about residuals for these new platforms. No one is saying, Oh, we paid you to be on this Netflix show, and we never have to pay you a residual. The problem is that it’s not as hearty as it used to be for these other mediums. But the idea of residuals … is not going away, unless [the companies] decide to try to break the unions and just use nonunion actors and not pay residuals.\n</p></blockquote><p>One of the ways Netflix broke into Hollywood was by eschewing back-end deals for top talent and paying more upfront; this removed the potential for huge upside if a show was a massive hit, but it guaranteed that talent got paid, even if a show wasn’t a success. Over time Netflix and other streamers have started to pay back-end bonuses in addition to residuals, but as Federman notes, the lack of transparency into how exactly those residuals are calculated is a big sticking point.</p><p>The most interesting paragraph to me, though, is the last one: at the risk of taking Federman’s word for it, the sentiment that unions saw streaming services as a net positive rings true, and aligns with the previous item. The entire idea of residuals arose from the idea that talent shouldn’t have to compete with itself when it came to re-running a movie or show; the key thing to note, though, is that this concern made sense in a world where there was scarce distribution. To go back to the 1960s, there were only three networks: that meant there were only 504 hours in a week to air content on television; airing a two-hour movie reduced the available space for talent to 502 hours.</p><p>Streaming, though, is purely additive. The Internet makes distribution effectively free, which means there are an infinite number of hours available for talent to monetize. This does, it’s worth noting, render talent’s original argument for residuals moot; if anything Netflix had it right when it temporarily shifted the model to simply paying up front. In fact, Federman unwittingly makes this point when he describes the mindset of studio heads in 1960:</p><blockquote><p>\n  Let’s say you get hired to act in a film. Basically, the person hiring you is taking the risk. They’re paying you your salary, and in return, they own that product. So, what SAG was saying was, You can play that film anywhere in the world, you can play it in Italy, you can have it dubbed — but when you put it on television, that’s a new revenue stream. Also, the argument was that that is taking work away from other actors. Because if you have this movie on, that time slot is no longer available for working actors.</p>\n<p>  On the other side, the head of 20th Century Fox [Spyros Skouras], his argument was very simple: Why should I pay you twice for the same job? I’ve already paid you for this job. I own this at this point. And that was basically the position of all of these studio owners. At the beginning of the strike, they were like, We’re not even going to talk about residuals. It’s a nonstarter. And Reagan said, We’re “trying to negotiate for the right to negotiate.” That’s how far apart they were. It was so foreign to these guys that they would have to share their revenues with actors after they’d already paid the actors. Ultimately, one studio, Universal Pictures—believe it or not, the head of Universal, a guy named Lew Wasserman, used to be Ronald Reagan’s agent—was the first domino that dropped. I think Lew Wasserman thought it was inevitable anyway: If it wasn’t going to happen in 1960, it might happen in ’65. And then one after another [gave in], until, I think, the 20th Century guy was the last guy, who was like, All right, I’ll give it, I’ll pay you again for something I’ve already paid you for, through clenched teeth.\n</p></blockquote><p>Wasserman was right: studios were going to have to share the scarce resource, which was time on TV, with talent. Again, though, scarcity in terms of distribution is now gone; the only scarce resource on the Internet is consumer time and attention, and commanding that is far more difficult and risky. Look no further than the deteriorating financial condition of most of Hollywood: not only are the studios competing with Netflix and Amazon and Apple, but also with things like YouTube and social media. Indeed, you could very easily make the case that a far more legible labor action would be for the studios to lock out the talent in an attempt to remove residuals completely, given how much more risk any content producer is taking on today.</p><p>This angle is, obviously, a non-starter, but it does point at why these negotiations are likely to be so fraught: actors and writers are angling to get a larger share of revenue that they arguably no longer deserve.</p>",
          "contentText": "There was a very interesting answer in this Slate interview with Wayne Federman, who wrote a piece in The Atlantic a decade ago about the 1960 strike:\n  For this strike, before we even negotiated, we already had strike authorization from the membership. In 1960, they didn’t. The 1960 strike was really about one issue, and this strike is about multiple issues. This is about how residuals, specifically for streaming entertainment, are being calculated. Those numbers are … not really released. It’s not like a Nielsen rating. Sometimes you’ll hear something like, Oh, 1.2 million minutes of Squid Game — what does that mean? Does that mean that many people watched one minute of it, or does that mean people watched it a number of times, or … ? I don’t know why it’s all proprietary for these streamers, but that’s just where we’re at. We want a little more transparency in that, [to consider] that if we’re on a hit show, is that paid differently than a [nonhit] show? And then there’s this A.I. situation.\n  You said that the studios were sort of giving up these residuals through clenched teeth. Do you think that their position on that has changed?\n  That’s the amazing outcome of what Ronald Reagan — and other negotiators at the time — was able to do: In a way, they were changing the paradigm of how Hollywood money is divided up. They were striking for an idea: that we deserve this for A, B, C, and D reasons. You get residuals now. Not everyone; editors don’t get residuals, but directors do. I get residuals for streaming services, but they’re just not the same. They’re not as good as cable, and they’re not as good as network. When you look at the check, you’re like, OK, this doesn’t seem like a lot. But, again, you don’t know how many people are watching it.\n  And also, I think when we first started looking at streaming services, we were like, We want these services to thrive so that there’ll be more work for actors. So I think that’s why we were not militant about residuals for these new platforms. No one is saying, Oh, we paid you to be on this Netflix show, and we never have to pay you a residual. The problem is that it’s not as hearty as it used to be for these other mediums. But the idea of residuals … is not going away, unless [the companies] decide to try to break the unions and just use nonunion actors and not pay residuals.\nOne of the ways Netflix broke into Hollywood was by eschewing back-end deals for top talent and paying more upfront; this removed the potential for huge upside if a show was a massive hit, but it guaranteed that talent got paid, even if a show wasn’t a success. Over time Netflix and other streamers have started to pay back-end bonuses in addition to residuals, but as Federman notes, the lack of transparency into how exactly those residuals are calculated is a big sticking point.The most interesting paragraph to me, though, is the last one: at the risk of taking Federman’s word for it, the sentiment that unions saw streaming services as a net positive rings true, and aligns with the previous item. The entire idea of residuals arose from the idea that talent shouldn’t have to compete with itself when it came to re-running a movie or show; the key thing to note, though, is that this concern made sense in a world where there was scarce distribution. To go back to the 1960s, there were only three networks: that meant there were only 504 hours in a week to air content on television; airing a two-hour movie reduced the available space for talent to 502 hours.Streaming, though, is purely additive. The Internet makes distribution effectively free, which means there are an infinite number of hours available for talent to monetize. This does, it’s worth noting, render talent’s original argument for residuals moot; if anything Netflix had it right when it temporarily shifted the model to simply paying up front. In fact, Federman unwittingly makes this point when he describes the mindset of studio heads in 1960:\n  Let’s say you get hired to act in a film. Basically, the person hiring you is taking the risk. They’re paying you your salary, and in return, they own that product. So, what SAG was saying was, You can play that film anywhere in the world, you can play it in Italy, you can have it dubbed — but when you put it on television, that’s a new revenue stream. Also, the argument was that that is taking work away from other actors. Because if you have this movie on, that time slot is no longer available for working actors.\n  On the other side, the head of 20th Century Fox [Spyros Skouras], his argument was very simple: Why should I pay you twice for the same job? I’ve already paid you for this job. I own this at this point. And that was basically the position of all of these studio owners. At the beginning of the strike, they were like, We’re not even going to talk about residuals. It’s a nonstarter. And Reagan said, We’re “trying to negotiate for the right to negotiate.” That’s how far apart they were. It was so foreign to these guys that they would have to share their revenues with actors after they’d already paid the actors. Ultimately, one studio, Universal Pictures—believe it or not, the head of Universal, a guy named Lew Wasserman, used to be Ronald Reagan’s agent—was the first domino that dropped. I think Lew Wasserman thought it was inevitable anyway: If it wasn’t going to happen in 1960, it might happen in ’65. And then one after another [gave in], until, I think, the 20th Century guy was the last guy, who was like, All right, I’ll give it, I’ll pay you again for something I’ve already paid you for, through clenched teeth.\nWasserman was right: studios were going to have to share the scarce resource, which was time on TV, with talent. Again, though, scarcity in terms of distribution is now gone; the only scarce resource on the Internet is consumer time and attention, and commanding that is far more difficult and risky. Look no further than the deteriorating financial condition of most of Hollywood: not only are the studios competing with Netflix and Amazon and Apple, but also with things like YouTube and social media. Indeed, you could very easily make the case that a far more legible labor action would be for the studios to lock out the talent in an attempt to remove residuals completely, given how much more risk any content producer is taking on today.This angle is, obviously, a non-starter, but it does point at why these negotiations are likely to be so fraught: actors and writers are angling to get a larger share of revenue that they arguably no longer deserve."
        },
        {
          "subtitle": "The Cost of Streaming",
          "contentHtml": "<p>There is an even larger problem, though, which is that studios have — in my estimation — yet to come to grips with the true cost of streaming. To go back to the old model, studios were in the business of making movies or TV shows and then selling them to distributors. Ideally they would sell the same piece of content multiple times, better leveraging the cost of making the content in the first place. Indeed, this was a sticking point in the 1960 strike; from <a href=\"https://timesmachine.nytimes.com/timesmachine/1955/08/02/83365852.html?pageNumber=49\">that 1960 New York Times article</a>:</p><blockquote><p>\n  The guild asked for 100 percent payment of minimum salaries for the second showing in a new contract. The producers insisted that the first two runs be covered by the original salary. The producers contend that it is virtually impossible to get sufficient money out of the first showing of a movie produced solely for television to pay off the initial production investment. It is reported that many bank loans are predicated on earnings from the second run.\n</p></blockquote><p>Content costs a lot to produce up front, but the marginal cost of showing it again is effectively zero; that means the more times you can show a piece of content the more you can spend up front. The number of times you could show it, though, was, as noted above, governed by available distribution; if distribution was scarce then there was an opportunity cost of showing old content, because you couldn’t show something new (which again, was why talent wanted a share of multiple airings).</p><p>What is critical to note is that this leverage was best realized by selling to as many distributors as possible. The classic example is the traditional movie window: first you sell a movie to first-run theaters, then to budget theaters, then to hotels and airlines, then to pay-per-view, then to videocassettes/DVDs, then to cable, and finally to broadcast TV. That’s seven distinct opportunities to sell a piece of content. Going straight to streaming, though, collapses seven windows to one, reducing the ability to make money off of a particular piece of content.</p><p>Studios are enduring this cost, though, in the service of building up their own streaming services, but that has its own costs: running a streaming service entails being in the direct-to-consumer business, which is a costly one: not only do you have to build up and maintain the technical infrastructure of the service, and incur costs in customer support, but you also have to worry about things like churn that simply aren’t a consideration when you’re selling content. All of this is very expensive!</p><p>The real pain, though, is opportunity cost: while studios are missing out on multi-window revenue, paying for their streaming service, and trying to simultaneously acquire customers and stopping them from churning, they are also forgoing revenue from established services like Netflix that would not only happily pay them for their content, but could actually justify a much higher price given their significantly larger user base across which that cost could be leveraged.</p><p>All of these costs, it should be noted, occur in the aggregate, which is a real problem in these negotiations: talent is concerned about their compensation on a per-show basis, but studios are bleeding money on an entity-level in their foolhardy pursuit of customer-facing streaming services. Most of the discussion about this mismatch is focused on how to properly compensate the talent; note this item from <a href=\"https://puck.news/the-nanny-steps-all-over-bob-iger/?_cio_id=f6c6060fc9af019aeb1c\">Puck</a>:</p><blockquote><p>\n  The union and AMPTP have by and large agreed on the residuals improvements the DGA obtained in its recent deal, but the union also wants 2 percent of subscriber revenue to be shared with the cast of a successful show, with success measured by Parrot Analytics, an analysis firm that looks at viewership, social media engagement, and other factors, to determine “demand.” That proxy metric was proposed because the companies refuse to share their internal measurements, of course. But the studios declined to engage on that issue, and the management-side source asked how the producer of a show could be expected to share revenue earned not by the producer but by the platform (i.e., subscribers pay platforms; subscribers don’t pay producers).\n</p></blockquote><p>I get the talent’s perspective, but I’m pretty sure the talent doesn’t want to pay for the cost of customer service or customer acquisition or churn mitigation! Then again, neither should the studios: it doesn’t make any sense to me why the studios decided they wanted to bear these costs, and that’s not the talent’s problem.</p>",
          "contentText": "There is an even larger problem, though, which is that studios have — in my estimation — yet to come to grips with the true cost of streaming. To go back to the old model, studios were in the business of making movies or TV shows and then selling them to distributors. Ideally they would sell the same piece of content multiple times, better leveraging the cost of making the content in the first place. Indeed, this was a sticking point in the 1960 strike; from that 1960 New York Times article:\n  The guild asked for 100 percent payment of minimum salaries for the second showing in a new contract. The producers insisted that the first two runs be covered by the original salary. The producers contend that it is virtually impossible to get sufficient money out of the first showing of a movie produced solely for television to pay off the initial production investment. It is reported that many bank loans are predicated on earnings from the second run.\nContent costs a lot to produce up front, but the marginal cost of showing it again is effectively zero; that means the more times you can show a piece of content the more you can spend up front. The number of times you could show it, though, was, as noted above, governed by available distribution; if distribution was scarce then there was an opportunity cost of showing old content, because you couldn’t show something new (which again, was why talent wanted a share of multiple airings).What is critical to note is that this leverage was best realized by selling to as many distributors as possible. The classic example is the traditional movie window: first you sell a movie to first-run theaters, then to budget theaters, then to hotels and airlines, then to pay-per-view, then to videocassettes/DVDs, then to cable, and finally to broadcast TV. That’s seven distinct opportunities to sell a piece of content. Going straight to streaming, though, collapses seven windows to one, reducing the ability to make money off of a particular piece of content.Studios are enduring this cost, though, in the service of building up their own streaming services, but that has its own costs: running a streaming service entails being in the direct-to-consumer business, which is a costly one: not only do you have to build up and maintain the technical infrastructure of the service, and incur costs in customer support, but you also have to worry about things like churn that simply aren’t a consideration when you’re selling content. All of this is very expensive!The real pain, though, is opportunity cost: while studios are missing out on multi-window revenue, paying for their streaming service, and trying to simultaneously acquire customers and stopping them from churning, they are also forgoing revenue from established services like Netflix that would not only happily pay them for their content, but could actually justify a much higher price given their significantly larger user base across which that cost could be leveraged.All of these costs, it should be noted, occur in the aggregate, which is a real problem in these negotiations: talent is concerned about their compensation on a per-show basis, but studios are bleeding money on an entity-level in their foolhardy pursuit of customer-facing streaming services. Most of the discussion about this mismatch is focused on how to properly compensate the talent; note this item from Puck:\n  The union and AMPTP have by and large agreed on the residuals improvements the DGA obtained in its recent deal, but the union also wants 2 percent of subscriber revenue to be shared with the cast of a successful show, with success measured by Parrot Analytics, an analysis firm that looks at viewership, social media engagement, and other factors, to determine “demand.” That proxy metric was proposed because the companies refuse to share their internal measurements, of course. But the studios declined to engage on that issue, and the management-side source asked how the producer of a show could be expected to share revenue earned not by the producer but by the platform (i.e., subscribers pay platforms; subscribers don’t pay producers).\nI get the talent’s perspective, but I’m pretty sure the talent doesn’t want to pay for the cost of customer service or customer acquisition or churn mitigation! Then again, neither should the studios: it doesn’t make any sense to me why the studios decided they wanted to bear these costs, and that’s not the talent’s problem."
        },
        {
          "subtitle": "The Shrinking Pie",
          "contentHtml": "<p>There remains, though, the shrinking pie I noted in the introduction: the removal of distribution costs that enabled the rise of streaming was not a benefit that was limited to Hollywood, nor was the shift to attention as the only scarce resource. Every person on earth has only 168 hours in a week, during which time they are presumably sleeping and working. Those few remaining hours can now be filled by YouTube, or gaming, or podcasts, or reading this Article; every single minute spent doing something other than consuming Hollywood content is a minute lost forever.</p><p>This is consideration enough without a labor battle: thanks to COVID a lot of people fell out of the habit of going to the movie theater, and it appears around 25% of the audience permanently found something better to do with their time; that same reality applies to TV. Just as newspapers once thought the Internet was a boon because it increased their addressable market, only to find out that it also drastically increased competition for readers’ attention, Hollywood has to face the reality that the ability to make far more shows extends not only to studios but also to literally anyone. That reality is going to come to the fore if this strike drags on: if people don’t have new movies or shows to watch they will find far more options to fill their time than existed in 1960; the risk to Hollywood is that some of those alternatives become a permanent feature of people’s media diets, in line with what seems to have happened during COVID.</p><p>The broader issue is that the video industry finally seems to be facing what happened to the print and music industry before them: the Internet comes bearing gifts like infinite capacity and free distribution, but those gifts are a poisoned chalice for industries predicated on scarcity. When anyone could publish text, most text-based businesses went from massive profitability to terminal decline; when anyone could distribute music the music industry could only be saved by tech companies like Spotify helping them sell convenience in place of plastic discs.</p><p>For the video industry the first step to survival must be to retreat to what they are good at — producing content that isn’t available anywhere else — and getting away from what they are not, i.e. running undifferentiated streaming services with massive direct costs and even larger opportunity ones. Talent, meanwhile, has to realize that they and the studios are not divided by this new paradigm, but jointly threatened: the Internet is bad news for content producers with outsized costs, and long-term sustainability will be that much harder to achieve if the focus is on increasing them.</p><p><em>I wrote a follow-up to this Article in <a href=\"https://stratechery.com/2023/streaming-residuals-and-spotify-llama-2-open-sourced-llamas-license/\">this Daily Update</a>.</em></p>",
          "contentText": "There remains, though, the shrinking pie I noted in the introduction: the removal of distribution costs that enabled the rise of streaming was not a benefit that was limited to Hollywood, nor was the shift to attention as the only scarce resource. Every person on earth has only 168 hours in a week, during which time they are presumably sleeping and working. Those few remaining hours can now be filled by YouTube, or gaming, or podcasts, or reading this Article; every single minute spent doing something other than consuming Hollywood content is a minute lost forever.This is consideration enough without a labor battle: thanks to COVID a lot of people fell out of the habit of going to the movie theater, and it appears around 25% of the audience permanently found something better to do with their time; that same reality applies to TV. Just as newspapers once thought the Internet was a boon because it increased their addressable market, only to find out that it also drastically increased competition for readers’ attention, Hollywood has to face the reality that the ability to make far more shows extends not only to studios but also to literally anyone. That reality is going to come to the fore if this strike drags on: if people don’t have new movies or shows to watch they will find far more options to fill their time than existed in 1960; the risk to Hollywood is that some of those alternatives become a permanent feature of people’s media diets, in line with what seems to have happened during COVID.The broader issue is that the video industry finally seems to be facing what happened to the print and music industry before them: the Internet comes bearing gifts like infinite capacity and free distribution, but those gifts are a poisoned chalice for industries predicated on scarcity. When anyone could publish text, most text-based businesses went from massive profitability to terminal decline; when anyone could distribute music the music industry could only be saved by tech companies like Spotify helping them sell convenience in place of plastic discs.For the video industry the first step to survival must be to retreat to what they are good at — producing content that isn’t available anywhere else — and getting away from what they are not, i.e. running undifferentiated streaming services with massive direct costs and even larger opportunity ones. Talent, meanwhile, has to realize that they and the studios are not divided by this new paradigm, but jointly threatened: the Internet is bad news for content producers with outsized costs, and long-term sustainability will be that much harder to achieve if the focus is on increasing them.I wrote a follow-up to this Article in this Daily Update."
        }
      ]
    }
  ]
}